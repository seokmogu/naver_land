#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë¶€ë™ì‚° ë§¤ë¬¼ ìƒì„¸ ì •ë³´ API ë¶„ì„ê¸°
ë§¤ë¬¼ ëª©ë¡ì—ì„œ ê°œë³„ ë§¤ë¬¼ í´ë¦­ ì‹œ í˜¸ì¶œë˜ëŠ” APIë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""

from playwright.sync_api import sync_playwright
import json
import time
from datetime import datetime

class DetailAPIAnalyzer:
    def __init__(self):
        self.captured_requests = []
        self.captured_responses = []
        self.detail_apis = []
        
    def analyze_detail_apis(self, url):
        """ë§¤ë¬¼ ìƒì„¸ ì •ë³´ API ë¶„ì„"""
        print("ğŸ” ë§¤ë¬¼ ìƒì„¸ ì •ë³´ API ë¶„ì„ ì‹œì‘...")
        print(f"ğŸ“„ ë¶„ì„ URL: {url}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False, slow_mo=500)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            
            page = context.new_page()
            
            # ë„¤íŠ¸ì›Œí¬ ìš”ì²­/ì‘ë‹µ ìº¡ì²˜
            def handle_request(request):
                if 'new.land.naver.com/api' in request.url:
                    req_data = {
                        'url': request.url,
                        'method': request.method,
                        'headers': dict(request.headers),
                        'timestamp': datetime.now().isoformat()
                    }
                    self.captured_requests.append(req_data)
                    
                    # ìƒì„¸ ì •ë³´ ê´€ë ¨ API í•„í„°ë§
                    if any(keyword in request.url for keyword in ['article', 'detail', 'info']):
                        print(f"ğŸ“¡ ìƒì„¸ API ìš”ì²­: {request.method} {request.url}")
            
            def handle_response(response):
                if 'new.land.naver.com/api' in response.url:
                    try:
                        response_data = {
                            'url': response.url,
                            'status': response.status,
                            'headers': dict(response.headers),
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        # JSON ì‘ë‹µ íŒŒì‹±
                        try:
                            if 'application/json' in response.headers.get('content-type', ''):
                                body = response.json()
                                response_data['body'] = body
                                
                                # ìƒì„¸ ì •ë³´ API ì‘ë‹µ í•„í„°ë§
                                if any(keyword in response.url for keyword in ['article', 'detail', 'info']):
                                    print(f"âœ… ìƒì„¸ API ì‘ë‹µ: {response.status} {response.url}")
                                    self.detail_apis.append(response_data)
                        except:
                            response_data['body'] = 'Failed to parse JSON'
                        
                        self.captured_responses.append(response_data)
                        
                    except Exception as e:
                        print(f"âŒ ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            page.on('request', handle_request)
            page.on('response', handle_response)
            
            try:
                print("\nğŸŒ í˜ì´ì§€ ë¡œë”© ì¤‘...")
                page.goto(url, wait_until="networkidle")
                time.sleep(3)
                
                print("ğŸ“‹ ë§¤ë¬¼ ëª©ë¡ ë¡œë”© ëŒ€ê¸°...")
                # ë§¤ë¬¼ ëª©ë¡ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                try:
                    page.wait_for_selector(".item_link", timeout=10000)
                    print("âœ… ë§¤ë¬¼ ëª©ë¡ ë¡œë“œ ì™„ë£Œ")
                except:
                    print("âš ï¸ ë§¤ë¬¼ ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨, ê³„ì† ì§„í–‰...")
                
                # ì²« ë²ˆì§¸ ë§¤ë¬¼ í´ë¦­
                print("\nğŸ–±ï¸ ì²« ë²ˆì§¸ ë§¤ë¬¼ í´ë¦­...")
                try:
                    # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
                    selectors = [
                        ".item_link",
                        ".article_item",
                        ".list_item",
                        "[data-article-no]",
                        ".item"
                    ]
                    
                    clicked = False
                    for selector in selectors:
                        try:
                            elements = page.query_selector_all(selector)
                            if elements:
                                print(f"ğŸ¯ {selector} ì„ íƒìë¡œ {len(elements)}ê°œ ë§¤ë¬¼ ë°œê²¬")
                                elements[0].scroll_into_view_if_needed()
                                time.sleep(1)
                                elements[0].click()
                                clicked = True
                                print("âœ… ë§¤ë¬¼ í´ë¦­ ì„±ê³µ")
                                break
                        except Exception as e:
                            print(f"âŒ {selector} í´ë¦­ ì‹¤íŒ¨: {e}")
                            continue
                    
                    if not clicked:
                        print("âš ï¸ ìë™ í´ë¦­ ì‹¤íŒ¨, ìˆ˜ë™ìœ¼ë¡œ ë§¤ë¬¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”...")
                        time.sleep(10)
                
                except Exception as e:
                    print(f"âŒ ë§¤ë¬¼ í´ë¦­ ì˜¤ë¥˜: {e}")
                
                # ìƒì„¸ ì •ë³´ ë¡œë”© ëŒ€ê¸°
                print("â³ ìƒì„¸ ì •ë³´ ë¡œë”© ëŒ€ê¸°...")
                time.sleep(5)
                
                # ì¶”ê°€ ìƒì„¸ ì •ë³´ ë¡œë”©ì„ ìœ„í•œ ìŠ¤í¬ë¡¤
                print("ğŸ“œ ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë¡¤...")
                try:
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(2)
                    page.evaluate("window.scrollTo(0, 0)")
                    time.sleep(2)
                except:
                    pass
                
                # ë‘ ë²ˆì§¸ ë§¤ë¬¼ë„ í´ë¦­í•´ë³´ê¸°
                print("\nğŸ–±ï¸ ë‘ ë²ˆì§¸ ë§¤ë¬¼ í´ë¦­ ì‹œë„...")
                try:
                    elements = page.query_selector_all(".item_link, .article_item, .list_item")
                    if len(elements) > 1:
                        elements[1].click()
                        time.sleep(3)
                        print("âœ… ë‘ ë²ˆì§¸ ë§¤ë¬¼ í´ë¦­ ì„±ê³µ")
                except:
                    print("âš ï¸ ë‘ ë²ˆì§¸ ë§¤ë¬¼ í´ë¦­ ì‹¤íŒ¨")
                
                print("ğŸ“Š ë¶„ì„ ì™„ë£Œ!")
                
            except Exception as e:
                print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                time.sleep(2)  # ë§ˆì§€ë§‰ ì‘ë‹µ ëŒ€ê¸°
                browser.close()
    
    def save_detail_analysis(self):
        """ìƒì„¸ API ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ìƒì„¸ APIë§Œ í•„í„°ë§
        detail_requests = [req for req in self.captured_requests 
                          if any(keyword in req['url'] for keyword in ['article', 'detail', 'info'])]
        
        analysis = {
            "ë¶„ì„ì‹œê°„": timestamp,
            "ìƒì„¸_API_ìš”ì²­ìˆ˜": len(detail_requests),
            "ìƒì„¸_API_ì‘ë‹µìˆ˜": len(self.detail_apis),
            "ì „ì²´_ìš”ì²­ìˆ˜": len(self.captured_requests),
            "ì „ì²´_ì‘ë‹µìˆ˜": len(self.captured_responses),
            "ìƒì„¸_API_ëª©ë¡": list(set([req['url'].split('?')[0] for req in detail_requests])),
            "ìƒì„¸_ìš”ì²­": detail_requests,
            "ìƒì„¸_ì‘ë‹µ": self.detail_apis,
            "ì „ì²´_ì‘ë‹µ": self.captured_responses
        }
        
        filename = f"detail_api_analysis_{timestamp}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì €ì¥: {filename}")
        return filename
    
    def print_detail_summary(self):
        """ìƒì„¸ API ë¶„ì„ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ë§¤ë¬¼ ìƒì„¸ API ë¶„ì„ ìš”ì•½")
        print("="*60)
        
        # ìƒì„¸ API ìš”ì²­ í†µê³„
        detail_requests = [req for req in self.captured_requests 
                          if any(keyword in req['url'] for keyword in ['article', 'detail', 'info'])]
        
        print(f"ğŸ” ìƒì„¸ API ìš”ì²­: {len(detail_requests)}ê°œ")
        print(f"ğŸ“¥ ìƒì„¸ API ì‘ë‹µ: {len(self.detail_apis)}ê°œ")
        
        # ìƒì„¸ API ì—”ë“œí¬ì¸íŠ¸
        if detail_requests:
            endpoints = list(set([req['url'].split('?')[0] for req in detail_requests]))
            print(f"\nğŸ¯ ë°œê²¬ëœ ìƒì„¸ API ì—”ë“œí¬ì¸íŠ¸:")
            for endpoint in endpoints:
                count = len([req for req in detail_requests if endpoint in req['url']])
                print(f"  - {endpoint}: {count}íšŒ")
        
        # ìƒì„¸ ì‘ë‹µ ìƒ˜í”Œ
        if self.detail_apis:
            print(f"\nğŸ“„ ìƒì„¸ ì‘ë‹µ ìƒ˜í”Œ:")
            for i, resp in enumerate(self.detail_apis[:3]):
                print(f"\n{i+1}. {resp['url']}")
                print(f"   ìƒíƒœ: {resp['status']}")
                if 'body' in resp and isinstance(resp['body'], dict):
                    keys = list(resp['body'].keys())[:5]
                    print(f"   ì£¼ìš” ë°ì´í„°: {keys}")

def main():
    analyzer = DetailAPIAnalyzer()
    
    # ë¶„ì„í•  URL
    test_url = "https://new.land.naver.com/offices?ms=37.522786,127.0466693,15&a=SG:SMS:GJCG:APTHGJ:GM:TJ&e=RETAIL"
    
    # ìƒì„¸ API ë¶„ì„ ì‹¤í–‰
    analyzer.analyze_detail_apis(test_url)
    
    # ê²°ê³¼ ì¶œë ¥
    analyzer.print_detail_summary()
    
    # ê²°ê³¼ ì €ì¥
    analyzer.save_detail_analysis()

if __name__ == "__main__":
    main()