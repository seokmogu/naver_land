#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë¶€ë™ì‚° ë¬´í•œìŠ¤í¬ë¡¤ ë¶„ì„ê¸°
ì œê³µëœ íŠ¹ì • URLì—ì„œ ëª¨ë“  ë§¤ë¬¼ì„ ë¬´í•œìŠ¤í¬ë¡¤ë¡œ ë¡œë”©í•˜ë©° API íŒ¨í„´ ë¶„ì„
"""

from playwright.sync_api import sync_playwright
import json
import time
from datetime import datetime

class InfinityScrollAnalyzer:
    def __init__(self):
        self.captured_requests = []
        self.captured_responses = []
        self.page_loads = []  # í˜ì´ì§€ë³„ ë¡œë”© ì¶”ì 
        
    def analyze_infinity_scroll(self, url):
        """ë¬´í•œìŠ¤í¬ë¡¤ íŒ¨í„´ ì •í™• ë¶„ì„"""
        print("ğŸ”„ ë¬´í•œìŠ¤í¬ë¡¤ íŒ¨í„´ ë¶„ì„ ì‹œì‘...")
        print(f"ğŸ“„ URL: {url}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False, slow_mo=200)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            
            page = context.new_page()
            
            # ë„¤íŠ¸ì›Œí¬ ìš”ì²­/ì‘ë‹µ ìº¡ì²˜
            def handle_request(request):
                if 'new.land.naver.com/api' in request.url:
                    req_data = {
                        'url': request.url,
                        'method': request.method,
                        'timestamp': datetime.now().isoformat(),
                        'is_articles': '/api/articles?' in request.url
                    }
                    self.captured_requests.append(req_data)
                    
                    # articles API ì¶”ì 
                    if '/api/articles?' in request.url:
                        import re
                        page_match = re.search(r'page=(\d+)', request.url)
                        page_num = page_match.group(1) if page_match else '1'
                        print(f"ğŸ“„ API ìš”ì²­ - í˜ì´ì§€ {page_num}")
            
            def handle_response(response):
                if 'new.land.naver.com/api' in response.url:
                    try:
                        response_data = {
                            'url': response.url,
                            'status': response.status,
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        # articles API ì‘ë‹µ ë¶„ì„
                        if '/api/articles?' in response.url and response.status == 200:
                            try:
                                body = response.json()
                                if 'articleList' in body:
                                    article_count = len(body['articleList'])
                                    total_count = body.get('articleCount', 0)
                                    
                                    import re
                                    page_match = re.search(r'page=(\d+)', response.url)
                                    page_num = page_match.group(1) if page_match else '1'
                                    
                                    response_data['body'] = body
                                    response_data['article_count'] = article_count
                                    response_data['total_count'] = total_count
                                    response_data['page_num'] = page_num
                                    
                                    self.page_loads.append({
                                        'page': page_num,
                                        'article_count': article_count,
                                        'total_count': total_count,
                                        'timestamp': datetime.now().isoformat()
                                    })
                                    
                                    print(f"âœ… API ì‘ë‹µ - í˜ì´ì§€ {page_num}: {article_count}ê°œ ë§¤ë¬¼ (ì „ì²´: {total_count})")
                            except:
                                pass
                        
                        self.captured_responses.append(response_data)
                        
                    except Exception as e:
                        print(f"âŒ ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            page.on('request', handle_request)
            page.on('response', handle_response)
            
            try:
                print("\nğŸŒ í˜ì´ì§€ ë¡œë”©...")
                page.goto(url, wait_until="networkidle")
                time.sleep(5)
                
                # ë§¤ë¬¼ ëª©ë¡ ëŒ€ê¸°
                print("ğŸ“‹ ë§¤ë¬¼ ëª©ë¡ ë¡œë”© ëŒ€ê¸°...")
                page.wait_for_selector(".item_link", timeout=15000)
                
                # ì´ˆê¸° ë§¤ë¬¼ ìˆ˜ í™•ì¸
                initial_items = page.query_selector_all(".item_link")
                print(f"ğŸ“Š ì´ˆê¸° ë§¤ë¬¼: {len(initial_items)}ê°œ")
                
                # í˜ì´ì§€ ì •ë³´ í™•ì¸
                try:
                    # ì´ ë§¤ë¬¼ ìˆ˜ í‘œì‹œ ì°¾ê¸°
                    total_element = page.query_selector(".filter_number, .total_count, .result_count")
                    if total_element:
                        total_text = total_element.inner_text()
                        print(f"ğŸ“Š ì´ ë§¤ë¬¼ í‘œì‹œ: {total_text}")
                except:
                    pass
                
                # ë¬´í•œìŠ¤í¬ë¡¤ ì‹œì‘
                print(f"\nğŸ”„ ë¬´í•œìŠ¤í¬ë¡¤ ì‹œì‘...")
                scroll_count = 0
                max_scrolls = 20  # ìµœëŒ€ 20ë²ˆ ìŠ¤í¬ë¡¤
                no_change_count = 0  # ë³€í™” ì—†ëŠ” íšŸìˆ˜
                
                last_count = len(initial_items)
                
                for i in range(max_scrolls):
                    scroll_count += 1
                    print(f"\nğŸ“œ ìŠ¤í¬ë¡¤ {scroll_count}ë²ˆ...")
                    
                    # í˜„ì¬ ìœ„ì¹˜ì—ì„œ ìŠ¤í¬ë¡¤
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    
                    # ë¡œë”© ëŒ€ê¸°
                    time.sleep(3)
                    
                    # ë§¤ë¬¼ ìˆ˜ í™•ì¸
                    current_items = page.query_selector_all(".item_link")
                    current_count = len(current_items)
                    added = current_count - last_count
                    
                    print(f"ğŸ“Š ë§¤ë¬¼ ìˆ˜: {last_count} â†’ {current_count} ({added:+d})")
                    
                    if added > 0:
                        no_change_count = 0
                        last_count = current_count
                        print(f"âœ… {added}ê°œ ë§¤ë¬¼ ì¶”ê°€ ë¡œë”©ë¨")
                    else:
                        no_change_count += 1
                        print(f"â¸ï¸ ì¶”ê°€ ë¡œë”© ì—†ìŒ ({no_change_count}/3)")
                        
                        # 3ë²ˆ ì—°ì† ë³€í™”ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                        if no_change_count >= 3:
                            print("ğŸ“„ ë” ì´ìƒ ë¡œë”©í•  ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                            break
                    
                    # ë¡œë”© ì¸ë””ì¼€ì´í„° í™•ì¸
                    try:
                        loading = page.query_selector(".loading, .spinner, .load-more")
                        if loading and loading.is_visible():
                            print("â³ ë¡œë”© ì¤‘...")
                            time.sleep(2)
                    except:
                        pass
                
                # ìµœì¢… ê²°ê³¼
                final_items = page.query_selector_all(".item_link")
                total_loaded = len(final_items)
                total_added = total_loaded - len(initial_items)
                
                print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
                print(f"   ì´ˆê¸°: {len(initial_items)}ê°œ")
                print(f"   ìµœì¢…: {total_loaded}ê°œ")
                print(f"   ì¶”ê°€: {total_added}ê°œ")
                print(f"   ìŠ¤í¬ë¡¤: {scroll_count}ë²ˆ")
                
                # í˜ì´ì§€ ë¡œë”© í†µê³„
                if self.page_loads:
                    pages = [int(p['page']) for p in self.page_loads if p['page'].isdigit()]
                    if pages:
                        print(f"   ìµœëŒ€ í˜ì´ì§€: {max(pages)}")
                        print(f"   ì´ í˜ì´ì§€ ìˆ˜: {len(set(pages))}")
                
                print("ğŸ“Š ë¶„ì„ ì™„ë£Œ!")
                
            except Exception as e:
                print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                time.sleep(3)  # ë§ˆì§€ë§‰ ì‘ë‹µ ëŒ€ê¸°
                browser.close()
    
    def save_infinity_analysis(self):
        """ë¬´í•œìŠ¤í¬ë¡¤ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # articles APIë§Œ í•„í„°ë§
        articles_requests = [req for req in self.captured_requests if req.get('is_articles')]
        articles_responses = [resp for resp in self.captured_responses 
                            if '/api/articles?' in resp['url'] and 'body' in resp]
        
        analysis = {
            "ë¶„ì„ì‹œê°„": timestamp,
            "ì´_ìš”ì²­ìˆ˜": len(self.captured_requests),
            "articles_ìš”ì²­ìˆ˜": len(articles_requests),
            "articles_ì‘ë‹µìˆ˜": len(articles_responses),
            "í˜ì´ì§€_ë¡œë”©_ìˆœì„œ": self.page_loads,
            "ë¡œë”©ëœ_í˜ì´ì§€ìˆ˜": len(set(p['page'] for p in self.page_loads)),
            "ì „ì²´_ìš”ì²­": self.captured_requests,
            "articles_ì‘ë‹µ": articles_responses
        }
        
        filename = f"infinity_scroll_analysis_{timestamp}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {filename}")
        return filename
    
    def print_infinity_summary(self):
        """ë¬´í•œìŠ¤í¬ë¡¤ ë¶„ì„ ìš”ì•½"""
        print("\n" + "="*60)
        print("ğŸ“Š ë¬´í•œìŠ¤í¬ë¡¤ ë¶„ì„ ìš”ì•½")
        print("="*60)
        
        articles_requests = [req for req in self.captured_requests if req.get('is_articles')]
        
        print(f"ğŸ“„ ì „ì²´ API ìš”ì²­: {len(self.captured_requests)}ê°œ")
        print(f"ğŸ“‹ ë§¤ë¬¼ API ìš”ì²­: {len(articles_requests)}ê°œ")
        
        if self.page_loads:
            pages = [p['page'] for p in self.page_loads]
            unique_pages = list(set(pages))
            
            print(f"\nğŸ“„ ë¡œë”©ëœ í˜ì´ì§€:")
            print(f"   - ì´ í˜ì´ì§€ ìˆ˜: {len(unique_pages)}")
            print(f"   - í˜ì´ì§€ ë²”ìœ„: {min(unique_pages)} ~ {max(unique_pages)}")
            
            print(f"\nğŸ“Š í˜ì´ì§€ë³„ ë§¤ë¬¼ ìˆ˜:")
            for load in self.page_loads:
                print(f"   - í˜ì´ì§€ {load['page']}: {load['article_count']}ê°œ")
            
            total_articles = sum(load['article_count'] for load in self.page_loads)
            print(f"\nğŸ¯ ì´ ë¡œë”©ëœ ë§¤ë¬¼: {total_articles}ê°œ")
            
            # ë¬´í•œìŠ¤í¬ë¡¤ íŒ¨í„´ í™•ì¸
            if len(unique_pages) > 1:
                print(f"âœ… ë¬´í•œìŠ¤í¬ë¡¤ íŒ¨í„´ í™•ì¸ë¨!")
            else:
                print(f"âŒ ë¬´í•œìŠ¤í¬ë¡¤ íŒ¨í„´ ë¯¸í™•ì¸")

def main():
    analyzer = InfinityScrollAnalyzer()
    
    # ì›ë˜ ì œê³µëœ URL ì‚¬ìš©
    original_url = "https://new.land.naver.com/offices?ms=37.522786,127.0466693,15&a=SG:SMS:GJCG:APTHGJ:GM:TJ&e=RETAIL"
    
    # ë¬´í•œìŠ¤í¬ë¡¤ ë¶„ì„
    analyzer.analyze_infinity_scroll(original_url)
    
    # ê²°ê³¼ ì¶œë ¥
    analyzer.print_infinity_summary()
    
    # ê²°ê³¼ ì €ì¥
    analyzer.save_infinity_analysis()

if __name__ == "__main__":
    main()