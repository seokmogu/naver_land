#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë¶€ë™ì‚° ì¢Œì¸¡ ë§¤ë¬¼ ëª©ë¡ ìŠ¤í¬ë¡¤ ë¶„ì„ê¸°
ì¢Œì¸¡ ë§¤ë¬¼ ëª©ë¡ ì˜ì—­ì˜ ìŠ¤í¬ë¡¤ì„ í†µí•œ ë¬´í•œë¡œë”© íŒ¨í„´ ë¶„ì„
"""

from playwright.sync_api import sync_playwright
import json
import time
from datetime import datetime

class ListScrollAnalyzer:
    def __init__(self):
        self.captured_requests = []
        self.captured_responses = []
        self.page_loads = []
        
    def analyze_list_scroll(self, url):
        """ì¢Œì¸¡ ë§¤ë¬¼ ëª©ë¡ ìŠ¤í¬ë¡¤ ë¶„ì„"""
        print("ğŸ“‹ ì¢Œì¸¡ ë§¤ë¬¼ ëª©ë¡ ìŠ¤í¬ë¡¤ ë¶„ì„ ì‹œì‘...")
        print(f"ğŸ“„ URL: {url}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False, slow_mo=300)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            
            page = context.new_page()
            
            # ë„¤íŠ¸ì›Œí¬ ìš”ì²­/ì‘ë‹µ ìº¡ì²˜
            def handle_request(request):
                if 'new.land.naver.com/api' in request.url:
                    req_data = {
                        'url': request.url,
                        'method': request.method,
                        'timestamp': datetime.now().isoformat(),
                        'is_articles': '/api/articles?' in request.url
                    }
                    self.captured_requests.append(req_data)
                    
                    if '/api/articles?' in request.url:
                        import re
                        page_match = re.search(r'page=(\d+)', request.url)
                        page_num = page_match.group(1) if page_match else '1'
                        print(f"ğŸ“„ ë§¤ë¬¼ API ìš”ì²­ - í˜ì´ì§€ {page_num}")
            
            def handle_response(response):
                if 'new.land.naver.com/api' in response.url:
                    try:
                        response_data = {
                            'url': response.url,
                            'status': response.status,
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        if '/api/articles?' in response.url and response.status == 200:
                            try:
                                body = response.json()
                                if 'articleList' in body:
                                    article_count = len(body['articleList'])
                                    total_count = body.get('articleCount', 0)
                                    
                                    import re
                                    page_match = re.search(r'page=(\d+)', response.url)
                                    page_num = page_match.group(1) if page_match else '1'
                                    
                                    response_data['body'] = body
                                    response_data['article_count'] = article_count
                                    response_data['total_count'] = total_count
                                    response_data['page_num'] = page_num
                                    
                                    self.page_loads.append({
                                        'page': page_num,
                                        'article_count': article_count,
                                        'total_count': total_count,
                                        'timestamp': datetime.now().isoformat()
                                    })
                                    
                                    print(f"âœ… ë§¤ë¬¼ API ì‘ë‹µ - í˜ì´ì§€ {page_num}: {article_count}ê°œ (ì „ì²´: {total_count})")
                            except:
                                pass
                        
                        self.captured_responses.append(response_data)
                        
                    except Exception as e:
                        print(f"âŒ ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            page.on('request', handle_request)
            page.on('response', handle_response)
            
            try:
                print("\nğŸŒ í˜ì´ì§€ ë¡œë”©...")
                page.goto(url, wait_until="networkidle")
                time.sleep(5)
                
                # ì¢Œì¸¡ ë§¤ë¬¼ ëª©ë¡ ì˜ì—­ ì°¾ê¸°
                print("ğŸ” ì¢Œì¸¡ ë§¤ë¬¼ ëª©ë¡ ì˜ì—­ ì°¾ëŠ” ì¤‘...")
                
                # ë‹¤ì–‘í•œ ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ì„ íƒì ì‹œë„
                list_selectors = [
                    ".ArticleList", 
                    ".article_list", 
                    ".list_contents",
                    ".item_article_list",
                    ".ArticleListArea",
                    ".list_wrap",
                    "#ArticleListApp",
                    "[class*='article'][class*='list']",
                    "[class*='List'][class*='Container']"
                ]
                
                list_container = None
                for selector in list_selectors:
                    try:
                        element = page.query_selector(selector)
                        if element:
                            list_container = element
                            print(f"âœ… ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                            break
                    except:
                        continue
                
                if not list_container:
                    print("âš ï¸ ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ìŠ¤í¬ë¡¤ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                
                # ì´ˆê¸° ë§¤ë¬¼ ìˆ˜ í™•ì¸
                initial_items = page.query_selector_all(".item_link, .article_item")
                print(f"ğŸ“Š ì´ˆê¸° ë§¤ë¬¼: {len(initial_items)}ê°œ")
                
                # ì¢Œì¸¡ ëª©ë¡ ì˜ì—­ ìŠ¤í¬ë¡¤ ì‹œì‘
                print(f"\nğŸ“‹ ì¢Œì¸¡ ë§¤ë¬¼ ëª©ë¡ ìŠ¤í¬ë¡¤ ì‹œì‘...")
                scroll_count = 0
                max_scrolls = 15
                no_change_count = 0
                last_count = len(initial_items)
                
                for i in range(max_scrolls):
                    scroll_count += 1
                    print(f"\nğŸ“œ ëª©ë¡ ìŠ¤í¬ë¡¤ {scroll_count}ë²ˆ...")
                    
                    # ì¢Œì¸¡ ëª©ë¡ ì˜ì—­ ìŠ¤í¬ë¡¤
                    if list_container:
                        # íŠ¹ì • ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ìŠ¤í¬ë¡¤
                        try:
                            page.evaluate(f"""
                                const container = document.querySelector('{list_selectors[0]}');
                                if (container) {{
                                    container.scrollTop = container.scrollHeight;
                                }}
                            """)
                            print("ğŸ¯ ë§¤ë¬¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ìŠ¤í¬ë¡¤ ì‹¤í–‰")
                        except:
                            # ëŒ€ì•ˆ: í˜ì´ì§€ ì „ì²´ ìŠ¤í¬ë¡¤
                            page.evaluate("window.scrollTo(0, window.scrollY + 1000)")
                            print("ğŸ”„ í˜ì´ì§€ ìŠ¤í¬ë¡¤ë¡œ ëŒ€ì²´")
                    else:
                        # ì¢Œì¸¡ ì˜ì—­ ì¶”ì •í•˜ì—¬ ìŠ¤í¬ë¡¤
                        try:
                            # ì¢Œì¸¡ ì ˆë°˜ ì˜ì—­ì—ì„œ ìŠ¤í¬ë¡¤ ì´ë²¤íŠ¸ ë°œìƒ
                            page.evaluate("""
                                const leftArea = document.elementFromPoint(300, 400);
                                if (leftArea) {
                                    let scrollable = leftArea;
                                    while (scrollable && scrollable.scrollHeight <= scrollable.clientHeight) {
                                        scrollable = scrollable.parentElement;
                                    }
                                    if (scrollable) {
                                        scrollable.scrollTop += 1000;
                                    }
                                }
                            """)
                            print("ğŸ¯ ì¢Œì¸¡ ì˜ì—­ ìŠ¤í¬ë¡¤ ì‹¤í–‰")
                        except:
                            page.evaluate("window.scrollTo(0, window.scrollY + 1000)")
                            print("ğŸ”„ ì¼ë°˜ ìŠ¤í¬ë¡¤ë¡œ ëŒ€ì²´")
                    
                    # ë¡œë”© ëŒ€ê¸°
                    time.sleep(4)
                    
                    # ë§¤ë¬¼ ìˆ˜ í™•ì¸
                    current_items = page.query_selector_all(".item_link, .article_item")
                    current_count = len(current_items)
                    added = current_count - last_count
                    
                    print(f"ğŸ“Š ë§¤ë¬¼ ìˆ˜: {last_count} â†’ {current_count} ({added:+d})")
                    
                    if added > 0:
                        no_change_count = 0
                        last_count = current_count
                        print(f"âœ… {added}ê°œ ë§¤ë¬¼ ì¶”ê°€ ë¡œë”©ë¨")
                    else:
                        no_change_count += 1
                        print(f"â¸ï¸ ì¶”ê°€ ë¡œë”© ì—†ìŒ ({no_change_count}/3)")
                        
                        # 3ë²ˆ ì—°ì† ë³€í™”ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                        if no_change_count >= 3:
                            print("ğŸ“„ ë” ì´ìƒ ë¡œë”©í•  ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                            break
                    
                    # ë¡œë”© ìƒíƒœ í™•ì¸
                    try:
                        loading_elements = page.query_selector_all(".loading, .spinner, [class*='load']")
                        visible_loading = [el for el in loading_elements if el.is_visible()]
                        if visible_loading:
                            print("â³ ë¡œë”© ì¤‘... ì¶”ê°€ ëŒ€ê¸°")
                            time.sleep(3)
                    except:
                        pass
                
                # ìµœì¢… ê²°ê³¼
                final_items = page.query_selector_all(".item_link, .article_item")
                total_loaded = len(final_items)
                total_added = total_loaded - len(initial_items)
                
                print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
                print(f"   ì´ˆê¸°: {len(initial_items)}ê°œ")
                print(f"   ìµœì¢…: {total_loaded}ê°œ")
                print(f"   ì¶”ê°€: {total_added}ê°œ")
                print(f"   ìŠ¤í¬ë¡¤: {scroll_count}ë²ˆ")
                
                if self.page_loads:
                    pages = [int(p['page']) for p in self.page_loads if p['page'].isdigit()]
                    if pages:
                        print(f"   ìµœëŒ€ í˜ì´ì§€: {max(pages)}")
                        print(f"   ë¡œë”©ëœ í˜ì´ì§€: {len(set(pages))}ê°œ")
                
                print("ğŸ“Š ë¶„ì„ ì™„ë£Œ!")
                
            except Exception as e:
                print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                time.sleep(3)
                browser.close()
    
    def save_list_scroll_analysis(self):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        articles_requests = [req for req in self.captured_requests if req.get('is_articles')]
        
        analysis = {
            "ë¶„ì„ì‹œê°„": timestamp,
            "ë¶„ì„íƒ€ì…": "ì¢Œì¸¡_ë§¤ë¬¼_ëª©ë¡_ìŠ¤í¬ë¡¤",
            "ì´_ìš”ì²­ìˆ˜": len(self.captured_requests),
            "ë§¤ë¬¼_API_ìš”ì²­ìˆ˜": len(articles_requests),
            "í˜ì´ì§€_ë¡œë”©_ìˆœì„œ": self.page_loads,
            "ë¡œë”©ëœ_í˜ì´ì§€ìˆ˜": len(set(p['page'] for p in self.page_loads)),
            "ì „ì²´_ì‘ë‹µ": self.captured_responses
        }
        
        filename = f"list_scroll_analysis_{timestamp}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {filename}")
        return filename
    
    def print_list_scroll_summary(self):
        """ë¶„ì„ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ì¢Œì¸¡ ë§¤ë¬¼ ëª©ë¡ ìŠ¤í¬ë¡¤ ë¶„ì„ ìš”ì•½")
        print("="*60)
        
        articles_requests = [req for req in self.captured_requests if req.get('is_articles')]
        
        print(f"ğŸ“„ ì „ì²´ API ìš”ì²­: {len(self.captured_requests)}ê°œ")
        print(f"ğŸ“‹ ë§¤ë¬¼ API ìš”ì²­: {len(articles_requests)}ê°œ")
        
        if self.page_loads:
            pages = [p['page'] for p in self.page_loads]
            unique_pages = list(set(pages))
            
            print(f"\nğŸ“„ ë¡œë”©ëœ í˜ì´ì§€:")
            print(f"   - ì´ í˜ì´ì§€ ìˆ˜: {len(unique_pages)}")
            if len(unique_pages) > 1:
                print(f"   - í˜ì´ì§€ ë²”ìœ„: {min(unique_pages)} ~ {max(unique_pages)}")
            
            print(f"\nğŸ“Š í˜ì´ì§€ë³„ ë§¤ë¬¼ ìˆ˜:")
            for load in self.page_loads:
                print(f"   - í˜ì´ì§€ {load['page']}: {load['article_count']}ê°œ")
            
            if len(unique_pages) > 1:
                print(f"\nâœ… ì¢Œì¸¡ ëª©ë¡ ë¬´í•œìŠ¤í¬ë¡¤ íŒ¨í„´ í™•ì¸ë¨!")
            else:
                print(f"\nâŒ ë¬´í•œìŠ¤í¬ë¡¤ íŒ¨í„´ ë¯¸í™•ì¸ (ë§¤ë¬¼ ë¶€ì¡±)")

def main():
    analyzer = ListScrollAnalyzer()
    
    # ì›ë˜ ì œê³µëœ URL
    original_url = "https://new.land.naver.com/offices?ms=37.522786,127.0466693,15&a=SG:SMS:GJCG:APTHGJ:GM:TJ&e=RETAIL"
    
    # ì¢Œì¸¡ ëª©ë¡ ìŠ¤í¬ë¡¤ ë¶„ì„
    analyzer.analyze_list_scroll(original_url)
    
    # ê²°ê³¼ ì¶œë ¥
    analyzer.print_list_scroll_summary()
    
    # ê²°ê³¼ ì €ì¥
    analyzer.save_list_scroll_analysis()

if __name__ == "__main__":
    main()