#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë¶€ë™ì‚° ìŠ¤í¬ë¡¤ë§ ë¬´í•œë¡œë”© ë¶„ì„ê¸°
ë§¤ë¬¼ ëª©ë¡ì„ ìŠ¤í¬ë¡¤í•˜ë©´ì„œ ì¶”ê°€ API í˜¸ì¶œ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.
"""

from playwright.sync_api import sync_playwright
import json
import time
from datetime import datetime

class ScrollLoadingAnalyzer:
    def __init__(self):
        self.captured_requests = []
        self.captured_responses = []
        self.page_requests = []  # í˜ì´ì§€ë³„ ìš”ì²­ ì¶”ì 
        
    def analyze_scroll_loading(self, url):
        """ìŠ¤í¬ë¡¤ë§ ë¬´í•œë¡œë”© íŒ¨í„´ ë¶„ì„"""
        print("ğŸ” ìŠ¤í¬ë¡¤ë§ ë¬´í•œë¡œë”© íŒ¨í„´ ë¶„ì„ ì‹œì‘...")
        print(f"ğŸ“„ ë¶„ì„ URL: {url}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False, slow_mo=300)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            
            page = context.new_page()
            
            # ë„¤íŠ¸ì›Œí¬ ìš”ì²­/ì‘ë‹µ ìº¡ì²˜
            def handle_request(request):
                if 'new.land.naver.com/api' in request.url:
                    req_data = {
                        'url': request.url,
                        'method': request.method,
                        'headers': dict(request.headers),
                        'timestamp': datetime.now().isoformat()
                    }
                    self.captured_requests.append(req_data)
                    
                    # articles APIë§Œ ì¶”ì 
                    if '/api/articles?' in request.url:
                        # page íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                        if 'page=' in request.url:
                            import re
                            page_match = re.search(r'page=(\d+)', request.url)
                            page_num = page_match.group(1) if page_match else 'unknown'
                        else:
                            page_num = '1'  # ê¸°ë³¸ê°’
                            
                        print(f"ğŸ“„ í˜ì´ì§€ {page_num} ìš”ì²­: {request.url.split('?')[0]}")
                        self.page_requests.append({
                            'page': page_num,
                            'url': request.url,
                            'timestamp': datetime.now().isoformat()
                        })
            
            def handle_response(response):
                if 'new.land.naver.com/api' in response.url:
                    try:
                        response_data = {
                            'url': response.url,
                            'status': response.status,
                            'headers': dict(response.headers),
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        # JSON ì‘ë‹µ íŒŒì‹±
                        try:
                            if 'application/json' in response.headers.get('content-type', ''):
                                body = response.json()
                                response_data['body'] = body
                                
                                # articles API ì‘ë‹µ ë¶„ì„
                                if '/api/articles?' in response.url and 'articleList' in body:
                                    article_count = len(body['articleList'])
                                    total_count = body.get('articleCount', 0)
                                    
                                    # page íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                                    import re
                                    page_match = re.search(r'page=(\d+)', response.url)
                                    page_num = page_match.group(1) if page_match else '1'
                                    
                                    print(f"âœ… í˜ì´ì§€ {page_num} ì‘ë‹µ: {article_count}ê°œ ë§¤ë¬¼ (ì „ì²´: {total_count})")
                        except:
                            response_data['body'] = 'Failed to parse JSON'
                        
                        self.captured_responses.append(response_data)
                        
                    except Exception as e:
                        print(f"âŒ ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            page.on('request', handle_request)
            page.on('response', handle_response)
            
            try:
                print("\nğŸŒ í˜ì´ì§€ ë¡œë”© ì¤‘...")
                page.goto(url, wait_until="networkidle")
                time.sleep(3)
                
                # ë§¤ë¬¼ ëª©ë¡ ë¡œë”© ëŒ€ê¸°
                print("ğŸ“‹ ë§¤ë¬¼ ëª©ë¡ ë¡œë”© ëŒ€ê¸°...")
                try:
                    page.wait_for_selector(".item_link, .article_item, .list_item", timeout=10000)
                    print("âœ… ë§¤ë¬¼ ëª©ë¡ ë¡œë“œ ì™„ë£Œ")
                except:
                    print("âš ï¸ ë§¤ë¬¼ ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨, ê³„ì† ì§„í–‰...")
                
                # ì´ˆê¸° ë§¤ë¬¼ ìˆ˜ í™•ì¸
                initial_items = page.query_selector_all(".item_link, .article_item, .list_item")
                print(f"ğŸ“Š ì´ˆê¸° ë§¤ë¬¼ ìˆ˜: {len(initial_items)}ê°œ")
                
                # ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ì¶”ê°€ ë§¤ë¬¼ ë¡œë”© í…ŒìŠ¤íŠ¸
                scroll_count = 0
                max_scrolls = 10  # ìµœëŒ€ 10ë²ˆ ìŠ¤í¬ë¡¤
                
                print(f"\nğŸ“œ ìŠ¤í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ìµœëŒ€ {max_scrolls}ë²ˆ)...")
                
                for i in range(max_scrolls):
                    scroll_count += 1
                    print(f"\nğŸ”„ ìŠ¤í¬ë¡¤ {scroll_count}ë²ˆì§¸...")
                    
                    # í˜„ì¬ ë§¤ë¬¼ ìˆ˜ í™•ì¸
                    before_items = page.query_selector_all(".item_link, .article_item, .list_item")
                    before_count = len(before_items)
                    
                    # í˜ì´ì§€ ë§¨ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    
                    # ë¡œë”© ëŒ€ê¸°
                    time.sleep(3)
                    
                    # ìŠ¤í¬ë¡¤ í›„ ë§¤ë¬¼ ìˆ˜ í™•ì¸
                    after_items = page.query_selector_all(".item_link, .article_item, .list_item")
                    after_count = len(after_items)
                    
                    added_count = after_count - before_count
                    print(f"ğŸ“Š ìŠ¤í¬ë¡¤ í›„: {before_count} â†’ {after_count} ({added_count:+d}ê°œ)")
                    
                    # ë” ì´ìƒ ë§¤ë¬¼ì´ ì¶”ê°€ë˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨
                    if added_count == 0:
                        print("ğŸ“„ ë” ì´ìƒ ë¡œë”©í•  ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    # ì ì‹œ ëŒ€ê¸°
                    time.sleep(2)
                
                # ìµœì¢… ë§¤ë¬¼ ìˆ˜
                final_items = page.query_selector_all(".item_link, .article_item, .list_item")
                print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼: {len(initial_items)} â†’ {len(final_items)} ({len(final_items) - len(initial_items):+d}ê°œ ì¶”ê°€)")
                
                # ì¶”ê°€ ìŠ¤í¬ë¡¤ í…ŒìŠ¤íŠ¸ (ë” ë§ì€ ë°ì´í„° í™•ì¸)
                print(f"\nğŸ”„ ì¶”ê°€ ìŠ¤í¬ë¡¤ í…ŒìŠ¤íŠ¸...")
                for i in range(5):
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(2)
                
                print("ğŸ“Š ë¶„ì„ ì™„ë£Œ!")
                
            except Exception as e:
                print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                time.sleep(2)  # ë§ˆì§€ë§‰ ì‘ë‹µ ëŒ€ê¸°
                browser.close()
    
    def save_scroll_analysis(self):
        """ìŠ¤í¬ë¡¤ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # articles API ìš”ì²­ë§Œ í•„í„°ë§
        articles_requests = [req for req in self.captured_requests 
                           if '/api/articles?' in req['url']]
        
        # í˜ì´ì§€ë³„ ìš”ì²­ ë¶„ì„
        page_analysis = {}
        for req in self.page_requests:
            page_num = req['page']
            if page_num not in page_analysis:
                page_analysis[page_num] = []
            page_analysis[page_num].append(req)
            
        analysis = {
            "ë¶„ì„ì‹œê°„": timestamp,
            "ì´_API_ìš”ì²­ìˆ˜": len(self.captured_requests),
            "articles_API_ìš”ì²­ìˆ˜": len(articles_requests),
            "í˜ì´ì§€ë³„_ìš”ì²­ìˆ˜": {page: len(reqs) for page, reqs in page_analysis.items()},
            "í˜ì´ì§€ë³„_ë¶„ì„": page_analysis,
            "ì „ì²´_ìš”ì²­": self.captured_requests,
            "ì „ì²´_ì‘ë‹µ": self.captured_responses
        }
        
        filename = f"scroll_analysis_{timestamp}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ìŠ¤í¬ë¡¤ ë¶„ì„ ê²°ê³¼ ì €ì¥: {filename}")
        return filename
    
    def print_scroll_summary(self):
        """ìŠ¤í¬ë¡¤ ë¶„ì„ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ìŠ¤í¬ë¡¤ë§ ë¬´í•œë¡œë”© ë¶„ì„ ìš”ì•½")
        print("="*60)
        
        # articles API ìš”ì²­ í†µê³„
        articles_requests = [req for req in self.captured_requests 
                           if '/api/articles?' in req['url']]
        
        print(f"ğŸ“„ ì „ì²´ API ìš”ì²­: {len(self.captured_requests)}ê°œ")
        print(f"ğŸ“‹ ë§¤ë¬¼ ëª©ë¡ ìš”ì²­: {len(articles_requests)}ê°œ")
        
        # í˜ì´ì§€ë³„ ë¶„ì„
        if self.page_requests:
            print(f"\nğŸ“„ í˜ì´ì§€ë³„ ìš”ì²­ íŒ¨í„´:")
            page_counts = {}
            for req in self.page_requests:
                page = req['page']
                page_counts[page] = page_counts.get(page, 0) + 1
            
            for page, count in sorted(page_counts.items(), key=lambda x: int(x[0]) if x[0].isdigit() else 999):
                print(f"  - í˜ì´ì§€ {page}: {count}íšŒ")
        
        # ë¬´í•œë¡œë”© íŒ¨í„´ ë¶„ì„
        page_params = set()
        for req in articles_requests:
            if 'page=' in req['url']:
                import re
                page_match = re.search(r'page=(\d+)', req['url'])
                if page_match:
                    page_params.add(int(page_match.group(1)))
        
        if page_params:
            max_page = max(page_params)
            print(f"\nğŸ”„ ìŠ¤í¬ë¡¤ë§ ê²°ê³¼:")
            print(f"  - ìµœëŒ€ í˜ì´ì§€: {max_page}")
            print(f"  - ì´ í˜ì´ì§€ ìˆ˜: {len(page_params)}")
            if max_page > 1:
                print(f"  - ë¬´í•œë¡œë”© íŒ¨í„´: âœ… í™•ì¸ë¨")
            else:
                print(f"  - ë¬´í•œë¡œë”© íŒ¨í„´: âŒ ë¯¸í™•ì¸")

def main():
    analyzer = ScrollLoadingAnalyzer()
    
    # ë¶„ì„í•  URL
    test_url = "https://new.land.naver.com/offices?ms=37.522786,127.0466693,15&a=SG:SMS:GJCG:APTHGJ:GM:TJ&e=RETAIL"
    
    # ìŠ¤í¬ë¡¤ ë¶„ì„ ì‹¤í–‰
    analyzer.analyze_scroll_loading(test_url)
    
    # ê²°ê³¼ ì¶œë ¥
    analyzer.print_scroll_summary()
    
    # ê²°ê³¼ ì €ì¥
    analyzer.save_scroll_analysis()

if __name__ == "__main__":
    main()