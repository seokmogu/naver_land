#!/usr/bin/env python3
"""
Í≥†ÎèÑÌôîÎêú Î°úÍ∑∏ ÏàòÏßë Î∞è Î∂ÑÏÑù Ï≤¥Í≥Ñ
- Íµ¨Ï°∞ÌôîÎêú Î°úÍπÖ (JSON ÌòïÏãù)
- Î°úÍ∑∏ Î†àÎ≤®Î≥Ñ Î∂ÑÎ•ò
- ÏûêÎèô Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Ïù∏ÏÇ¨Ïù¥Ìä∏
- Î°úÍ∑∏ Í∏∞Î∞ò ÏÑ±Îä• ÏµúÏ†ÅÌôî
- ELK Ïä§ÌÉù Ïó∞Îèô Ï§ÄÎπÑ
"""

import json
import logging
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from pathlib import Path
import gzip
import threading
from collections import defaultdict, Counter
from supabase_client import SupabaseHelper

@dataclass
class LogEntry:
    """Íµ¨Ï°∞ÌôîÎêú Î°úÍ∑∏ ÏóîÌä∏Î¶¨"""
    timestamp: str
    level: str
    logger_name: str
    message: str
    context: Dict[str, Any]
    session_id: Optional[str] = None
    user_id: Optional[str] = None
    trace_id: Optional[str] = None

class StructuredLogger:
    """Íµ¨Ï°∞ÌôîÎêú Î°úÍ±∞"""
    
    def __init__(self, name: str, session_id: str = None):
        self.name = name
        self.session_id = session_id or f"session_{int(datetime.now().timestamp())}"
        self.trace_id = None
        
        # Î°úÍ∑∏ ÌååÏùº ÏÑ§Ï†ï
        self.log_dir = Path("logs")
        self.log_dir.mkdir(exist_ok=True)
        
        # ÏùºÎ≥Ñ Î°úÍ∑∏ ÌååÏùº
        today = datetime.now().strftime("%Y%m%d")
        self.log_file = self.log_dir / f"collector_{today}.jsonl"
        self.error_log_file = self.log_dir / f"errors_{today}.jsonl"
        
        # Î°úÍ∑∏ Î≤ÑÌçº (Î∞∞Ïπò Ï≤òÎ¶¨Ïö©)
        self.log_buffer = []
        self.buffer_lock = threading.Lock()
        self.buffer_size = 100
        
        # Í∏∞Î≥∏ Python Î°úÍ±∞ÎèÑ ÏÑ§Ï†ï (Ìò∏ÌôòÏÑ±)
        self._setup_python_logger()
    
    def _setup_python_logger(self):
        """Í∏∞Î≥∏ Python Î°úÍ±∞ ÏÑ§Ï†ï"""
        self.python_logger = logging.getLogger(self.name)
        self.python_logger.setLevel(logging.DEBUG)
        
        # ÏΩòÏÜî Ìï∏Îì§Îü¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # Ìè¨Îß∑ÌÑ∞
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)
        
        if not self.python_logger.handlers:
            self.python_logger.addHandler(console_handler)
    
    def _create_log_entry(self, level: str, message: str, **context) -> LogEntry:
        """Î°úÍ∑∏ ÏóîÌä∏Î¶¨ ÏÉùÏÑ±"""
        return LogEntry(
            timestamp=datetime.now().isoformat(),
            level=level,
            logger_name=self.name,
            message=message,
            context=context,
            session_id=self.session_id,
            trace_id=self.trace_id
        )
    
    def _write_log(self, entry: LogEntry):
        """Î°úÍ∑∏ ÌååÏùºÏóê Í∏∞Î°ù"""
        with self.buffer_lock:
            self.log_buffer.append(entry)
            
            # Î≤ÑÌçºÍ∞Ä Í∞ÄÎìù Ï∞®Î©¥ ÌîåÎü¨Ïãú
            if len(self.log_buffer) >= self.buffer_size:
                self._flush_buffer()
    
    def _flush_buffer(self):
        """Î≤ÑÌçºÎ•º ÌååÏùºÏóê ÌîåÎü¨Ïãú"""
        if not self.log_buffer:
            return
        
        # ÏùºÎ∞ò Î°úÍ∑∏ÏôÄ ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÎ¶¨
        normal_logs = []
        error_logs = []
        
        for entry in self.log_buffer:
            if entry.level in ['ERROR', 'CRITICAL']:
                error_logs.append(entry)
            normal_logs.append(entry)
        
        # ÌååÏùºÏóê Í∏∞Î°ù
        if normal_logs:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                for entry in normal_logs:
                    f.write(json.dumps(asdict(entry), ensure_ascii=False) + '\n')
        
        if error_logs:
            with open(self.error_log_file, 'a', encoding='utf-8') as f:
                for entry in error_logs:
                    f.write(json.dumps(asdict(entry), ensure_ascii=False) + '\n')
        
        self.log_buffer.clear()
    
    def info(self, message: str, **context):
        """Ï†ïÎ≥¥ Î°úÍ∑∏"""
        entry = self._create_log_entry('INFO', message, **context)
        self._write_log(entry)
        self.python_logger.info(message)
    
    def error(self, message: str, **context):
        """ÏóêÎü¨ Î°úÍ∑∏"""
        entry = self._create_log_entry('ERROR', message, **context)
        self._write_log(entry)
        self.python_logger.error(message)
    
    def warning(self, message: str, **context):
        """Í≤ΩÍ≥† Î°úÍ∑∏"""
        entry = self._create_log_entry('WARNING', message, **context)
        self._write_log(entry)
        self.python_logger.warning(message)
    
    def debug(self, message: str, **context):
        """ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏"""
        entry = self._create_log_entry('DEBUG', message, **context)
        self._write_log(entry)
        self.python_logger.debug(message)
    
    def critical(self, message: str, **context):
        """Ï§ëÏöî Î°úÍ∑∏"""
        entry = self._create_log_entry('CRITICAL', message, **context)
        self._write_log(entry)
        self.python_logger.critical(message)
    
    def collection_start(self, cortar_no: str, max_pages: int):
        """ÏàòÏßë ÏãúÏûë Î°úÍ∑∏"""
        self.info("Collection started", 
                 event_type="collection_start",
                 cortar_no=cortar_no,
                 max_pages=max_pages,
                 start_time=datetime.now().isoformat())
    
    def collection_page(self, cortar_no: str, page: int, properties_count: int):
        """ÌéòÏù¥ÏßÄ ÏàòÏßë Î°úÍ∑∏"""
        self.info("Page collected",
                 event_type="page_collected",
                 cortar_no=cortar_no,
                 page=page,
                 properties_count=properties_count)
    
    def collection_complete(self, cortar_no: str, total_collected: int, duration: float):
        """ÏàòÏßë ÏôÑÎ£å Î°úÍ∑∏"""
        self.info("Collection completed",
                 event_type="collection_complete",
                 cortar_no=cortar_no,
                 total_collected=total_collected,
                 duration_seconds=duration,
                 collection_rate=total_collected/duration if duration > 0 else 0)
    
    def data_quality_check(self, total_records: int, quality_issues: Dict):
        """Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Ï≤¥ÌÅ¨ Î°úÍ∑∏"""
        self.info("Data quality checked",
                 event_type="quality_check",
                 total_records=total_records,
                 quality_issues=quality_issues)
    
    def database_operation(self, operation: str, table: str, record_count: int, 
                          duration: float):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏûëÏóÖ Î°úÍ∑∏"""
        self.info("Database operation",
                 event_type="db_operation", 
                 operation=operation,
                 table=table,
                 record_count=record_count,
                 duration_seconds=duration)
    
    def performance_metric(self, metric_name: str, metric_value: float, unit: str):
        """ÏÑ±Îä• Î©îÌä∏Î¶≠ Î°úÍ∑∏"""
        self.info("Performance metric",
                 event_type="performance_metric",
                 metric_name=metric_name,
                 metric_value=metric_value,
                 unit=unit)
    
    def flush(self):
        """Î≤ÑÌçº Í∞ïÏ†ú ÌîåÎü¨Ïãú"""
        with self.buffer_lock:
            self._flush_buffer()
    
    def __del__(self):
        """ÏÜåÎ©∏Ïûê - Î≤ÑÌçº ÌîåÎü¨Ïãú"""
        try:
            self.flush()
        except:
            pass

class LogAnalyzer:
    """Î°úÍ∑∏ Î∂ÑÏÑùÍ∏∞"""
    
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        self.helper = SupabaseHelper()
    
    def analyze_daily_logs(self, target_date: str = None) -> Dict:
        """ÏùºÎ≥Ñ Î°úÍ∑∏ Î∂ÑÏÑù"""
        if not target_date:
            target_date = datetime.now().strftime("%Y%m%d")
        
        print(f"üìä {target_date} Î°úÍ∑∏ Î∂ÑÏÑù ÏãúÏûë")
        
        log_file = self.log_dir / f"collector_{target_date}.jsonl"
        error_file = self.log_dir / f"errors_{target_date}.jsonl"
        
        analysis = {
            'date': target_date,
            'summary': {},
            'collection_performance': {},
            'error_analysis': {},
            'quality_metrics': {},
            'recommendations': []
        }
        
        # Í∏∞Î≥∏ Î°úÍ∑∏ Î∂ÑÏÑù
        if log_file.exists():
            analysis['summary'] = self._analyze_general_logs(log_file)
            analysis['collection_performance'] = self._analyze_collection_performance(log_file)
            analysis['quality_metrics'] = self._analyze_quality_metrics(log_file)
        
        # ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù  
        if error_file.exists():
            analysis['error_analysis'] = self._analyze_error_logs(error_file)
        
        # Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±
        analysis['recommendations'] = self._generate_recommendations(analysis)
        
        self._print_analysis_report(analysis)
        return analysis
    
    def _analyze_general_logs(self, log_file: Path) -> Dict:
        """ÏùºÎ∞ò Î°úÍ∑∏ Î∂ÑÏÑù"""
        log_counts = defaultdict(int)
        event_types = defaultdict(int)
        sessions = set()
        loggers = defaultdict(int)
        
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    log_counts[entry['level']] += 1
                    
                    if 'event_type' in entry['context']:
                        event_types[entry['context']['event_type']] += 1
                    
                    if entry['session_id']:
                        sessions.add(entry['session_id'])
                    
                    loggers[entry['logger_name']] += 1
                    
                except json.JSONDecodeError:
                    continue
        
        return {
            'total_logs': sum(log_counts.values()),
            'log_level_distribution': dict(log_counts),
            'event_type_distribution': dict(event_types),
            'session_count': len(sessions),
            'logger_distribution': dict(loggers)
        }
    
    def _analyze_collection_performance(self, log_file: Path) -> Dict:
        """ÏàòÏßë ÏÑ±Îä• Î∂ÑÏÑù"""
        collection_sessions = {}
        page_collections = []
        completion_times = []
        
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    context = entry['context']
                    event_type = context.get('event_type')
                    
                    if event_type == 'collection_start':
                        session_id = entry['session_id']
                        collection_sessions[session_id] = {
                            'cortar_no': context.get('cortar_no'),
                            'start_time': entry['timestamp'],
                            'max_pages': context.get('max_pages')
                        }
                    
                    elif event_type == 'page_collected':
                        page_collections.append({
                            'properties_count': context.get('properties_count', 0),
                            'page': context.get('page', 0)
                        })
                    
                    elif event_type == 'collection_complete':
                        completion_times.append({
                            'duration': context.get('duration_seconds', 0),
                            'total_collected': context.get('total_collected', 0),
                            'collection_rate': context.get('collection_rate', 0)
                        })
                
                except (json.JSONDecodeError, KeyError):
                    continue
        
        # ÏÑ±Îä• Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
        avg_collection_rate = 0
        avg_page_properties = 0
        
        if completion_times:
            avg_collection_rate = sum(c['collection_rate'] for c in completion_times) / len(completion_times)
        
        if page_collections:
            avg_page_properties = sum(p['properties_count'] for p in page_collections) / len(page_collections)
        
        return {
            'total_sessions': len(collection_sessions),
            'total_pages_collected': len(page_collections),
            'total_completions': len(completion_times),
            'avg_collection_rate': round(avg_collection_rate, 2),
            'avg_properties_per_page': round(avg_page_properties, 1),
            'completion_rate': (len(completion_times) / len(collection_sessions) * 100) if collection_sessions else 0
        }
    
    def _analyze_error_logs(self, error_file: Path) -> Dict:
        """ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù"""
        error_types = defaultdict(int)
        error_messages = Counter()
        error_contexts = []
        
        with open(error_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    error_types[entry['level']] += 1
                    error_messages[entry['message']] += 1
                    
                    # Ïª®ÌÖçÏä§Ìä∏ Ï†ïÎ≥¥ ÏàòÏßë
                    if 'cortar_no' in entry['context']:
                        error_contexts.append({
                            'cortar_no': entry['context']['cortar_no'],
                            'message': entry['message'],
                            'timestamp': entry['timestamp']
                        })
                
                except json.JSONDecodeError:
                    continue
        
        # Í∞ÄÏû• ÎπàÎ≤àÌïú ÏóêÎü¨Îì§
        top_errors = error_messages.most_common(5)
        
        return {
            'total_errors': sum(error_types.values()),
            'error_level_distribution': dict(error_types),
            'top_error_messages': [{'message': msg, 'count': count} for msg, count in top_errors],
            'errors_by_region': self._group_errors_by_region(error_contexts)
        }
    
    def _group_errors_by_region(self, error_contexts: List[Dict]) -> Dict:
        """ÏßÄÏó≠Î≥Ñ ÏóêÎü¨ Í∑∏Î£πÌïë"""
        region_errors = defaultdict(int)
        for error in error_contexts:
            if error['cortar_no']:
                region_errors[error['cortar_no']] += 1
        
        return dict(region_errors)
    
    def _analyze_quality_metrics(self, log_file: Path) -> Dict:
        """ÌíàÏßà Î©îÌä∏Î¶≠ Î∂ÑÏÑù"""
        quality_checks = []
        
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    context = entry['context']
                    
                    if context.get('event_type') == 'quality_check':
                        quality_checks.append({
                            'total_records': context.get('total_records', 0),
                            'quality_issues': context.get('quality_issues', {})
                        })
                
                except (json.JSONDecodeError, KeyError):
                    continue
        
        if not quality_checks:
            return {}
        
        # ÌèâÍ∑† ÌíàÏßà Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
        total_records = sum(q['total_records'] for q in quality_checks)
        
        # ÌíàÏßà Ïù¥Ïäà ÏßëÍ≥Ñ
        aggregated_issues = defaultdict(int)
        for check in quality_checks:
            for issue_type, count in check['quality_issues'].items():
                aggregated_issues[issue_type] += count
        
        return {
            'total_quality_checks': len(quality_checks),
            'total_records_checked': total_records,
            'quality_issue_summary': dict(aggregated_issues),
            'avg_records_per_check': round(total_records / len(quality_checks), 1) if quality_checks else 0
        }
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """Î∂ÑÏÑù Í≤∞Í≥º Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±"""
        recommendations = []
        
        # ÏóêÎü¨ Î∂ÑÏÑù Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠
        error_analysis = analysis.get('error_analysis', {})
        if error_analysis.get('total_errors', 0) > 10:
            recommendations.append("üö® ÏóêÎü¨ Î∞úÏÉùÎüâÏù¥ ÎÜíÏäµÎãàÎã§. Ï£ºÏöî ÏóêÎü¨ Ìå®ÌÑ¥ÏùÑ Ï†êÍ≤ÄÌïòÏÑ∏Ïöî.")
        
        # ÏÑ±Îä• Î∂ÑÏÑù Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠
        performance = analysis.get('collection_performance', {})
        completion_rate = performance.get('completion_rate', 0)
        if completion_rate < 90:
            recommendations.append(f"‚ö†Ô∏è ÏàòÏßë ÏôÑÎ£åÏú®Ïù¥ {completion_rate:.1f}%ÏûÖÎãàÎã§. ÏïàÏ†ïÏÑ± Í∞úÏÑ†Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.")
        
        avg_rate = performance.get('avg_collection_rate', 0)
        if avg_rate < 5:
            recommendations.append(f"üêå ÌèâÍ∑† ÏàòÏßë ÏÜçÎèÑÍ∞Ä {avg_rate:.1f}Í±¥/Ï¥àÎ°ú ÎÇÆÏäµÎãàÎã§. ÏÑ±Îä• ÏµúÏ†ÅÌôîÎ•º Í≥†Î†§ÌïòÏÑ∏Ïöî.")
        
        # ÌíàÏßà Î∂ÑÏÑù Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠
        quality = analysis.get('quality_metrics', {})
        quality_issues = quality.get('quality_issue_summary', {})
        if quality_issues:
            major_issue = max(quality_issues, key=quality_issues.get)
            recommendations.append(f"üìä {major_issue} ÌíàÏßà Ïù¥ÏäàÍ∞Ä {quality_issues[major_issue]}Í±¥ Î∞úÍ≤¨ÎêòÏóàÏäµÎãàÎã§.")
        
        if not recommendations:
            recommendations.append("‚úÖ ÏãúÏä§ÌÖúÏù¥ Ï†ïÏÉÅÏ†ÅÏúºÎ°ú Ïö¥ÏòÅÎêòÍ≥† ÏûàÏäµÎãàÎã§.")
        
        return recommendations
    
    def _print_analysis_report(self, analysis: Dict):
        """Î∂ÑÏÑù Î≥¥Í≥†ÏÑú Ï∂úÎ†•"""
        print(f"\nüìã {analysis['date']} Î°úÍ∑∏ Î∂ÑÏÑù Î≥¥Í≥†ÏÑú")
        print("=" * 60)
        
        # ÏöîÏïΩ Ï†ïÎ≥¥
        summary = analysis.get('summary', {})
        if summary:
            print(f"üìä Î°úÍ∑∏ ÏöîÏïΩ:")
            print(f"   Ï¥ù Î°úÍ∑∏: {summary.get('total_logs', 0):,}Í∞ú")
            print(f"   ÏÑ∏ÏÖòÏàò: {summary.get('session_count', 0)}Í∞ú")
            
            level_dist = summary.get('log_level_distribution', {})
            for level, count in level_dist.items():
                print(f"   {level}: {count:,}Í∞ú")
        
        # ÏàòÏßë ÏÑ±Îä•
        performance = analysis.get('collection_performance', {})
        if performance:
            print(f"\n‚ö° ÏàòÏßë ÏÑ±Îä•:")
            print(f"   ÏôÑÎ£åÏú®: {performance.get('completion_rate', 0):.1f}%")
            print(f"   ÌèâÍ∑† ÏàòÏßëÏÜçÎèÑ: {performance.get('avg_collection_rate', 0):.2f}Í±¥/Ï¥à")
            print(f"   ÌéòÏù¥ÏßÄÎãπ Îß§Î¨º: {performance.get('avg_properties_per_page', 0):.1f}Í∞ú")
        
        # ÏóêÎü¨ Î∂ÑÏÑù
        error_analysis = analysis.get('error_analysis', {})
        if error_analysis and error_analysis.get('total_errors', 0) > 0:
            print(f"\n‚ùå ÏóêÎü¨ Î∂ÑÏÑù:")
            print(f"   Ï¥ù ÏóêÎü¨: {error_analysis.get('total_errors', 0)}Í∞ú")
            
            top_errors = error_analysis.get('top_error_messages', [])
            if top_errors:
                print(f"   Ï£ºÏöî ÏóêÎü¨:")
                for error in top_errors[:3]:
                    print(f"     ‚Ä¢ {error['message'][:50]}... ({error['count']}Ìöå)")
        
        # Í∂åÏû•ÏÇ¨Ìï≠
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            print(f"\nüí° Í∂åÏû•ÏÇ¨Ìï≠:")
            for rec in recommendations:
                print(f"   {rec}")
        
        print("=" * 60)
    
    def create_log_insights_dashboard(self, days: int = 7) -> Dict:
        """Î°úÍ∑∏ Ïù∏ÏÇ¨Ïù¥Ìä∏ ÎåÄÏãúÎ≥¥Îìú Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±"""
        print(f"üìà ÏµúÍ∑º {days}Ïùº Î°úÍ∑∏ Ïù∏ÏÇ¨Ïù¥Ìä∏ ÎåÄÏãúÎ≥¥Îìú ÏÉùÏÑ±")
        
        insights = {
            'period_days': days,
            'daily_summaries': [],
            'trends': {},
            'anomalies': [],
            'performance_insights': {}
        }
        
        # ÏùºÎ≥Ñ Î∂ÑÏÑù ÏàòÌñâ
        for i in range(days):
            target_date = (datetime.now() - timedelta(days=i)).strftime("%Y%m%d")
            daily_analysis = self.analyze_daily_logs(target_date)
            
            # ÏöîÏïΩ Ï†ïÎ≥¥Îßå Ï†ÄÏû•
            daily_summary = {
                'date': target_date,
                'total_logs': daily_analysis.get('summary', {}).get('total_logs', 0),
                'error_count': daily_analysis.get('error_analysis', {}).get('total_errors', 0),
                'collection_rate': daily_analysis.get('collection_performance', {}).get('avg_collection_rate', 0),
                'completion_rate': daily_analysis.get('collection_performance', {}).get('completion_rate', 0)
            }
            insights['daily_summaries'].append(daily_summary)
        
        # Ìä∏Î†åÎìú Î∂ÑÏÑù
        insights['trends'] = self._analyze_trends(insights['daily_summaries'])
        
        # Ïù¥ÏÉÅ ÏßïÌõÑ ÌÉêÏßÄ
        insights['anomalies'] = self._detect_anomalies(insights['daily_summaries'])
        
        # ÏÑ±Îä• Ïù∏ÏÇ¨Ïù¥Ìä∏
        insights['performance_insights'] = self._generate_performance_insights(insights['daily_summaries'])
        
        return insights
    
    def _analyze_trends(self, daily_summaries: List[Dict]) -> Dict:
        """Ìä∏Î†åÎìú Î∂ÑÏÑù"""
        if len(daily_summaries) < 3:
            return {}
        
        # ÏµúÍ∑º 3Ïùº vs Ïù¥Ï†Ñ Í∏∞Í∞Ñ ÎπÑÍµê
        recent = daily_summaries[:3]
        previous = daily_summaries[3:6] if len(daily_summaries) >= 6 else daily_summaries[3:]
        
        if not previous:
            return {}
        
        recent_avg_rate = sum(d['collection_rate'] for d in recent) / len(recent)
        previous_avg_rate = sum(d['collection_rate'] for d in previous) / len(previous)
        
        rate_change = ((recent_avg_rate - previous_avg_rate) / previous_avg_rate * 100) if previous_avg_rate > 0 else 0
        
        recent_avg_errors = sum(d['error_count'] for d in recent) / len(recent)
        previous_avg_errors = sum(d['error_count'] for d in previous) / len(previous)
        
        error_change = ((recent_avg_errors - previous_avg_errors) / previous_avg_errors * 100) if previous_avg_errors > 0 else 0
        
        return {
            'collection_rate_change_percent': round(rate_change, 1),
            'error_rate_change_percent': round(error_change, 1),
            'trend_direction': 'improving' if rate_change > 5 and error_change < -10 else 'declining' if rate_change < -5 or error_change > 20 else 'stable'
        }
    
    def _detect_anomalies(self, daily_summaries: List[Dict]) -> List[Dict]:
        """Ïù¥ÏÉÅ ÏßïÌõÑ ÌÉêÏßÄ"""
        anomalies = []
        
        if len(daily_summaries) < 3:
            return anomalies
        
        # ÏóêÎü¨ Í∏âÏ¶ù Í∞êÏßÄ
        error_counts = [d['error_count'] for d in daily_summaries]
        avg_errors = sum(error_counts) / len(error_counts)
        
        for i, summary in enumerate(daily_summaries):
            if summary['error_count'] > avg_errors * 3:  # ÌèâÍ∑†Ïùò 3Î∞∞ Ïù¥ÏÉÅ
                anomalies.append({
                    'type': 'error_spike',
                    'date': summary['date'],
                    'value': summary['error_count'],
                    'threshold': avg_errors * 3,
                    'severity': 'high'
                })
        
        # ÏàòÏßë ÏÑ±Îä• Í∏âÎùΩ Í∞êÏßÄ
        collection_rates = [d['collection_rate'] for d in daily_summaries if d['collection_rate'] > 0]
        if collection_rates:
            avg_rate = sum(collection_rates) / len(collection_rates)
            
            for summary in daily_summaries:
                if summary['collection_rate'] < avg_rate * 0.5:  # ÌèâÍ∑†Ïùò 50% ÎØ∏Îßå
                    anomalies.append({
                        'type': 'performance_drop',
                        'date': summary['date'],
                        'value': summary['collection_rate'],
                        'threshold': avg_rate * 0.5,
                        'severity': 'medium'
                    })
        
        return anomalies
    
    def _generate_performance_insights(self, daily_summaries: List[Dict]) -> Dict:
        """ÏÑ±Îä• Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±"""
        if not daily_summaries:
            return {}
        
        # ÏµúÍ≥†/ÏµúÏ†Ä ÏÑ±Îä•Ïùº Ï∞æÍ∏∞
        best_day = max(daily_summaries, key=lambda x: x['collection_rate'])
        worst_day = min(daily_summaries, key=lambda x: x['collection_rate']) if daily_summaries else best_day
        
        # ÌèâÍ∑† ÏÑ±Îä•
        avg_rate = sum(d['collection_rate'] for d in daily_summaries) / len(daily_summaries)
        avg_completion = sum(d['completion_rate'] for d in daily_summaries) / len(daily_summaries)
        
        return {
            'avg_collection_rate': round(avg_rate, 2),
            'avg_completion_rate': round(avg_completion, 1),
            'best_performance_date': best_day['date'],
            'best_performance_rate': best_day['collection_rate'],
            'worst_performance_date': worst_day['date'],
            'worst_performance_rate': worst_day['collection_rate'],
            'performance_consistency': self._calculate_consistency(daily_summaries)
        }
    
    def _calculate_consistency(self, daily_summaries: List[Dict]) -> str:
        """ÏÑ±Îä• ÏùºÍ¥ÄÏÑ± Í≥ÑÏÇ∞"""
        rates = [d['collection_rate'] for d in daily_summaries if d['collection_rate'] > 0]
        if len(rates) < 2:
            return 'insufficient_data'
        
        avg = sum(rates) / len(rates)
        variance = sum((r - avg) ** 2 for r in rates) / len(rates)
        std_dev = variance ** 0.5
        
        cv = (std_dev / avg) * 100 if avg > 0 else 0  # Î≥ÄÎèôÍ≥ÑÏàò
        
        if cv < 20:
            return 'highly_consistent'
        elif cv < 40:
            return 'moderately_consistent'
        else:
            return 'inconsistent'

def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Í≥†ÎèÑÌôîÎêú Î°úÍ∑∏ Î∂ÑÏÑù ÏãúÏä§ÌÖú')
    parser.add_argument('--analyze', action='store_true', help='Ïò§Îäò Î°úÍ∑∏ Î∂ÑÏÑù')
    parser.add_argument('--date', type=str, help='Î∂ÑÏÑùÌï† ÎÇ†Ïßú (YYYYMMDD)')
    parser.add_argument('--dashboard', type=int, default=7, help='ÎåÄÏãúÎ≥¥Îìú Í∏∞Í∞Ñ (Ïùº)')
    parser.add_argument('--compress-old', action='store_true', help='Ïò§ÎûòÎêú Î°úÍ∑∏ ÏïïÏ∂ï')
    
    args = parser.parse_args()
    
    print("üìä Í≥†ÎèÑÌôîÎêú Î°úÍ∑∏ Î∂ÑÏÑù ÏãúÏä§ÌÖú v2.0")
    print("=" * 50)
    
    analyzer = LogAnalyzer()
    
    if args.analyze:
        analysis = analyzer.analyze_daily_logs(args.date)
        
        # Í≤∞Í≥º Ï†ÄÏû•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = f"log_analysis_{timestamp}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"üìÑ Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•: {result_file}")
    
    elif args.dashboard:
        insights = analyzer.create_log_insights_dashboard(args.dashboard)
        
        # ÎåÄÏãúÎ≥¥Îìú Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
        with open('log_insights_dashboard.json', 'w', encoding='utf-8') as f:
            json.dump(insights, f, ensure_ascii=False, indent=2)
        
        print("üìä ÎåÄÏãúÎ≥¥Îìú Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ÏôÑÎ£å: log_insights_dashboard.json")
    
    if args.compress_old:
        # 7Ïùº Ïù¥Ï†Ñ Î°úÍ∑∏ ÏïïÏ∂ï
        log_dir = Path("logs")
        cutoff_date = datetime.now() - timedelta(days=7)
        
        for log_file in log_dir.glob("*.jsonl"):
            # ÌååÏùºÎ™ÖÏóêÏÑú ÎÇ†Ïßú Ï∂îÏ∂ú
            try:
                file_date_str = log_file.stem.split('_')[-1]
                file_date = datetime.strptime(file_date_str, "%Y%m%d")
                
                if file_date < cutoff_date:
                    # ÏïïÏ∂ï
                    with open(log_file, 'rb') as f_in:
                        with gzip.open(f"{log_file}.gz", 'wb') as f_out:
                            f_out.writelines(f_in)
                    
                    # ÏõêÎ≥∏ ÏÇ≠Ï†ú
                    log_file.unlink()
                    print(f"üì¶ {log_file.name} ÏïïÏ∂ï ÏôÑÎ£å")
            
            except (ValueError, IndexError):
                continue
        
        print("‚úÖ Ïò§ÎûòÎêú Î°úÍ∑∏ ÏïïÏ∂ï ÏôÑÎ£å")

if __name__ == "__main__":
    main()