#!/usr/bin/env python3
"""
ê³ ë„í™”ëœ ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„ ì²´ê³„
- êµ¬ì¡°í™”ëœ ë¡œê¹… (JSON í˜•ì‹)
- ë¡œê·¸ ë ˆë²¨ë³„ ë¶„ë¥˜
- ìë™ ë¡œê·¸ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸
- ë¡œê·¸ ê¸°ë°˜ ì„±ëŠ¥ ìµœì í™”
- ELK ìŠ¤íƒ ì—°ë™ ì¤€ë¹„
"""

import json
import logging
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from pathlib import Path
import gzip
import threading
from collections import defaultdict, Counter
from supabase_client import SupabaseHelper

@dataclass
class LogEntry:
    """êµ¬ì¡°í™”ëœ ë¡œê·¸ ì—”íŠ¸ë¦¬"""
    timestamp: str
    level: str
    logger_name: str
    message: str
    context: Dict[str, Any]
    session_id: Optional[str] = None
    user_id: Optional[str] = None
    trace_id: Optional[str] = None

class StructuredLogger:
    """êµ¬ì¡°í™”ëœ ë¡œê±°"""
    
    def __init__(self, name: str, session_id: str = None):
        self.name = name
        self.session_id = session_id or f"session_{int(datetime.now().timestamp())}"
        self.trace_id = None
        
        # ë¡œê·¸ íŒŒì¼ ì„¤ì •
        self.log_dir = Path("logs")
        self.log_dir.mkdir(exist_ok=True)
        
        # ì¼ë³„ ë¡œê·¸ íŒŒì¼
        today = datetime.now().strftime("%Y%m%d")
        self.log_file = self.log_dir / f"collector_{today}.jsonl"
        self.error_log_file = self.log_dir / f"errors_{today}.jsonl"
        
        # ë¡œê·¸ ë²„í¼ (ë°°ì¹˜ ì²˜ë¦¬ìš©)
        self.log_buffer = []
        self.buffer_lock = threading.Lock()
        self.buffer_size = 100
        
        # ê¸°ë³¸ Python ë¡œê±°ë„ ì„¤ì • (í˜¸í™˜ì„±)
        self._setup_python_logger()
    
    def _setup_python_logger(self):
        """ê¸°ë³¸ Python ë¡œê±° ì„¤ì •"""
        self.python_logger = logging.getLogger(self.name)
        self.python_logger.setLevel(logging.DEBUG)
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # í¬ë§·í„°
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)
        
        if not self.python_logger.handlers:
            self.python_logger.addHandler(console_handler)
    
    def _create_log_entry(self, level: str, message: str, **context) -> LogEntry:
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ ìƒì„±"""
        return LogEntry(
            timestamp=datetime.now().isoformat(),
            level=level,
            logger_name=self.name,
            message=message,
            context=context,
            session_id=self.session_id,
            trace_id=self.trace_id
        )
    
    def _write_log(self, entry: LogEntry):
        """ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡"""
        with self.buffer_lock:
            self.log_buffer.append(entry)
            
            # ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ í”ŒëŸ¬ì‹œ
            if len(self.log_buffer) >= self.buffer_size:
                self._flush_buffer()
    
    def _flush_buffer(self):
        """ë²„í¼ë¥¼ íŒŒì¼ì— í”ŒëŸ¬ì‹œ"""
        if not self.log_buffer:
            return
        
        # ì¼ë°˜ ë¡œê·¸ì™€ ì—ëŸ¬ ë¡œê·¸ ë¶„ë¦¬
        normal_logs = []
        error_logs = []
        
        for entry in self.log_buffer:
            if entry.level in ['ERROR', 'CRITICAL']:
                error_logs.append(entry)
            normal_logs.append(entry)
        
        # íŒŒì¼ì— ê¸°ë¡
        if normal_logs:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                for entry in normal_logs:
                    f.write(json.dumps(asdict(entry), ensure_ascii=False) + '\n')
        
        if error_logs:
            with open(self.error_log_file, 'a', encoding='utf-8') as f:
                for entry in error_logs:
                    f.write(json.dumps(asdict(entry), ensure_ascii=False) + '\n')
        
        self.log_buffer.clear()
    
    def info(self, message: str, **context):
        """ì •ë³´ ë¡œê·¸"""
        entry = self._create_log_entry('INFO', message, **context)
        self._write_log(entry)
        self.python_logger.info(message)
    
    def error(self, message: str, **context):
        """ì—ëŸ¬ ë¡œê·¸"""
        entry = self._create_log_entry('ERROR', message, **context)
        self._write_log(entry)
        self.python_logger.error(message)
    
    def warning(self, message: str, **context):
        """ê²½ê³  ë¡œê·¸"""
        entry = self._create_log_entry('WARNING', message, **context)
        self._write_log(entry)
        self.python_logger.warning(message)
    
    def debug(self, message: str, **context):
        """ë””ë²„ê·¸ ë¡œê·¸"""
        entry = self._create_log_entry('DEBUG', message, **context)
        self._write_log(entry)
        self.python_logger.debug(message)
    
    def critical(self, message: str, **context):
        """ì¤‘ìš” ë¡œê·¸"""
        entry = self._create_log_entry('CRITICAL', message, **context)
        self._write_log(entry)
        self.python_logger.critical(message)
    
    def collection_start(self, cortar_no: str, max_pages: int):
        """ìˆ˜ì§‘ ì‹œì‘ ë¡œê·¸"""
        self.info("Collection started", 
                 event_type="collection_start",
                 cortar_no=cortar_no,
                 max_pages=max_pages,
                 start_time=datetime.now().isoformat())
    
    def collection_page(self, cortar_no: str, page: int, properties_count: int):
        """í˜ì´ì§€ ìˆ˜ì§‘ ë¡œê·¸"""
        self.info("Page collected",
                 event_type="page_collected",
                 cortar_no=cortar_no,
                 page=page,
                 properties_count=properties_count)
    
    def collection_complete(self, cortar_no: str, total_collected: int, duration: float):
        """ìˆ˜ì§‘ ì™„ë£Œ ë¡œê·¸"""
        self.info("Collection completed",
                 event_type="collection_complete",
                 cortar_no=cortar_no,
                 total_collected=total_collected,
                 duration_seconds=duration,
                 collection_rate=total_collected/duration if duration > 0 else 0)
    
    def data_quality_check(self, total_records: int, quality_issues: Dict):
        """ë°ì´í„° í’ˆì§ˆ ì²´í¬ ë¡œê·¸"""
        self.info("Data quality checked",
                 event_type="quality_check",
                 total_records=total_records,
                 quality_issues=quality_issues)
    
    def database_operation(self, operation: str, table: str, record_count: int, 
                          duration: float):
        """ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ë¡œê·¸"""
        self.info("Database operation",
                 event_type="db_operation", 
                 operation=operation,
                 table=table,
                 record_count=record_count,
                 duration_seconds=duration)
    
    def performance_metric(self, metric_name: str, metric_value: float, unit: str):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê·¸"""
        self.info("Performance metric",
                 event_type="performance_metric",
                 metric_name=metric_name,
                 metric_value=metric_value,
                 unit=unit)
    
    def flush(self):
        """ë²„í¼ ê°•ì œ í”ŒëŸ¬ì‹œ"""
        with self.buffer_lock:
            self._flush_buffer()
    
    def __del__(self):
        """ì†Œë©¸ì - ë²„í¼ í”ŒëŸ¬ì‹œ"""
        try:
            self.flush()
        except:
            pass

class LogAnalyzer:
    """ë¡œê·¸ ë¶„ì„ê¸°"""
    
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        self.helper = SupabaseHelper()
    
    def analyze_daily_logs(self, target_date: str = None) -> Dict:
        """ì¼ë³„ ë¡œê·¸ ë¶„ì„"""
        if not target_date:
            target_date = datetime.now().strftime("%Y%m%d")
        
        print(f"ğŸ“Š {target_date} ë¡œê·¸ ë¶„ì„ ì‹œì‘")
        
        log_file = self.log_dir / f"collector_{target_date}.jsonl"
        error_file = self.log_dir / f"errors_{target_date}.jsonl"
        
        analysis = {
            'date': target_date,
            'summary': {},
            'collection_performance': {},
            'error_analysis': {},
            'quality_metrics': {},
            'recommendations': []
        }
        
        # ê¸°ë³¸ ë¡œê·¸ ë¶„ì„
        if log_file.exists():
            analysis['summary'] = self._analyze_general_logs(log_file)
            analysis['collection_performance'] = self._analyze_collection_performance(log_file)
            analysis['quality_metrics'] = self._analyze_quality_metrics(log_file)
        
        # ì—ëŸ¬ ë¡œê·¸ ë¶„ì„  
        if error_file.exists():
            analysis['error_analysis'] = self._analyze_error_logs(error_file)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        analysis['recommendations'] = self._generate_recommendations(analysis)
        
        self._print_analysis_report(analysis)
        return analysis
    
    def _analyze_general_logs(self, log_file: Path) -> Dict:
        """ì¼ë°˜ ë¡œê·¸ ë¶„ì„"""
        log_counts = defaultdict(int)
        event_types = defaultdict(int)
        sessions = set()
        loggers = defaultdict(int)
        
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    log_counts[entry['level']] += 1
                    
                    if 'event_type' in entry['context']:
                        event_types[entry['context']['event_type']] += 1
                    
                    if entry['session_id']:
                        sessions.add(entry['session_id'])
                    
                    loggers[entry['logger_name']] += 1
                    
                except json.JSONDecodeError:
                    continue
        
        return {
            'total_logs': sum(log_counts.values()),
            'log_level_distribution': dict(log_counts),
            'event_type_distribution': dict(event_types),
            'session_count': len(sessions),
            'logger_distribution': dict(loggers)
        }
    
    def _analyze_collection_performance(self, log_file: Path) -> Dict:
        """ìˆ˜ì§‘ ì„±ëŠ¥ ë¶„ì„"""
        collection_sessions = {}
        page_collections = []
        completion_times = []
        
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    context = entry['context']
                    event_type = context.get('event_type')
                    
                    if event_type == 'collection_start':
                        session_id = entry['session_id']
                        collection_sessions[session_id] = {
                            'cortar_no': context.get('cortar_no'),
                            'start_time': entry['timestamp'],
                            'max_pages': context.get('max_pages')
                        }
                    
                    elif event_type == 'page_collected':
                        page_collections.append({
                            'properties_count': context.get('properties_count', 0),
                            'page': context.get('page', 0)
                        })
                    
                    elif event_type == 'collection_complete':
                        completion_times.append({
                            'duration': context.get('duration_seconds', 0),
                            'total_collected': context.get('total_collected', 0),
                            'collection_rate': context.get('collection_rate', 0)
                        })
                
                except (json.JSONDecodeError, KeyError):
                    continue
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_collection_rate = 0
        avg_page_properties = 0
        
        if completion_times:
            avg_collection_rate = sum(c['collection_rate'] for c in completion_times) / len(completion_times)
        
        if page_collections:
            avg_page_properties = sum(p['properties_count'] for p in page_collections) / len(page_collections)
        
        return {
            'total_sessions': len(collection_sessions),
            'total_pages_collected': len(page_collections),
            'total_completions': len(completion_times),
            'avg_collection_rate': round(avg_collection_rate, 2),
            'avg_properties_per_page': round(avg_page_properties, 1),
            'completion_rate': (len(completion_times) / len(collection_sessions) * 100) if collection_sessions else 0
        }
    
    def _analyze_error_logs(self, error_file: Path) -> Dict:
        """ì—ëŸ¬ ë¡œê·¸ ë¶„ì„"""
        error_types = defaultdict(int)
        error_messages = Counter()
        error_contexts = []
        
        with open(error_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    error_types[entry['level']] += 1
                    error_messages[entry['message']] += 1
                    
                    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘
                    if 'cortar_no' in entry['context']:
                        error_contexts.append({
                            'cortar_no': entry['context']['cortar_no'],
                            'message': entry['message'],
                            'timestamp': entry['timestamp']
                        })
                
                except json.JSONDecodeError:
                    continue
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ì—ëŸ¬ë“¤
        top_errors = error_messages.most_common(5)
        
        return {
            'total_errors': sum(error_types.values()),
            'error_level_distribution': dict(error_types),
            'top_error_messages': [{'message': msg, 'count': count} for msg, count in top_errors],
            'errors_by_region': self._group_errors_by_region(error_contexts)
        }
    
    def _group_errors_by_region(self, error_contexts: List[Dict]) -> Dict:
        """ì§€ì—­ë³„ ì—ëŸ¬ ê·¸ë£¹í•‘"""
        region_errors = defaultdict(int)
        for error in error_contexts:
            if error['cortar_no']:
                region_errors[error['cortar_no']] += 1
        
        return dict(region_errors)
    
    def _analyze_quality_metrics(self, log_file: Path) -> Dict:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¶„ì„"""
        quality_checks = []
        
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    context = entry['context']
                    
                    if context.get('event_type') == 'quality_check':
                        quality_checks.append({
                            'total_records': context.get('total_records', 0),
                            'quality_issues': context.get('quality_issues', {})
                        })
                
                except (json.JSONDecodeError, KeyError):
                    continue
        
        if not quality_checks:
            return {}
        
        # í‰ê·  í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        total_records = sum(q['total_records'] for q in quality_checks)
        
        # í’ˆì§ˆ ì´ìŠˆ ì§‘ê³„
        aggregated_issues = defaultdict(int)
        for check in quality_checks:
            for issue_type, count in check['quality_issues'].items():
                aggregated_issues[issue_type] += count
        
        return {
            'total_quality_checks': len(quality_checks),
            'total_records_checked': total_records,
            'quality_issue_summary': dict(aggregated_issues),
            'avg_records_per_check': round(total_records / len(quality_checks), 1) if quality_checks else 0
        }
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì—ëŸ¬ ë¶„ì„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        error_analysis = analysis.get('error_analysis', {})
        if error_analysis.get('total_errors', 0) > 10:
            recommendations.append("ğŸš¨ ì—ëŸ¬ ë°œìƒëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ì£¼ìš” ì—ëŸ¬ íŒ¨í„´ì„ ì ê²€í•˜ì„¸ìš”.")
        
        # ì„±ëŠ¥ ë¶„ì„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        performance = analysis.get('collection_performance', {})
        completion_rate = performance.get('completion_rate', 0)
        if completion_rate < 90:
            recommendations.append(f"âš ï¸ ìˆ˜ì§‘ ì™„ë£Œìœ¨ì´ {completion_rate:.1f}%ì…ë‹ˆë‹¤. ì•ˆì •ì„± ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        avg_rate = performance.get('avg_collection_rate', 0)
        if avg_rate < 5:
            recommendations.append(f"ğŸŒ í‰ê·  ìˆ˜ì§‘ ì†ë„ê°€ {avg_rate:.1f}ê±´/ì´ˆë¡œ ë‚®ìŠµë‹ˆë‹¤. ì„±ëŠ¥ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # í’ˆì§ˆ ë¶„ì„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        quality = analysis.get('quality_metrics', {})
        quality_issues = quality.get('quality_issue_summary', {})
        if quality_issues:
            major_issue = max(quality_issues, key=quality_issues.get)
            recommendations.append(f"ğŸ“Š {major_issue} í’ˆì§ˆ ì´ìŠˆê°€ {quality_issues[major_issue]}ê±´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        if not recommendations:
            recommendations.append("âœ… ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ìš´ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
        
        return recommendations
    
    def _print_analysis_report(self, analysis: Dict):
        """ë¶„ì„ ë³´ê³ ì„œ ì¶œë ¥"""
        print(f"\nğŸ“‹ {analysis['date']} ë¡œê·¸ ë¶„ì„ ë³´ê³ ì„œ")
        print("=" * 60)
        
        # ìš”ì•½ ì •ë³´
        summary = analysis.get('summary', {})
        if summary:
            print(f"ğŸ“Š ë¡œê·¸ ìš”ì•½:")
            print(f"   ì´ ë¡œê·¸: {summary.get('total_logs', 0):,}ê°œ")
            print(f"   ì„¸ì…˜ìˆ˜: {summary.get('session_count', 0)}ê°œ")
            
            level_dist = summary.get('log_level_distribution', {})
            for level, count in level_dist.items():
                print(f"   {level}: {count:,}ê°œ")
        
        # ìˆ˜ì§‘ ì„±ëŠ¥
        performance = analysis.get('collection_performance', {})
        if performance:
            print(f"\nâš¡ ìˆ˜ì§‘ ì„±ëŠ¥:")
            print(f"   ì™„ë£Œìœ¨: {performance.get('completion_rate', 0):.1f}%")
            print(f"   í‰ê·  ìˆ˜ì§‘ì†ë„: {performance.get('avg_collection_rate', 0):.2f}ê±´/ì´ˆ")
            print(f"   í˜ì´ì§€ë‹¹ ë§¤ë¬¼: {performance.get('avg_properties_per_page', 0):.1f}ê°œ")
        
        # ì—ëŸ¬ ë¶„ì„
        error_analysis = analysis.get('error_analysis', {})
        if error_analysis and error_analysis.get('total_errors', 0) > 0:
            print(f"\nâŒ ì—ëŸ¬ ë¶„ì„:")
            print(f"   ì´ ì—ëŸ¬: {error_analysis.get('total_errors', 0)}ê°œ")
            
            top_errors = error_analysis.get('top_error_messages', [])
            if top_errors:
                print(f"   ì£¼ìš” ì—ëŸ¬:")
                for error in top_errors[:3]:
                    print(f"     â€¢ {error['message'][:50]}... ({error['count']}íšŒ)")
        
        # ê¶Œì¥ì‚¬í•­
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            for rec in recommendations:
                print(f"   {rec}")
        
        print("=" * 60)
    
    def create_log_insights_dashboard(self, days: int = 7) -> Dict:
        """ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±"""
        print(f"ğŸ“ˆ ìµœê·¼ {days}ì¼ ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ ëŒ€ì‹œë³´ë“œ ìƒì„±")
        
        insights = {
            'period_days': days,
            'daily_summaries': [],
            'trends': {},
            'anomalies': [],
            'performance_insights': {}
        }
        
        # ì¼ë³„ ë¶„ì„ ìˆ˜í–‰
        for i in range(days):
            target_date = (datetime.now() - timedelta(days=i)).strftime("%Y%m%d")
            daily_analysis = self.analyze_daily_logs(target_date)
            
            # ìš”ì•½ ì •ë³´ë§Œ ì €ì¥
            daily_summary = {
                'date': target_date,
                'total_logs': daily_analysis.get('summary', {}).get('total_logs', 0),
                'error_count': daily_analysis.get('error_analysis', {}).get('total_errors', 0),
                'collection_rate': daily_analysis.get('collection_performance', {}).get('avg_collection_rate', 0),
                'completion_rate': daily_analysis.get('collection_performance', {}).get('completion_rate', 0)
            }
            insights['daily_summaries'].append(daily_summary)
        
        # íŠ¸ë Œë“œ ë¶„ì„
        insights['trends'] = self._analyze_trends(insights['daily_summaries'])
        
        # ì´ìƒ ì§•í›„ íƒì§€
        insights['anomalies'] = self._detect_anomalies(insights['daily_summaries'])
        
        # ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
        insights['performance_insights'] = self._generate_performance_insights(insights['daily_summaries'])
        
        return insights
    
    def _analyze_trends(self, daily_summaries: List[Dict]) -> Dict:
        """íŠ¸ë Œë“œ ë¶„ì„"""
        if len(daily_summaries) < 3:
            return {}
        
        # ìµœê·¼ 3ì¼ vs ì´ì „ ê¸°ê°„ ë¹„êµ
        recent = daily_summaries[:3]
        previous = daily_summaries[3:6] if len(daily_summaries) >= 6 else daily_summaries[3:]
        
        if not previous:
            return {}
        
        recent_avg_rate = sum(d['collection_rate'] for d in recent) / len(recent)
        previous_avg_rate = sum(d['collection_rate'] for d in previous) / len(previous)
        
        rate_change = ((recent_avg_rate - previous_avg_rate) / previous_avg_rate * 100) if previous_avg_rate > 0 else 0
        
        recent_avg_errors = sum(d['error_count'] for d in recent) / len(recent)
        previous_avg_errors = sum(d['error_count'] for d in previous) / len(previous)
        
        error_change = ((recent_avg_errors - previous_avg_errors) / previous_avg_errors * 100) if previous_avg_errors > 0 else 0
        
        return {
            'collection_rate_change_percent': round(rate_change, 1),
            'error_rate_change_percent': round(error_change, 1),
            'trend_direction': 'improving' if rate_change > 5 and error_change < -10 else 'declining' if rate_change < -5 or error_change > 20 else 'stable'
        }
    
    def _detect_anomalies(self, daily_summaries: List[Dict]) -> List[Dict]:
        """ì´ìƒ ì§•í›„ íƒì§€"""
        anomalies = []
        
        if len(daily_summaries) < 3:
            return anomalies
        
        # ì—ëŸ¬ ê¸‰ì¦ ê°ì§€
        error_counts = [d['error_count'] for d in daily_summaries]
        avg_errors = sum(error_counts) / len(error_counts)
        
        for i, summary in enumerate(daily_summaries):
            if summary['error_count'] > avg_errors * 3:  # í‰ê· ì˜ 3ë°° ì´ìƒ
                anomalies.append({
                    'type': 'error_spike',
                    'date': summary['date'],
                    'value': summary['error_count'],
                    'threshold': avg_errors * 3,
                    'severity': 'high'
                })
        
        # ìˆ˜ì§‘ ì„±ëŠ¥ ê¸‰ë½ ê°ì§€
        collection_rates = [d['collection_rate'] for d in daily_summaries if d['collection_rate'] > 0]
        if collection_rates:
            avg_rate = sum(collection_rates) / len(collection_rates)
            
            for summary in daily_summaries:
                if summary['collection_rate'] < avg_rate * 0.5:  # í‰ê· ì˜ 50% ë¯¸ë§Œ
                    anomalies.append({
                        'type': 'performance_drop',
                        'date': summary['date'],
                        'value': summary['collection_rate'],
                        'threshold': avg_rate * 0.5,
                        'severity': 'medium'
                    })
        
        return anomalies
    
    def _generate_performance_insights(self, daily_summaries: List[Dict]) -> Dict:
        """ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        if not daily_summaries:
            return {}
        
        # ìµœê³ /ìµœì € ì„±ëŠ¥ì¼ ì°¾ê¸°
        best_day = max(daily_summaries, key=lambda x: x['collection_rate'])
        worst_day = min(daily_summaries, key=lambda x: x['collection_rate']) if daily_summaries else best_day
        
        # í‰ê·  ì„±ëŠ¥
        avg_rate = sum(d['collection_rate'] for d in daily_summaries) / len(daily_summaries)
        avg_completion = sum(d['completion_rate'] for d in daily_summaries) / len(daily_summaries)
        
        return {
            'avg_collection_rate': round(avg_rate, 2),
            'avg_completion_rate': round(avg_completion, 1),
            'best_performance_date': best_day['date'],
            'best_performance_rate': best_day['collection_rate'],
            'worst_performance_date': worst_day['date'],
            'worst_performance_rate': worst_day['collection_rate'],
            'performance_consistency': self._calculate_consistency(daily_summaries)
        }
    
    def _calculate_consistency(self, daily_summaries: List[Dict]) -> str:
        """ì„±ëŠ¥ ì¼ê´€ì„± ê³„ì‚°"""
        rates = [d['collection_rate'] for d in daily_summaries if d['collection_rate'] > 0]
        if len(rates) < 2:
            return 'insufficient_data'
        
        avg = sum(rates) / len(rates)
        variance = sum((r - avg) ** 2 for r in rates) / len(rates)
        std_dev = variance ** 0.5
        
        cv = (std_dev / avg) * 100 if avg > 0 else 0  # ë³€ë™ê³„ìˆ˜
        
        if cv < 20:
            return 'highly_consistent'
        elif cv < 40:
            return 'moderately_consistent'
        else:
            return 'inconsistent'

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ê³ ë„í™”ëœ ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ')
    parser.add_argument('--analyze', action='store_true', help='ì˜¤ëŠ˜ ë¡œê·¸ ë¶„ì„')
    parser.add_argument('--date', type=str, help='ë¶„ì„í•  ë‚ ì§œ (YYYYMMDD)')
    parser.add_argument('--dashboard', type=int, default=7, help='ëŒ€ì‹œë³´ë“œ ê¸°ê°„ (ì¼)')
    parser.add_argument('--compress-old', action='store_true', help='ì˜¤ë˜ëœ ë¡œê·¸ ì••ì¶•')
    
    args = parser.parse_args()
    
    print("ğŸ“Š ê³ ë„í™”ëœ ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ v2.0")
    print("=" * 50)
    
    analyzer = LogAnalyzer()
    
    if args.analyze:
        analysis = analyzer.analyze_daily_logs(args.date)
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = f"log_analysis_{timestamp}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ë¶„ì„ ê²°ê³¼ ì €ì¥: {result_file}")
    
    elif args.dashboard:
        insights = analyzer.create_log_insights_dashboard(args.dashboard)
        
        # ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì €ì¥
        with open('log_insights_dashboard.json', 'w', encoding='utf-8') as f:
            json.dump(insights, f, ensure_ascii=False, indent=2)
        
        print("ğŸ“Š ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„± ì™„ë£Œ: log_insights_dashboard.json")
    
    if args.compress_old:
        # 7ì¼ ì´ì „ ë¡œê·¸ ì••ì¶•
        log_dir = Path("logs")
        cutoff_date = datetime.now() - timedelta(days=7)
        
        for log_file in log_dir.glob("*.jsonl"):
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            try:
                file_date_str = log_file.stem.split('_')[-1]
                file_date = datetime.strptime(file_date_str, "%Y%m%d")
                
                if file_date < cutoff_date:
                    # ì••ì¶•
                    with open(log_file, 'rb') as f_in:
                        with gzip.open(f"{log_file}.gz", 'wb') as f_out:
                            f_out.writelines(f_in)
                    
                    # ì›ë³¸ ì‚­ì œ
                    log_file.unlink()
                    print(f"ğŸ“¦ {log_file.name} ì••ì¶• ì™„ë£Œ")
            
            except (ValueError, IndexError):
                continue
        
        print("âœ… ì˜¤ë˜ëœ ë¡œê·¸ ì••ì¶• ì™„ë£Œ")

if __name__ == "__main__":
    main()