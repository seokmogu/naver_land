#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì„±ëŠ¥ ìµœì í™”ëœ ë„¤ì´ë²„ ìˆ˜ì§‘ê¸° v2.0
- ë³‘ë ¬ ì²˜ë¦¬ ë° ë°°ì¹˜ ìµœì í™”
- ì§€ëŠ¥í˜• ì¤‘ë³µ ì œê±°
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
"""

import asyncio
import time
from datetime import datetime, date
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from final_safe_collector import FinalSafeCollector
from supabase_client import SupabaseHelper

@dataclass
class CollectionMetrics:
    """ìˆ˜ì§‘ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    start_time: float
    end_time: float = 0
    total_collected: int = 0
    duplicates_removed: int = 0
    errors_count: int = 0
    memory_peak_mb: float = 0
    
    @property
    def duration(self) -> float:
        return self.end_time - self.start_time if self.end_time else time.time() - self.start_time
    
    @property
    def collection_rate(self) -> float:
        return self.total_collected / self.duration if self.duration > 0 else 0

class EnhancedPerformanceCollector:
    """ì„±ëŠ¥ ìµœì í™”ëœ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, max_workers: int = 3):
        self.base_collector = FinalSafeCollector()
        self.db_helper = SupabaseHelper()
        self.max_workers = max_workers
        self.metrics = CollectionMetrics(start_time=time.time())
        
        # ì„±ëŠ¥ ì„¤ì •
        self.batch_size = 50  # ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
        self.cache_size = 1000  # ì¤‘ë³µ ì²´í¬ ìºì‹œ í¬ê¸°
        self.duplicate_cache = set()  # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
    
    def get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (MB)"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return 0.0
    
    def collect_region_batch(self, region_codes: List[str], max_pages: int = 999) -> Dict:
        """ì—¬ëŸ¬ ì§€ì—­ ë°°ì¹˜ ìˆ˜ì§‘"""
        print(f"ğŸš€ ë°°ì¹˜ ìˆ˜ì§‘ ì‹œì‘: {len(region_codes)}ê°œ ì§€ì—­")
        print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: {self.max_workers}ê°œ ìŠ¤ë ˆë“œ")
        
        results = {
            'total_regions': len(region_codes),
            'successful': 0,
            'failed': 0,
            'results': [],
            'performance': {}
        }
        
        # ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_region = {
                executor.submit(self._collect_single_region_optimized, region_code, max_pages): region_code
                for region_code in region_codes
            }
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_region):
                region_code = future_to_region[future]
                try:
                    result = future.result()
                    if result['status'] == 'success':
                        results['successful'] += 1
                        self.metrics.total_collected += result.get('collected_count', 0)
                    else:
                        results['failed'] += 1
                        self.metrics.errors_count += 1
                    
                    results['results'].append(result)
                    
                    # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
                    current_memory = self.get_memory_usage()
                    if current_memory > self.metrics.memory_peak_mb:
                        self.metrics.memory_peak_mb = current_memory
                    
                    print(f"  âœ… {region_code} ì™„ë£Œ: {result.get('collected_count', 0)}ê°œ ë§¤ë¬¼")
                    
                except Exception as e:
                    print(f"  âŒ {region_code} ì‹¤íŒ¨: {str(e)}")
                    results['failed'] += 1
                    self.metrics.errors_count += 1
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        self.metrics.end_time = time.time()
        results['performance'] = {
            'duration_seconds': self.metrics.duration,
            'total_collected': self.metrics.total_collected,
            'collection_rate': self.metrics.collection_rate,
            'duplicates_removed': self.metrics.duplicates_removed,
            'memory_peak_mb': self.metrics.memory_peak_mb,
            'errors_count': self.metrics.errors_count
        }
        
        print(f"\nğŸ“Š ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"   â±ï¸ ì†Œìš” ì‹œê°„: {self.metrics.duration:.1f}ì´ˆ")
        print(f"   ğŸ“¦ ì´ ìˆ˜ì§‘: {self.metrics.total_collected:,}ê°œ")
        print(f"   âš¡ ìˆ˜ì§‘ ì†ë„: {self.metrics.collection_rate:.1f}ê°œ/ì´ˆ")
        print(f"   ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {self.metrics.duplicates_removed}ê°œ")
        print(f"   ğŸ’¾ ìµœëŒ€ ë©”ëª¨ë¦¬: {self.metrics.memory_peak_mb:.1f}MB")
        
        return results
    
    def _collect_single_region_optimized(self, region_code: str, max_pages: int) -> Dict:
        """ìµœì í™”ëœ ë‹¨ì¼ ì§€ì—­ ìˆ˜ì§‘"""
        try:
            # ê¸°ì¡´ ìˆ˜ì§‘ê¸° ì‚¬ìš©í•˜ë˜ ìµœì í™” ì ìš©
            result = self.base_collector.collect_and_save_safely(
                cortar_no=region_code,
                max_pages=max_pages,
                save_to_db=True,
                test_mode=False
            )
            
            # ì¤‘ë³µ ì œê±° ë¡œì§ ì ìš© (ë©”ëª¨ë¦¬ ê¸°ë°˜)
            if result.get('status') == 'success':
                duplicates = self._remove_duplicates_in_memory(region_code)
                self.metrics.duplicates_removed += duplicates
                result['duplicates_removed'] = duplicates
            
            return result
            
        except Exception as e:
            return {
                'status': 'error',
                'region_code': region_code,
                'error': str(e)
            }
    
    def _remove_duplicates_in_memory(self, region_code: str) -> int:
        """ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¤‘ë³µ ì œê±°"""
        try:
            # ìµœê·¼ ìˆ˜ì§‘ëœ ë§¤ë¬¼ì—ì„œ ì¤‘ë³µ ì²´í¬
            recent_properties = self.db_helper.client.table('properties')\
                .select('article_no')\
                .eq('cortar_no', region_code)\
                .eq('is_active', True)\
                .order('updated_at', desc=True)\
                .limit(500)\
                .execute()
            
            duplicates_found = 0
            current_ids = set()
            
            for prop in recent_properties.data or []:
                article_no = prop['article_no']
                if article_no in current_ids:
                    duplicates_found += 1
                    # ì¤‘ë³µ ë§¤ë¬¼ ë¹„í™œì„±í™” (ì•ˆì „í•˜ê²Œ)
                    self.db_helper.client.table('properties')\
                        .update({'is_active': False, 'updated_at': datetime.now().isoformat()})\
                        .eq('article_no', article_no)\
                        .eq('cortar_no', region_code)\
                        .execute()
                else:
                    current_ids.add(article_no)
            
            return duplicates_found
            
        except Exception as e:
            print(f"âš ï¸ ì¤‘ë³µ ì œê±° ì˜¤ë¥˜: {e}")
            return 0
    
    def intelligent_scheduling(self, regions_priority: Dict[str, int]) -> List[str]:
        """ì§€ëŠ¥í˜• ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ë§ - ìš°ì„ ìˆœìœ„ ê¸°ë°˜"""
        
        # ìµœê·¼ ìˆ˜ì§‘ ì„±ê³¼ ì¡°íšŒ
        performance_data = {}
        for region_code in regions_priority.keys():
            try:
                # ìµœê·¼ 7ì¼ ìˆ˜ì§‘ í†µê³„
                week_ago = (date.today() - timedelta(days=7)).isoformat()
                stats = self.db_helper.client.table('daily_stats')\
                    .select('total_count, new_count')\
                    .eq('cortar_no', region_code)\
                    .gte('stat_date', week_ago)\
                    .execute()
                
                total_collected = sum(s['total_count'] or 0 for s in stats.data or [])
                avg_new = sum(s['new_count'] or 0 for s in stats.data or []) / max(len(stats.data or []), 1)
                
                performance_data[region_code] = {
                    'total_collected': total_collected,
                    'avg_new_per_day': avg_new,
                    'priority_score': regions_priority[region_code],
                    'efficiency_score': avg_new * 0.7 + regions_priority[region_code] * 0.3
                }
                
            except Exception as e:
                performance_data[region_code] = {
                    'total_collected': 0,
                    'avg_new_per_day': 0,
                    'priority_score': regions_priority[region_code],
                    'efficiency_score': regions_priority[region_code]
                }
        
        # íš¨ìœ¨ì„± ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        sorted_regions = sorted(
            performance_data.keys(),
            key=lambda x: performance_data[x]['efficiency_score'],
            reverse=True
        )
        
        print(f"ğŸ§  ì§€ëŠ¥í˜• ìŠ¤ì¼€ì¤„ë§ ê²°ê³¼:")
        for i, region in enumerate(sorted_regions[:5], 1):
            data = performance_data[region]
            print(f"  {i}. {region} - íš¨ìœ¨ì„±: {data['efficiency_score']:.1f}, "
                  f"ì¼í‰ê·  ì‹ ê·œ: {data['avg_new_per_day']:.1f}ê°œ")
        
        return sorted_regions
    
    def adaptive_collection(self, regions: List[str], target_duration_minutes: int = 60) -> Dict:
        """ì ì‘í˜• ìˆ˜ì§‘ - ëª©í‘œ ì‹œê°„ ë‚´ ìµœì í™”"""
        print(f"ğŸ¯ ì ì‘í˜• ìˆ˜ì§‘ ëª¨ë“œ: {target_duration_minutes}ë¶„ ëª©í‘œ")
        
        target_duration_seconds = target_duration_minutes * 60
        results = []
        start_time = time.time()
        
        for region in regions:
            current_duration = time.time() - start_time
            remaining_time = target_duration_seconds - current_duration
            
            if remaining_time <= 0:
                print(f"â° ëª©í‘œ ì‹œê°„ ì´ˆê³¼, ë‚¨ì€ ì§€ì—­ ìƒëµ")
                break
            
            # ë‚¨ì€ ì‹œê°„ ê¸°ì¤€ í˜ì´ì§€ ìˆ˜ ì¡°ì •
            estimated_pages = min(999, int(remaining_time / 30))  # í˜ì´ì§€ë‹¹ 30ì´ˆ ì˜ˆìƒ
            
            print(f"ğŸš€ {region} ìˆ˜ì§‘ ì¤‘... (ë‚¨ì€ì‹œê°„: {remaining_time:.0f}ì´ˆ, ì˜ˆìƒí˜ì´ì§€: {estimated_pages})")
            
            result = self._collect_single_region_optimized(region, estimated_pages)
            results.append(result)
            
            # ì„±ê³µë¥  ê¸°ë°˜ ë™ì  ì¡°ì •
            success_rate = sum(1 for r in results if r.get('status') == 'success') / len(results)
            if success_rate < 0.8:  # ì„±ê³µë¥  80% ë¯¸ë§Œì‹œ ì†ë„ ì¡°ì •
                print(f"âš ï¸ ì„±ê³µë¥  {success_rate:.1%} - ìˆ˜ì§‘ ì†ë„ ì¡°ì •")
                time.sleep(5)  # 5ì´ˆ íœ´ì‹
        
        total_duration = time.time() - start_time
        
        return {
            'completed_regions': len(results),
            'target_duration_minutes': target_duration_minutes,
            'actual_duration_minutes': total_duration / 60,
            'efficiency_ratio': (target_duration_seconds / total_duration) if total_duration > 0 else 0,
            'results': results
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ì„±ëŠ¥ ìµœì í™”ëœ ë„¤ì´ë²„ ìˆ˜ì§‘ê¸° v2.0')
    parser.add_argument('--regions', nargs='+', help='ìˆ˜ì§‘í•  ì§€ì—­ ì½”ë“œë“¤')
    parser.add_argument('--workers', type=int, default=3, help='ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜')
    parser.add_argument('--max-pages', type=int, default=999, help='ì§€ì—­ë³„ ìµœëŒ€ í˜ì´ì§€')
    parser.add_argument('--mode', choices=['batch', 'adaptive', 'intelligent'], default='batch',
                        help='ìˆ˜ì§‘ ëª¨ë“œ ì„ íƒ')
    parser.add_argument('--target-minutes', type=int, default=60, help='ì ì‘í˜• ëª¨ë“œ ëª©í‘œ ì‹œê°„(ë¶„)')
    
    args = parser.parse_args()
    
    # ê¸°ë³¸ ê°•ë‚¨êµ¬ ì§€ì—­ë“¤
    if not args.regions:
        args.regions = [
            "1168010100",  # ì—­ì‚¼ë™
            "1168010500",  # ì‚¼ì„±ë™
            "1168010800",  # ë…¼í˜„ë™
            "1168010600",  # ëŒ€ì¹˜ë™
            "1168010700"   # ì‹ ì‚¬ë™
        ]
    
    collector = EnhancedPerformanceCollector(max_workers=args.workers)
    
    print(f"ğŸš€ ì„±ëŠ¥ ìµœì í™” ìˆ˜ì§‘ê¸° v2.0 ì‹œì‘")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”§ ëª¨ë“œ: {args.mode}")
    print("=" * 60)
    
    if args.mode == 'batch':
        results = collector.collect_region_batch(args.regions, args.max_pages)
    
    elif args.mode == 'adaptive':
        results = collector.adaptive_collection(args.regions, args.target_minutes)
    
    elif args.mode == 'intelligent':
        # ì§€ì—­ë³„ ìš°ì„ ìˆœìœ„ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
        priority_map = {
            "1168010100": 30,  # ì—­ì‚¼ë™
            "1168010500": 26,  # ì‚¼ì„±ë™  
            "1168010800": 23,  # ë…¼í˜„ë™
            "1168010600": 22,  # ëŒ€ì¹˜ë™
            "1168010700": 22   # ì‹ ì‚¬ë™
        }
        
        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì •ë ¬ëœ ì§€ì—­ìœ¼ë¡œ ë°°ì¹˜ ìˆ˜ì§‘
        optimized_regions = collector.intelligent_scheduling(priority_map)
        results = collector.collect_region_batch(optimized_regions[:len(args.regions)], args.max_pages)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
    print(f"   ğŸ“Š ì²˜ë¦¬ëœ ì§€ì—­: {results.get('total_regions', 0)}ê°œ")
    print(f"   âœ… ì„±ê³µ: {results.get('successful', 0)}ê°œ")
    print(f"   âŒ ì‹¤íŒ¨: {results.get('failed', 0)}ê°œ")
    
    if 'performance' in results:
        perf = results['performance']
        print(f"   âš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        print(f"      - ìˆ˜ì§‘ ì†ë„: {perf['collection_rate']:.1f}ê°œ/ì´ˆ")
        print(f"      - ì´ ìˆ˜ì§‘ëŸ‰: {perf['total_collected']:,}ê°œ")
        print(f"      - ë©”ëª¨ë¦¬ ì‚¬ìš©: {perf['memory_peak_mb']:.1f}MB")
        print(f"      - ì¤‘ë³µ ì œê±°: {perf['duplicates_removed']}ê°œ")

if __name__ == "__main__":
    main()