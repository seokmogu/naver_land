#!/usr/bin/env python3
"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬
ê¸°ì¡´ ë°©ì‹ vs ìµœì í™”ëœ ì§ì ‘ ì €ì¥ ë°©ì‹ ì„±ëŠ¥ ë¹„êµ
"""

import time
import json
import os
import sys
import psutil
from datetime import datetime
from typing import Dict, List, Tuple
from dataclasses import dataclass, asdict

# ìƒëŒ€ ê²½ë¡œë¡œ ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    method_name: str
    region_name: str
    cortar_no: str
    success: bool
    
    # ì‹œê°„ ì¸¡ì •
    start_time: str
    end_time: str
    processing_time: float
    
    # ë°ì´í„° í†µê³„
    total_properties: int
    new_properties: int
    updated_properties: int
    failed_properties: int
    
    # ì„±ëŠ¥ ì§€í‘œ
    properties_per_second: float
    
    # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
    peak_memory_mb: float
    avg_memory_mb: float
    cpu_percent: float
    
    # ì¶”ê°€ ë©”íŠ¸ë¦­
    batch_count: int = 0
    api_calls: int = 0
    error_message: str = ""

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬"""
    
    def __init__(self):
        """ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ ì´ˆê¸°í™”"""
        self.results = []
        self.process = psutil.Process()
        
    def benchmark_method(self, method_name: str, method_func, *args, **kwargs) -> BenchmarkResult:
        """
        íŠ¹ì • ë©”ì†Œë“œì˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        
        Args:
            method_name: ë©”ì†Œë“œ ì´ë¦„
            method_func: ì‹¤í–‰í•  í•¨ìˆ˜
            *args, **kwargs: í•¨ìˆ˜ ë§¤ê°œë³€ìˆ˜
            
        Returns:
            BenchmarkResult: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        
        print(f"\nğŸ” ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {method_name}")
        print("=" * 50)
        
        # ì‹œì‘ ìƒíƒœ ê¸°ë¡
        start_time = datetime.now()
        start_memory = self.process.memory_info().rss / 1024 / 1024
        peak_memory = start_memory
        total_memory = 0
        memory_samples = 0
        
        # CPU ëª¨ë‹ˆí„°ë§ ì‹œì‘
        cpu_percent = self.process.cpu_percent()
        
        try:
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë³„ë„ ìŠ¤ë ˆë“œëŠ” ìƒëµí•˜ê³  ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬
            result = method_func(*args, **kwargs)
            
            # ì¢…ë£Œ ìƒíƒœ ê¸°ë¡
            end_time = datetime.now()
            end_memory = self.process.memory_info().rss / 1024 / 1024
            cpu_percent = self.process.cpu_percent()
            
            peak_memory = max(peak_memory, end_memory)
            processing_time = (end_time - start_time).total_seconds()
            
            # ê²°ê³¼ ì¶”ì¶œ
            if isinstance(result, dict) and result.get('success'):
                # ì„±ê³µì ì¸ ìˆ˜ì§‘ ê²°ê³¼ ì²˜ë¦¬
                collection_stats = result.get('collection_stats', {})
                session_info = result.get('session_info', {})
                performance = result.get('performance', {})
                
                total_properties = collection_stats.get('collected_properties', 0)
                new_properties = collection_stats.get('new_properties', 0)
                updated_properties = collection_stats.get('updated_properties', 0)
                failed_properties = collection_stats.get('failed_properties', 0)
                batch_count = result.get('batch_count', 0)
                
                properties_per_second = performance.get('records_per_second', 
                    total_properties / processing_time if processing_time > 0 else 0
                )
                
                benchmark_result = BenchmarkResult(
                    method_name=method_name,
                    region_name=result.get('region_name', ''),
                    cortar_no=result.get('cortar_no', ''),
                    success=True,
                    start_time=start_time.isoformat(),
                    end_time=end_time.isoformat(),
                    processing_time=processing_time,
                    total_properties=total_properties,
                    new_properties=new_properties,
                    updated_properties=updated_properties,
                    failed_properties=failed_properties,
                    properties_per_second=properties_per_second,
                    peak_memory_mb=peak_memory,
                    avg_memory_mb=(start_memory + end_memory) / 2,
                    cpu_percent=cpu_percent,
                    batch_count=batch_count,
                    api_calls=session_info.get('total_pages', 0)
                )
                
            else:
                # ì‹¤íŒ¨í•œ ê²½ìš°
                error_message = result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜') if isinstance(result, dict) else str(result)
                
                benchmark_result = BenchmarkResult(
                    method_name=method_name,
                    region_name=kwargs.get('region_name', ''),
                    cortar_no=args[0] if args else '',
                    success=False,
                    start_time=start_time.isoformat(),
                    end_time=end_time.isoformat(),
                    processing_time=processing_time,
                    total_properties=0,
                    new_properties=0,
                    updated_properties=0,
                    failed_properties=0,
                    properties_per_second=0.0,
                    peak_memory_mb=peak_memory,
                    avg_memory_mb=(start_memory + end_memory) / 2,
                    cpu_percent=cpu_percent,
                    error_message=error_message
                )
            
        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒ
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            benchmark_result = BenchmarkResult(
                method_name=method_name,
                region_name=kwargs.get('region_name', ''),
                cortar_no=args[0] if args else '',
                success=False,
                start_time=start_time.isoformat(),
                end_time=end_time.isoformat(),
                processing_time=processing_time,
                total_properties=0,
                new_properties=0,
                updated_properties=0,
                failed_properties=0,
                properties_per_second=0.0,
                peak_memory_mb=peak_memory,
                avg_memory_mb=(start_memory + peak_memory) / 2,
                cpu_percent=cpu_percent,
                error_message=str(e)
            )
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_benchmark_result(benchmark_result)
        
        # ê²°ê³¼ ì €ì¥
        self.results.append(benchmark_result)
        
        return benchmark_result
    
    def _print_benchmark_result(self, result: BenchmarkResult):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶œë ¥"""
        
        if result.success:
            print(f"âœ… {result.method_name} ì™„ë£Œ")
            print(f"   ì§€ì—­: {result.region_name} ({result.cortar_no})")
            print(f"   ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
            print(f"   ì´ ë§¤ë¬¼: {result.total_properties:,}ê°œ")
            print(f"   ì‹ ê·œ ë§¤ë¬¼: {result.new_properties:,}ê°œ")
            print(f"   ì—…ë°ì´íŠ¸: {result.updated_properties:,}ê°œ")
            print(f"   ì²˜ë¦¬ ì†ë„: {result.properties_per_second:.1f} ë§¤ë¬¼/ì´ˆ")
            print(f"   ìµœëŒ€ ë©”ëª¨ë¦¬: {result.peak_memory_mb:.1f}MB")
            print(f"   í‰ê·  ë©”ëª¨ë¦¬: {result.avg_memory_mb:.1f}MB")
            print(f"   CPU ì‚¬ìš©ë¥ : {result.cpu_percent:.1f}%")
            print(f"   ë°°ì¹˜ ìˆ˜: {result.batch_count}ê°œ")
            print(f"   API í˜¸ì¶œ: {result.api_calls}íšŒ")
        else:
            print(f"âŒ {result.method_name} ì‹¤íŒ¨")
            print(f"   ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
            print(f"   ì˜¤ë¥˜: {result.error_message}")
            print(f"   ìµœëŒ€ ë©”ëª¨ë¦¬: {result.peak_memory_mb:.1f}MB")
    
    def compare_methods(self, test_cases: List[Tuple[str, str]]) -> Dict:
        """
        ì—¬ëŸ¬ ë©”ì†Œë“œ ì„±ëŠ¥ ë¹„êµ
        
        Args:
            test_cases: [(cortar_no, region_name), ...] ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ë¹„êµ ê²°ê³¼
        """
        
        print(f"\nğŸ ì„±ëŠ¥ ë¹„êµ ì‹œì‘: {len(test_cases)}ê°œ ì§€ì—­")
        print("=" * 60)
        
        comparison_results = {
            'test_cases': test_cases,
            'methods': {},
            'summary': {}
        }
        
        for cortar_no, region_name in test_cases:
            print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì§€ì—­: {region_name} ({cortar_no})")
            
            # 1. ê¸°ì¡´ ë°©ì‹ (UnifiedCollector)
            try:
                from unified_collector import UnifiedCollector
                
                def test_unified():
                    collector = UnifiedCollector()
                    return collector.collect_and_save(cortar_no, region_name)
                
                unified_result = self.benchmark_method(
                    "ê¸°ì¡´ ë°©ì‹ (JSONâ†’DB)", 
                    test_unified
                )
                
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ë°©ì‹ í…ŒìŠ¤íŠ¸ ë¶ˆê°€: {e}")
                unified_result = None
            
            # 2. ìµœì í™”ëœ ì§ì ‘ ì €ì¥ ë°©ì‹
            try:
                from optimized_direct_collector import OptimizedDirectCollector
                
                def test_direct():
                    collector = OptimizedDirectCollector(batch_size=50)
                    return collector.collect_region_direct(cortar_no, region_name)
                
                direct_result = self.benchmark_method(
                    "ìµœì í™” ë°©ì‹ (Direct DB)", 
                    test_direct
                )
                
            except Exception as e:
                print(f"âš ï¸ ìµœì í™” ë°©ì‹ í…ŒìŠ¤íŠ¸ ë¶ˆê°€: {e}")
                direct_result = None
            
            # ê²°ê³¼ ì €ì¥
            comparison_results['methods'][f"{region_name}_{cortar_no}"] = {
                'unified': unified_result,
                'direct': direct_result
            }
        
        # ì „ì²´ ë¹„êµ ë¶„ì„
        comparison_results['summary'] = self._analyze_comparison(comparison_results['methods'])
        
        return comparison_results
    
    def _analyze_comparison(self, methods_results: Dict) -> Dict:
        """ë¹„êµ ë¶„ì„ ìˆ˜í–‰"""
        
        unified_results = []
        direct_results = []
        
        for region_results in methods_results.values():
            if region_results['unified'] and region_results['unified'].success:
                unified_results.append(region_results['unified'])
            
            if region_results['direct'] and region_results['direct'].success:
                direct_results.append(region_results['direct'])
        
        if not unified_results or not direct_results:
            return {'error': 'ë¹„êµí•  ìˆ˜ ìˆëŠ” ì„±ê³µì ì¸ ê²°ê³¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤'}
        
        # í‰ê·  ê³„ì‚°
        unified_avg = {
            'processing_time': sum(r.processing_time for r in unified_results) / len(unified_results),
            'properties_per_second': sum(r.properties_per_second for r in unified_results) / len(unified_results),
            'peak_memory_mb': sum(r.peak_memory_mb for r in unified_results) / len(unified_results),
            'total_properties': sum(r.total_properties for r in unified_results) / len(unified_results)
        }
        
        direct_avg = {
            'processing_time': sum(r.processing_time for r in direct_results) / len(direct_results),
            'properties_per_second': sum(r.properties_per_second for r in direct_results) / len(direct_results),
            'peak_memory_mb': sum(r.peak_memory_mb for r in direct_results) / len(direct_results),
            'total_properties': sum(r.total_properties for r in direct_results) / len(direct_results)
        }
        
        # ê°œì„ ë¥  ê³„ì‚°
        time_improvement = ((unified_avg['processing_time'] - direct_avg['processing_time']) / 
                           unified_avg['processing_time'] * 100) if unified_avg['processing_time'] > 0 else 0
        
        speed_improvement = ((direct_avg['properties_per_second'] - unified_avg['properties_per_second']) / 
                            unified_avg['properties_per_second'] * 100) if unified_avg['properties_per_second'] > 0 else 0
        
        memory_improvement = ((unified_avg['peak_memory_mb'] - direct_avg['peak_memory_mb']) / 
                             unified_avg['peak_memory_mb'] * 100) if unified_avg['peak_memory_mb'] > 0 else 0
        
        summary = {
            'unified_avg': unified_avg,
            'direct_avg': direct_avg,
            'improvements': {
                'processing_time': time_improvement,
                'processing_speed': speed_improvement,
                'memory_usage': memory_improvement
            },
            'test_count': {
                'unified': len(unified_results),
                'direct': len(direct_results)
            }
        }
        
        return summary
    
    def print_comparison_summary(self, comparison_results: Dict):
        """ë¹„êµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        
        summary = comparison_results.get('summary')
        if not summary or 'error' in summary:
            print(f"âŒ ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {summary.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return
        
        print(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        unified_avg = summary['unified_avg']
        direct_avg = summary['direct_avg']
        improvements = summary['improvements']
        
        print(f"\nğŸ“ˆ í‰ê·  ì„±ëŠ¥ ì§€í‘œ")
        print(f"{'ì§€í‘œ':<20} {'ê¸°ì¡´ ë°©ì‹':<15} {'ìµœì í™” ë°©ì‹':<15} {'ê°œì„ ë¥ ':<10}")
        print("-" * 60)
        print(f"{'ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)':<20} {unified_avg['processing_time']:<15.1f} {direct_avg['processing_time']:<15.1f} {improvements['processing_time']:>6.1f}%")
        print(f"{'ì²˜ë¦¬ ì†ë„ (ë§¤ë¬¼/ì´ˆ)':<20} {unified_avg['properties_per_second']:<15.1f} {direct_avg['properties_per_second']:<15.1f} {improvements['processing_speed']:>6.1f}%")
        print(f"{'ìµœëŒ€ ë©”ëª¨ë¦¬ (MB)':<20} {unified_avg['peak_memory_mb']:<15.1f} {direct_avg['peak_memory_mb']:<15.1f} {improvements['memory_usage']:>6.1f}%")
        print(f"{'í‰ê·  ë§¤ë¬¼ ìˆ˜':<20} {unified_avg['total_properties']:<15.0f} {direct_avg['total_properties']:<15.0f} {'N/A':>6}")
        
        print(f"\nğŸ¯ ê°œì„  íš¨ê³¼")
        if improvements['processing_time'] > 0:
            print(f"   âš¡ ì²˜ë¦¬ ì‹œê°„: {improvements['processing_time']:.1f}% ë‹¨ì¶•")
        if improvements['processing_speed'] > 0:
            print(f"   ğŸš€ ì²˜ë¦¬ ì†ë„: {improvements['processing_speed']:.1f}% í–¥ìƒ")
        if improvements['memory_usage'] > 0:
            print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {improvements['memory_usage']:.1f}% ê°ì†Œ")
        
        print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ì •ë³´")
        print(f"   ê¸°ì¡´ ë°©ì‹ ì„±ê³µ: {summary['test_count']['unified']}íšŒ")
        print(f"   ìµœì í™” ë°©ì‹ ì„±ê³µ: {summary['test_count']['direct']}íšŒ")
    
    def save_benchmark_report(self, filename: str = None):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"benchmark_report_{timestamp}.json"
        
        try:
            report_data = {
                'timestamp': datetime.now().isoformat(),
                'system_info': {
                    'cpu_count': psutil.cpu_count(),
                    'memory_total_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
                    'platform': sys.platform
                },
                'results': [asdict(result) for result in self.results]
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“„ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ì €ì¥: {filename}")
            
        except Exception as e:
            print(f"âŒ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

def run_comprehensive_benchmark():
    """ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    
    print("ğŸ§ª ë„¤ì´ë²„ ë¶€ë™ì‚° ìˆ˜ì§‘ê¸° ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 60)
    
    # ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ ì´ˆê¸°í™”
    benchmark = PerformanceBenchmark()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜ (ì†Œê·œëª¨ ì§€ì—­ìœ¼ë¡œ ì‹œì‘)
    test_cases = [
        ("1168010100", "ì—­ì‚¼ë™"),
        ("1168010200", "ì‚¼ì„±ë™"),
        ("1168010300", "ë…¼í˜„ë™")
    ]
    
    # ë¹„êµ ì‹¤í–‰
    comparison_results = benchmark.compare_methods(test_cases)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    benchmark.print_comparison_summary(comparison_results)
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    benchmark.save_benchmark_report()
    
    return comparison_results

if __name__ == "__main__":
    # ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results = run_comprehensive_benchmark()