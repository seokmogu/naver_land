#!/usr/bin/env python3
"""
ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ë„¤ì´ë²„ ë¶€ë™ì‚° ìˆ˜ì§‘ê¸°
ì‹¤ì œ API í˜¸ì¶œ íŒ¨í„´ì„ ì •í™•íˆ ì¬í˜„í•©ë‹ˆë‹¤.
"""

import requests
import json
import time
import os
import random
from datetime import datetime
from urllib.parse import urlparse, parse_qs
from kakao_address_converter import KakaoAddressConverter

def get_random_user_agent():
    """VM ì°¨ë‹¨ ìš°íšŒë¥¼ ìœ„í•œ ëœë¤ User-Agent ìƒì„±"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:133.0) Gecko/20100101 Firefox/133.0"
    ]
    return random.choice(user_agents)

class FixedNaverCollector:
    def __init__(self, token_data, use_address_converter=True):
        if not token_data:
            raise ValueError("JWT í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # token_dataê°€ ë¬¸ìì—´ì´ë©´ ê¸°ì¡´ ë°©ì‹, dictì´ë©´ ìƒˆë¡œìš´ ë°©ì‹
        if isinstance(token_data, str):
            self.token = token_data
            self.cookies = {}
        else:
            self.token = token_data['token']
            self.cookies = {cookie['name']: cookie['value'] for cookie in token_data['cookies']}
        self.headers = {
            'authorization': f'Bearer {self.token}',
            'User-Agent': get_random_user_agent(),
            'Accept': 'application/json',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Referer': 'https://new.land.naver.com/',
            'Origin': 'https://new.land.naver.com',
            'Sec-Ch-Ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
        
        # ì£¼ì†Œ ë³€í™˜ê¸° ì´ˆê¸°í™” (ì„ íƒì )
        self.address_converter = None
        if use_address_converter:
            try:
                self.address_converter = KakaoAddressConverter()
                print("ğŸ—ºï¸ ì¹´ì¹´ì˜¤ ì£¼ì†Œ ë³€í™˜ê¸° í™œì„±í™”")
            except ValueError as e:
                print(f"âš ï¸ ì£¼ì†Œ ë³€í™˜ê¸° ë¹„í™œì„±í™”: {e}")
                self.address_converter = None
        
    def parse_url(self, url):
        """URL íŒŒì‹±"""
        parsed = urlparse(url)
        query = parse_qs(parsed.query)
        
        # ms íŒŒë¼ë¯¸í„°ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ
        ms = query.get('ms', [''])[0]
        if ms:
            parts = ms.split(',')
            lat, lon, zoom = float(parts[0]), float(parts[1]), int(parts[2])
        else:
            lat, lon, zoom = 37.5665, 126.9780, 15
        
        # ë§¤ë¬¼ íƒ€ì… ì¶”ì¶œ
        article_types = query.get('a', [''])[0]
        purpose = query.get('e', [''])[0]
        
        return {
            'lat': lat,
            'lon': lon, 
            'zoom': zoom,
            'article_types': article_types,
            'purpose': purpose,
            'property_type': parsed.path.split('/')[-1]
        }
    
    def get_cortar_code(self, lat, lon, zoom):
        """ì¢Œí‘œë¡œ ì§€ì—­ì½”ë“œ ì¡°íšŒ (ë¶„ì„ëœ ì‹¤ì œ API)"""
        url = "https://new.land.naver.com/api/cortars"
        params = {
            'zoom': zoom,
            'centerLat': lat,
            'centerLon': lon
        }
        
        print(f"ğŸ” ì§€ì—­ì½”ë“œ ì¡°íšŒ: ìœ„ë„ {lat}, ê²½ë„ {lon}")
        print(f"ğŸŒ ìš”ì²­ URL: {url}")
        print(f"ğŸ“‹ íŒŒë¼ë¯¸í„°: {params}")
        
        try:
            # ìµœì í™”ëœ ìš”ì²­ ê°„ê²©
            time.sleep(1.5)  # 1.5ì´ˆ ëŒ€ê¸°ë¡œ ë‹¨ì¶•
            response = requests.get(url, headers=self.headers, params=params, cookies=self.cookies)
            print(f"ğŸ“Š ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    print(f"ğŸ“„ ì‘ë‹µ ë°ì´í„°: {data}")
                    
                    # ì‘ë‹µì´ ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš° ì²˜ë¦¬
                    if data and 'cortarNo' in data:
                        cortar_no = data.get('cortarNo')
                        cortar_name = data.get('cortarName')
                        print(f"âœ… ì§€ì—­: {cortar_name} (ì½”ë“œ: {cortar_no})")
                        return cortar_no
                    # ì‘ë‹µì´ ë°°ì—´ì¸ ê²½ìš° ì²˜ë¦¬
                    elif data and isinstance(data, list) and len(data) > 0:
                        cortar = data[0]
                        cortar_no = cortar.get('cortarNo')
                        cortar_name = cortar.get('cortarName')
                        print(f"âœ… ì§€ì—­: {cortar_name} (ì½”ë“œ: {cortar_no})")
                        return cortar_no
                    else:
                        print("âŒ ì§€ì—­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        print(f"ğŸ“„ ì‹¤ì œ ì‘ë‹µ: {data}")
                except json.JSONDecodeError as je:
                    print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {je}")
                    print(f"ğŸ“„ ì›ë³¸ ì‘ë‹µ: {response.text[:500]}")
            else:
                print(f"âŒ ì§€ì—­ì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                print(f"ğŸ“„ ì‘ë‹µ ë‚´ìš©: {response.text[:500]}")
                
        except Exception as e:
            print(f"âŒ ì§€ì—­ì½”ë“œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            print(f"ğŸ” ì˜¤ë¥˜ íƒ€ì…: {type(e)}")
        
        return None
    
    def get_article_detail(self, article_no):
        """ê°œë³„ ë§¤ë¬¼ ìƒì„¸ì •ë³´ ìˆ˜ì§‘"""
        url = f"https://new.land.naver.com/api/articles/{article_no}"
        params = {'complexNo': ''}
        
        try:
            # ìµœì í™”ëœ ìš”ì²­ ê°„ê²©
            time.sleep(1.5)  # 1.5ì´ˆ ëŒ€ê¸°ë¡œ ë‹¨ì¶•
            response = requests.get(url, headers=self.headers, params=params, cookies=self.cookies)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"âš ï¸ ë§¤ë¬¼ {article_no} ìƒì„¸ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except Exception as e:
            print(f"âŒ ë§¤ë¬¼ {article_no} ìƒì„¸ì •ë³´ ì˜¤ë¥˜: {e}")
            return None
    
    def extract_useful_details(self, detail_data):
        """ìƒì„¸ì •ë³´ì—ì„œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ"""
        if not detail_data:
            return None
            
        article_detail = detail_data.get('articleDetail', {})
        article_addition = detail_data.get('articleAddition', {})
        
        # ì¢Œí‘œì—ì„œ ì£¼ì†Œ ë³€í™˜ (ì¹´ì¹´ì˜¤ API ì‚¬ìš©)
        converted_address = None
        latitude = article_detail.get('latitude')
        longitude = article_detail.get('longitude')
        
        if self.address_converter and latitude and longitude:
            try:
                converted_address = self.address_converter.convert_coord_to_address(
                    str(latitude), str(longitude)
                )
            except Exception as e:
                print(f"âš ï¸ ì£¼ì†Œ ë³€í™˜ ì˜¤ë¥˜: {e}")
        
        # í•µì‹¬ ì •ë³´ë§Œ ì„ ë³„
        useful_info = {
            # ğŸ¢ ê±´ë¬¼ ìƒì„¸ ì •ë³´
            "ê±´ë¬¼ì •ë³´": {
                "ë²•ì ìš©ë„": article_detail.get('lawUsage'),
                "ì£¼ì°¨ëŒ€ìˆ˜": article_detail.get('parkingCount'),
                "ì£¼ì°¨ê°€ëŠ¥": "ì˜ˆ" if article_detail.get('parkingPossibleYN') == 'Y' else "ì•„ë‹ˆì˜¤",
                "ì—˜ë¦¬ë² ì´í„°": "ìˆìŒ" if "ì—˜ë¦¬ë² ì´í„°" in article_detail.get('tagList', []) else "ì—†ìŒ",
                "ì¸µêµ¬ì¡°": article_detail.get('floorLayerName')
            },
            
            # ğŸ“ ìœ„ì¹˜ ì •ë³´  
            "ìœ„ì¹˜ì •ë³´": {
                "ì •í™•í•œ_ìœ„ë„": article_detail.get('latitude'),
                "ì •í™•í•œ_ê²½ë„": article_detail.get('longitude'),
                "ìƒì„¸ì£¼ì†Œ": article_detail.get('exposureAddress'),
                "ì§€í•˜ì² ë„ë³´ì‹œê°„": f"{article_detail.get('walkingTimeToNearSubway', 0)}ë¶„"
            },
            
            # ğŸ’° ê´€ë¦¬ë¹„ ì •ë³´
            "ë¹„ìš©ì •ë³´": {
                "ì›”ê´€ë¦¬ë¹„": article_detail.get('monthlyManagementCost'),
                "ê´€ë¦¬ë¹„_í¬í•¨í•­ëª©": "ì „ê¸°, ìˆ˜ë„, ì¸í„°ë„· ë“±"  # í•„ìš”ì‹œ ìƒì„¸ íŒŒì‹±
            },
            
            # ğŸ  ì…ì£¼ ì •ë³´
            "ì…ì£¼ì •ë³´": {
                "ì…ì£¼ê°€ëŠ¥ì¼": article_detail.get('moveInTypeName'),
                "í˜‘ì˜ê°€ëŠ¥ì—¬ë¶€": article_detail.get('moveInDiscussionPossibleYN')
            },
            
            # ğŸ“· ì´ë¯¸ì§€ ì •ë³´
            "ì´ë¯¸ì§€": {
                "í˜„ì¥ì‚¬ì§„ìˆ˜": article_addition.get('siteImageCount', 0),
                "ëŒ€í‘œì´ë¯¸ì§€": article_addition.get('representativeImgUrl')
            },
            
            # ğŸ˜ï¸ ì£¼ë³€ ì‹œì„¸
            "ì£¼ë³€ì‹œì„¸": {
                "ë™ì¼ì£¼ì†Œ_ë§¤ë¬¼ìˆ˜": article_addition.get('sameAddrCnt', 0),
                "ìµœê³ ê°€": article_addition.get('sameAddrMaxPrc'),
                "ìµœì €ê°€": article_addition.get('sameAddrMinPrc')
            },
            
            # ğŸ“ ìƒì„¸ ì„¤ëª… (ì •ë¦¬ë¨)
            "ìƒì„¸ì„¤ëª…": article_detail.get('detailDescription', '').strip()[:200]  # 200ìë¡œ ì œí•œ
        }
        
        # ë³€í™˜ëœ ì£¼ì†Œ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if converted_address:
            useful_info["ì¹´ì¹´ì˜¤ì£¼ì†Œë³€í™˜"] = converted_address
        
        return useful_info
    
    def collect_articles(self, cortar_no, parsed_url, max_pages=999, include_details=True, output_file=None):
        """ë§¤ë¬¼ ìˆ˜ì§‘ (ë¬´í•œìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜ - ë¶„ì„ëœ ì‹¤ì œ íŒ¨í„´)"""
        url = "https://new.land.naver.com/api/articles"
        
        # ì‹¤ì œ ë¶„ì„ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš© (URL ì¸ì½”ë”© ì—†ì´)
        base_params = {
            'cortarNo': cortar_no,
            'order': 'rank',
            'realEstateType': 'SG:SMS:GJCG:APTHGJ:GM:TJ',  # ì˜¤í”¼ìŠ¤/ìƒê°€
            'tradeType': '',  # ëª¨ë“  ê±°ë˜ íƒ€ì…
            'tag': '::::::::',  # ë¹ˆ íƒœê·¸
            'rentPriceMin': '0',
            'rentPriceMax': '900000000',
            'priceMin': '0',
            'priceMax': '900000000',
            'areaMin': '0',
            'areaMax': '900000000',
            'oldBuildYears': '',
            'recentlyBuildYears': '',
            'minHouseHoldCount': '',
            'maxHouseHoldCount': '',
            'showArticle': 'false',
            'sameAddressGroup': 'false',
            'minMaintenanceCost': '',
            'maxMaintenanceCost': '',
            'priceType': 'RETAIL',  # ìƒì—…ìš©
            'directions': '',
            'articleState': ''
        }
        
        # ìŠ¤íŠ¸ë¦¬ë° íŒŒì¼ ì“°ê¸° ì¤€ë¹„
        if output_file:
            output_file.write('  "ë§¤ë¬¼ëª©ë¡": [\n')
            is_first_article = True
        
        total_collected = 0
        total_expected = 0
        
        print("ğŸ”„ ë¬´í•œìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜ (ë°ì´í„° ì—†ì„ ë•Œê¹Œì§€ ìë™ ìˆ˜ì§‘)")
        if output_file:
            print("ğŸ’¾ ì‹¤ì‹œê°„ íŒŒì¼ ì €ì¥ ëª¨ë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)")
        
        for page in range(1, max_pages + 1):
            params = base_params.copy()
            params['page'] = page
            
            print(f"ğŸ“„ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
            
            try:
                # ìš”ì²­ ê°„ ë”œë ˆì´ ì¶”ê°€ (ì°¨ë‹¨ ë°©ì§€)
                if page > 1:
                    time.sleep(0.3)  # 0.3ì´ˆ ëŒ€ê¸° (ì†ë„ ìµœì í™”)
                
                # ìµœì í™”ëœ ëŒ€ê¸° ì‹œê°„ (ì°¨ë‹¨ ë°©ì§€ + ì„±ëŠ¥ ê· í˜•)
                delay = random.uniform(2, 4)  # 2-4ì´ˆ ëœë¤ ëŒ€ê¸°ë¡œ ë‹¨ì¶•
                time.sleep(delay)
                response = requests.get(url, headers=self.headers, params=params, cookies=self.cookies)
                
                if response.status_code == 200:
                    data = response.json()
                    articles = data.get('articleList', [])
                    # articleCount í•„ë“œê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹¤ì œ ë§¤ë¬¼ ìˆ˜ë¡œ ì¶”ì •
                    total_count = data.get('articleCount')
                    is_more_data = data.get('isMoreData', False)
                    
                    if page == 1:
                        if total_count is not None and total_count > 0:
                            total_expected = total_count
                            print(f"ğŸ“Š ì „ì²´ ë§¤ë¬¼ ìˆ˜: {total_count}")
                            estimated_pages = (total_expected + 19) // 20
                            print(f"ğŸ“„ ì˜ˆìƒ í˜ì´ì§€ ìˆ˜: {estimated_pages}")
                        else:
                            # articleCountê°€ ì—†ìœ¼ë©´ ë¬´í•œìŠ¤í¬ë¡¤ë¡œ ì¶”ì •
                            print(f"ğŸ“Š ë¬´í•œìŠ¤í¬ë¡¤ ëª¨ë“œ (ì •í™•í•œ ì´ ê°œìˆ˜ ë¯¸ì œê³µ)")
                            print(f"ğŸ“Š ì²« í˜ì´ì§€ ë§¤ë¬¼: {len(articles)}ê°œ")
                            total_expected = 0  # ë™ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚´
                            print(f"ğŸ“„ ì˜ˆìƒ í˜ì´ì§€ ìˆ˜: ì•Œ ìˆ˜ ì—†ìŒ (ë¬´í•œìŠ¤í¬ë¡¤)")
                        print(f"ğŸ”„ ë” ë§ì€ ë°ì´í„° ì—¬ë¶€: {is_more_data}")
                    
                    if articles:
                        # ë§¤ë¬¼ë³„ ì²˜ë¦¬ (ì‹¤ì‹œê°„ ì €ì¥)
                        for i, article in enumerate(articles):
                            # ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
                            if include_details:
                                article_no = article.get('articleNo')
                                if article_no:
                                    detail = self.get_article_detail(article_no)
                                    if detail:
                                        # í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥
                                        useful_details = self.extract_useful_details(detail)
                                        if useful_details:
                                            article['ìƒì„¸ì •ë³´'] = useful_details
                                    
                                    # ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ê°„ ë”œë ˆì´ (ìµœì í™”)
                                    time.sleep(0.5)  # 0.5ì´ˆë¡œ ì ì ˆíˆ ì¡°ì •
                            
                            # ì‹¤ì‹œê°„ íŒŒì¼ ì“°ê¸°
                            if output_file:
                                if not is_first_article:
                                    output_file.write(',\n')
                                else:
                                    is_first_article = False
                                
                                # ë§¤ë¬¼ ì •ë³´ ì •ë¦¬
                                processed_article = {
                                    "ë§¤ë¬¼ë²ˆí˜¸": article.get('articleNo'),
                                    "ë§¤ë¬¼ëª…": article.get('articleName'),
                                    "ë¶€ë™ì‚°íƒ€ì…": article.get('realEstateTypeName'),
                                    "ê±°ë˜íƒ€ì…": article.get('tradeTypeName'),
                                    "ë§¤ë§¤ê°€ê²©": article.get('dealOrWarrantPrc', ''),
                                    "ì›”ì„¸": article.get('rentPrc', ''),
                                    "ì „ìš©ë©´ì ": article.get('area1'),
                                    "ê³µê¸‰ë©´ì ": article.get('area2'),
                                    "ì¸µì •ë³´": article.get('floorInfo'),
                                    "ë°©í–¥": article.get('direction'),
                                    "ì£¼ì†Œ": article.get('address', ''),
                                    "ìƒì„¸ì£¼ì†Œ": article.get('buildingName', ''),
                                    "ë“±ë¡ì¼": article.get('articleConfirmYMD'),
                                    "íƒœê·¸": article.get('tagList', []),
                                    "ì„¤ëª…": article.get('articleFeatureDesc', '')
                                }
                                
                                # ì •ë¦¬ëœ ìƒì„¸ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                                if 'ìƒì„¸ì •ë³´' in article:
                                    processed_article['ìƒì„¸ì •ë³´'] = article['ìƒì„¸ì •ë³´']
                                
                                json.dump(processed_article, output_file, ensure_ascii=False, indent=2)
                                output_file.flush()  # ì¦‰ì‹œ ë””ìŠ¤í¬ì— ì“°ê¸°
                            
                            total_collected += 1
                        
                        # ì§„í–‰ë¥  í‘œì‹œ
                        if total_expected > 0:
                            percentage = min((total_collected / total_expected * 100), 100.0)  # 100% ì´ˆê³¼ ë°©ì§€
                            progress_msg = f" / {percentage:.1f}%"
                        else:
                            # ë¬´í•œìŠ¤í¬ë¡¤ ëª¨ë“œì—ì„œëŠ” ì§„í–‰ë¥  ëŒ€ì‹  ìˆ˜ì§‘ëŸ‰ë§Œ í‘œì‹œ
                            progress_msg = ""
                        
                        detail_msg = " (ìƒì„¸ì •ë³´ í¬í•¨)" if include_details else ""
                        print(f"âœ… {len(articles)}ê°œ ìˆ˜ì§‘{detail_msg} (ëˆ„ì : {total_collected}ê°œ{progress_msg})")
                        
                        # ì‹¤ì œ ë¬´í•œìŠ¤í¬ë¡¤ì²˜ëŸ¼ í˜ì´ì§€ë‹¹ 20ê°œê°€ ê¸°ë³¸
                        if len(articles) < 20 and page > 1:
                            print("ğŸ“„ ë§ˆì§€ë§‰ í˜ì´ì§€ ê°ì§€ (20ê°œ ë¯¸ë§Œ)")
                        
                        # isMoreDataê°€ Falseë©´ ë” ì´ìƒ ë°ì´í„° ì—†ìŒ
                        if not is_more_data:
                            print("ğŸ“„ APIì—ì„œ ë” ì´ìƒ ë°ì´í„° ì—†ìŒì„ ì•Œë¦¼ (isMoreData: false)")
                            break
                    else:
                        print("ğŸ“„ ë” ì´ìƒ ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        break
                        
                else:
                    print(f"âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                    if response.status_code == 401:
                        print("ğŸ”‘ í† í°ì´ ë§Œë£Œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    break
                    
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                break
        
        # íŒŒì¼ ë§ˆë¬´ë¦¬
        if output_file:
            output_file.write('\n  ]')  # ë§¤ë¬¼ëª©ë¡ë§Œ ë‹«ê¸° (ì „ì²´ JSONì€ ìƒìœ„ì—ì„œ ë‹«ìŒ)
            output_file.flush()
        
        print(f"\nğŸ¯ ë¬´í•œìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ:")
        print(f"   - ìš”ì²­í•œ í˜ì´ì§€: {page}")
        print(f"   - ìˆ˜ì§‘ëœ ë§¤ë¬¼: {total_collected}ê°œ")
        if total_expected > 0:
            completion = min((total_collected / total_expected * 100), 100.0)
            print(f"   - ì™„ë£Œìœ¨: {completion:.1f}%")
            if total_collected > total_expected:
                print(f"   âš ï¸ ì‹¤ì œ ìˆ˜ì§‘ëŸ‰ì´ ì˜ˆìƒë³´ë‹¤ ë§ìŒ (ì˜ˆìƒ: {total_expected}ê°œ)")
        else:
            print(f"   - ë¬´í•œìŠ¤í¬ë¡¤ ëª¨ë“œ (ì§„í–‰ë¥  ê³„ì‚° ë¶ˆê°€)")
        
        return total_collected
    
    def get_dong_name_by_cortar(self, cortar_no):
        """ì§€ì—­ì½”ë“œë¡œ ë™ ì´ë¦„ ì¡°íšŒ"""
        try:
            from supabase_client import SupabaseHelper
            helper = SupabaseHelper()
            result = helper.client.table('areas').select('dong_name, gu_name').eq('cortar_no', cortar_no).execute()
            
            if result.data and len(result.data) > 0:
                area = result.data[0]
                return f"{area['gu_name']}_{area['dong_name']}"
            else:
                return None
        except Exception as e:
            print(f"âš ï¸ ë™ ì´ë¦„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def save_results(self, articles, parsed_url, cortar_no):
        """ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ë§¤ë¬¼ ì •ë³´ ì •ë¦¬
        processed_articles = []
        for article in articles:
            processed_articles.append({
                "ë§¤ë¬¼ë²ˆí˜¸": article.get('articleNo'),
                "ë§¤ë¬¼ëª…": article.get('articleName'),
                "ë¶€ë™ì‚°íƒ€ì…": article.get('realEstateTypeName'),
                "ê±°ë˜íƒ€ì…": article.get('tradeTypeName'),
                "ë§¤ë§¤ê°€ê²©": article.get('dealOrWarrantPrc', ''),
                "ì›”ì„¸": article.get('rentPrc', ''),
                "ì „ìš©ë©´ì ": article.get('area1'),
                "ê³µê¸‰ë©´ì ": article.get('area2'),
                "ì¸µì •ë³´": article.get('floorInfo'),
                "ë°©í–¥": article.get('direction'),
                "ì£¼ì†Œ": article.get('address', ''),
                "ìƒì„¸ì£¼ì†Œ": article.get('buildingName', ''),
                "ë“±ë¡ì¼": article.get('articleConfirmYmd'),
                "íƒœê·¸": article.get('tagList', []),
                "ì„¤ëª…": article.get('articleFeatureDesc', '')
            })
        
        # í†µê³„ ìƒì„±
        stats = {
            "ì´ë§¤ë¬¼ìˆ˜": len(articles),
            "ë¶€ë™ì‚°íƒ€ì…ë³„": {},
            "ê±°ë˜íƒ€ì…ë³„": {},
            "ê°€ê²©ëŒ€ë³„": {"1ì–µë¯¸ë§Œ": 0, "1-5ì–µ": 0, "5-10ì–µ": 0, "10ì–µì´ìƒ": 0}
        }
        
        for article in articles:
            # íƒ€ì…ë³„ í†µê³„
            re_type = article.get('realEstateTypeName', 'ê¸°íƒ€')
            trade_type = article.get('tradeTypeName', 'ê¸°íƒ€')
            
            stats["ë¶€ë™ì‚°íƒ€ì…ë³„"][re_type] = stats["ë¶€ë™ì‚°íƒ€ì…ë³„"].get(re_type, 0) + 1
            stats["ê±°ë˜íƒ€ì…ë³„"][trade_type] = stats["ê±°ë˜íƒ€ì…ë³„"].get(trade_type, 0) + 1
            
            # ê°€ê²©ëŒ€ë³„ í†µê³„
            price = article.get('dealOrWarrantPrc', 0)
            if isinstance(price, str):
                price = 0
            
            if price < 10000:  # 1ì–µ ë¯¸ë§Œ
                stats["ê°€ê²©ëŒ€ë³„"]["1ì–µë¯¸ë§Œ"] += 1
            elif price < 50000:  # 5ì–µ ë¯¸ë§Œ
                stats["ê°€ê²©ëŒ€ë³„"]["1-5ì–µ"] += 1
            elif price < 100000:  # 10ì–µ ë¯¸ë§Œ
                stats["ê°€ê²©ëŒ€ë³„"]["5-10ì–µ"] += 1
            else:
                stats["ê°€ê²©ëŒ€ë³„"]["10ì–µì´ìƒ"] += 1
        
        # ìµœì¢… ê²°ê³¼
        result = {
            "ìˆ˜ì§‘ì •ë³´": {
                "ìˆ˜ì§‘ì‹œê°„": timestamp,
                "ì§€ì—­ì½”ë“œ": cortar_no,
                "ì›ë³¸URL": parsed_url,
                "APIíŒ¨í„´": "ì‹¤ì œ_ë¶„ì„ëœ_íŒ¨í„´"
            },
            "í†µê³„": stats,
            "ë§¤ë¬¼ëª©ë¡": processed_articles
        }
        
        # ê²°ê³¼ í´ë”ì— ì €ì¥
        results_dir = os.path.join(os.path.dirname(__file__), 'results')
        os.makedirs(results_dir, exist_ok=True)
        
        # ë™ ì´ë¦„ ì¡°íšŒ
        dong_info = self.get_dong_name_by_cortar(cortar_no)
        if dong_info:
            filename = f"naver_fixed_{dong_info}_{cortar_no}_{timestamp}.json"
        else:
            filename = f"naver_fixed_{cortar_no}_{timestamp}.json"
        filepath = os.path.join(results_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath
    
    def collect_from_url(self, url, include_details=True, max_pages=999):
        """URL ê¸°ë°˜ ì „ì²´ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤"""
        print("ğŸš€ ë„¤ì´ë²„ ë¶€ë™ì‚° ìˆ˜ì§‘ ì‹œì‘ (ë¶„ì„ëœ ì‹¤ì œ íŒ¨í„´)")
        print("=" * 60)
        
        # 1. URL íŒŒì‹±
        parsed = self.parse_url(url)
        print(f"ğŸ“ ì¢Œí‘œ: {parsed['lat']}, {parsed['lon']} (ì¤Œ: {parsed['zoom']})")
        
        # 2. ì§€ì—­ì½”ë“œ ì¡°íšŒ
        cortar_no = self.get_cortar_code(parsed['lat'], parsed['lon'], parsed['zoom'])
        if not cortar_no:
            print("âŒ ì§€ì—­ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return
        
        # 3. íŒŒì¼ ì¤€ë¹„ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ë™ ì´ë¦„ ì¡°íšŒ
        dong_info = self.get_dong_name_by_cortar(cortar_no)
        if dong_info:
            filename = f"naver_streaming_{dong_info}_{cortar_no}_{timestamp}.json"
        else:
            filename = f"naver_streaming_{cortar_no}_{timestamp}.json"
        
        # 4. ë§¤ë¬¼ ìˆ˜ì§‘ (ì‹¤ì‹œê°„ íŒŒì¼ ì“°ê¸°)
        # ê²°ê³¼ í´ë” ìƒì„±
        results_dir = os.path.join(os.path.dirname(__file__), 'results')
        os.makedirs(results_dir, exist_ok=True)
        
        filepath = os.path.join(results_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            # ì „ì²´ JSON êµ¬ì¡° ì‹œì‘
            f.write('{\n')
            
            # ë©”íƒ€ë°ì´í„° ì‘ì„±
            f.write('  "ìˆ˜ì§‘ì •ë³´": {\n')
            f.write('    "ìˆ˜ì§‘ì‹œê°„": "' + timestamp + '",\n')
            f.write('    "ì§€ì—­ì½”ë“œ": "' + cortar_no + '",\n')
            f.write('    "ì›ë³¸URL": ' + json.dumps(parsed, ensure_ascii=False) + ',\n')
            f.write('    "ìˆ˜ì§‘ë°©ì‹": "ì‹¤ì‹œê°„_ìŠ¤íŠ¸ë¦¬ë°"\n')
            f.write('  },\n')
            
            # ìŠ¤íŠ¸ë¦¬ë° ìˆ˜ì§‘ ì‹œì‘
            total_collected = self.collect_articles(cortar_no, parsed, max_pages=max_pages, include_details=include_details, output_file=f)
            
            # ì „ì²´ JSON êµ¬ì¡° ì¢…ë£Œ
            f.write('\n}')  # ìµœì¢… ë‹«ëŠ” ì¤‘ê´„í˜¸
        
        if total_collected > 0:
            # 5. ìš”ì•½ ì¶œë ¥
            print("\n" + "=" * 60)
            print("ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ ìš”ì•½")
            print("=" * 60)
            print(f"ì´ {total_collected}ê°œ ë§¤ë¬¼ ìˆ˜ì§‘")
            print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath}")
            print("ğŸ¯ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ìˆ˜ì§‘ë¨")
            return {'success': True, 'filepath': filepath, 'count': total_collected}
        else:
            print("\nâŒ ìˆ˜ì§‘ëœ ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {'success': False, 'filepath': None, 'count': 0}

def collect_by_cortar_no(cortar_no: str, include_details: bool = True, max_pages: int = 999) -> bool:
    """cortar_noë¡œ ë§¤ë¬¼ ìˆ˜ì§‘"""
    from playwright_token_collector import PlaywrightTokenCollector
    from supabase_client import SupabaseHelper
    
    try:
        # Supabaseì—ì„œ ì§€ì—­ ì •ë³´ ì¡°íšŒ
        helper = SupabaseHelper()
        result = helper.client.table('areas').select('*').eq('cortar_no', cortar_no).execute()
        
        if not result.data:
            print(f"âŒ ì§€ì—­ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cortar_no}")
            return False
        
        area_info = result.data[0]
        dong_name = area_info['dong_name']
        center_lat = area_info['center_lat']
        center_lon = area_info['center_lon']
        
        print(f"ğŸ¯ ìˆ˜ì§‘ ëŒ€ìƒ: {dong_name} ({cortar_no})")
        print(f"ğŸ“ ì¤‘ì‹¬ì¢Œí‘œ: {center_lat}, {center_lon}")
        
        # í† í° íšë“
        print("ğŸ”‘ í† í° ìˆ˜ì§‘ ì¤‘...")
        token_collector = PlaywrightTokenCollector()
        token_data = token_collector.get_token_with_playwright()
        
        if not token_data:
            print("âŒ í† í° íšë“ ì‹¤íŒ¨")
            return False
        
        # ìˆ˜ì§‘ê¸° ìƒì„±
        collector = FixedNaverCollector(token_data)
        
        # ì§ì ‘ cortar_noë¡œ ìˆ˜ì§‘ (ë¶ˆí•„ìš”í•œ ì§€ì—­ì½”ë“œ ì¡°íšŒ ê±´ë„ˆë›°ê¸°)
        print(f"ğŸš€ ì§ì ‘ cortar_noë¡œ ìˆ˜ì§‘ ì‹œì‘: {cortar_no}")
        
        # íŒŒì¼ ì¤€ë¹„ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"naver_streaming_{dong_name}_{cortar_no}_{timestamp}.json"
        
        # ê²°ê³¼ í´ë” ìƒì„±
        results_dir = os.path.join(os.path.dirname(__file__), 'results')
        os.makedirs(results_dir, exist_ok=True)
        
        filepath = os.path.join(results_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            # ì „ì²´ JSON êµ¬ì¡° ì‹œì‘
            f.write('{\n')
            
            # ë©”íƒ€ë°ì´í„° ì‘ì„±
            f.write('  "ìˆ˜ì§‘ì •ë³´": {\n')
            f.write('    "ìˆ˜ì§‘ì‹œê°„": "' + timestamp + '",\n')
            f.write('    "ì§€ì—­ì½”ë“œ": "' + cortar_no + '",\n')
            f.write('    "ë™ì´ë¦„": "' + dong_name + '",\n')
            f.write('    "ìˆ˜ì§‘ë°©ì‹": "cortar_no_ì§ì ‘ìˆ˜ì§‘"\n')
            f.write('  },\n')
            
            # ìŠ¤íŠ¸ë¦¬ë° ìˆ˜ì§‘ ì‹œì‘ (ì§€ì—­ì½”ë“œ ì¡°íšŒ ì—†ì´ ë°”ë¡œ ìˆ˜ì§‘)
            total_collected = collector.collect_articles(
                cortar_no=cortar_no,
                parsed_url={"direct_cortar": True},
                max_pages=max_pages,
                include_details=include_details,
                output_file=f
            )
            
            # ì „ì²´ JSON êµ¬ì¡° ì¢…ë£Œ
            f.write('\n}')
        
        result = {
            'success': total_collected > 0,
            'filepath': filepath,
            'count': total_collected
        }
        
        if result['success']:
            print(f"âœ… {dong_name} ìˆ˜ì§‘ ì™„ë£Œ ({result['count']}ê°œ ë§¤ë¬¼)")
            return result
        else:
            print(f"âŒ {dong_name} ìˆ˜ì§‘ ì‹¤íŒ¨")
            return result
        
    except Exception as e:
        print(f"âŒ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return {'success': False, 'filepath': None, 'count': 0, 'error': str(e)}

def main():
    import sys
    from playwright_token_collector import PlaywrightTokenCollector
    
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì í™•ì¸
    if len(sys.argv) > 1:
        cortar_no = sys.argv[1]
        include_details = len(sys.argv) <= 2 or sys.argv[2].lower() != 'false'
        max_pages = int(sys.argv[3]) if len(sys.argv) > 3 else 999
        
        print(f"ğŸš€ ë°°ì¹˜ ëª¨ë“œ: {cortar_no} ìˆ˜ì§‘ ì‹œì‘")
        result = collect_by_cortar_no(cortar_no, include_details, max_pages)
        sys.exit(0 if result['success'] else 1)
    
    # Interactive ëª¨ë“œ (ê¸°ì¡´ ë°©ì‹)
    # í† í° ìˆ˜ì§‘
    print("ğŸ”‘ í† í° ìˆ˜ì§‘ ì¤‘...")
    token_collector = PlaywrightTokenCollector()
    token = token_collector.get_token_with_playwright()
    
    if not token:
        print("âŒ í† í° íšë“ ì‹¤íŒ¨")
        return
    
    # ìˆ˜ì§‘ê¸° ìƒì„±
    collector = FixedNaverCollector(token)
    
    # URL ì…ë ¥
    url = input("ë„¤ì´ë²„ ë¶€ë™ì‚° URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    if not url:
        print("âŒ URLì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì˜µì…˜
    print("\nğŸ“‹ ìˆ˜ì§‘ ì˜µì…˜:")
    print("1. ê¸°ë³¸ ì •ë³´ë§Œ ìˆ˜ì§‘ (ë¹ ë¦„)")
    print("2. ìƒì„¸ì •ë³´ í¬í•¨ ìˆ˜ì§‘ (ëŠë¦¼, ë” ë§ì€ ë°ì´í„°)")
    choice = input("ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2, ê¸°ë³¸ê°’: 2): ").strip()
    
    include_details = choice != "1"
    if include_details:
        print("ğŸ” ìƒì„¸ì •ë³´ í¬í•¨ ëª¨ë“œë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    else:
        print("âš¡ ê¸°ë³¸ ì •ë³´ë§Œ ë¹ ë¥´ê²Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    
    # ìˆ˜ì§‘ ì‹¤í–‰
    collector.collect_from_url(url, include_details)

if __name__ == "__main__":
    main()