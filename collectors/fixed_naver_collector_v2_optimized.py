#!/usr/bin/env python3
"""
í† í° ìºì‹± ë„¤ì´ë²„ ë¶€ë™ì‚° ìˆ˜ì§‘ê¸°
Playwright ì‚¬ìš©ì„ ìµœì†Œí™”í•˜ì—¬ í† í°ë§Œ ìºì‹±í•˜ê³  ì¬ì‚¬ìš©
"""

import requests
import json
import time
import os
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs
from kakao_address_converter import KakaoAddressConverter

class CachedTokenCollector:
    def __init__(self, use_address_converter=True):
        """í† í° ìºì‹± ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”"""
        self.token_file = os.path.join(os.path.dirname(__file__), 'cached_token.json')
        self.token = None
        self.cookies = {}
        self.token_expires_at = None
        
        # ìºì‹œëœ í† í° ë¡œë“œ ì‹œë„
        self.load_cached_token()
        
        # ì£¼ì†Œ ë³€í™˜ê¸° ì´ˆê¸°í™”
        self.address_converter = None
        if use_address_converter:
            try:
                self.address_converter = KakaoAddressConverter()
                print("ğŸ—ºï¸ ì¹´ì¹´ì˜¤ ì£¼ì†Œ ë³€í™˜ê¸° í™œì„±í™”")
            except ValueError as e:
                print(f"âš ï¸ ì£¼ì†Œ ë³€í™˜ê¸° ë¹„í™œì„±í™”: {e}")
                self.address_converter = None
    
    def load_cached_token(self):
        """ìºì‹œëœ í† í° ë¡œë“œ"""
        if os.path.exists(self.token_file):
            try:
                with open(self.token_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                # í† í° ë§Œë£Œ í™•ì¸
                expires_at = datetime.fromisoformat(cache_data['expires_at'])
                if datetime.now() < expires_at:
                    self.token = cache_data['token']
                    # ì¿ í‚¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                    cookies_list = cache_data.get('cookies', [])
                    if isinstance(cookies_list, list):
                        self.cookies = {cookie['name']: cookie['value'] for cookie in cookies_list}
                    else:
                        self.cookies = cookies_list
                    self.token_expires_at = expires_at
                    print(f"âœ… ìºì‹œëœ í† í° ë¡œë“œ ì„±ê³µ (ë§Œë£Œ: {expires_at.strftime('%Y-%m-%d %H:%M:%S')})")
                    return True
                else:
                    print(f"â° ìºì‹œëœ í† í° ë§Œë£Œë¨ (ë§Œë£Œì‹œê°„: {expires_at.strftime('%Y-%m-%d %H:%M:%S')})")
                    
            except Exception as e:
                print(f"âš ï¸ ìºì‹œëœ í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("ğŸ”„ ìƒˆë¡œìš´ í† í° ìˆ˜ì§‘ í•„ìš”")
        return False
    
    def save_token_cache(self, token_data, expires_hours=6):
        """í† í° ìºì‹œ ì €ì¥"""
        try:
            expires_at = datetime.now() + timedelta(hours=expires_hours)
            
            cache_data = {
                'token': token_data['token'],
                'cookies': token_data.get('cookies', []),
                'cached_at': datetime.now().isoformat(),
                'expires_at': expires_at.isoformat()
            }
            
            with open(self.token_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            self.token = cache_data['token']
            self.cookies = {cookie['name']: cookie['value'] for cookie in cache_data['cookies']}
            self.token_expires_at = expires_at
            
            print(f"ğŸ’¾ í† í° ìºì‹œ ì €ì¥ ì™„ë£Œ (ë§Œë£Œ: {expires_at.strftime('%Y-%m-%d %H:%M:%S')})")
            return True
            
        except Exception as e:
            print(f"âŒ í† í° ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def get_fresh_token(self):
        """ìƒˆë¡œìš´ í† í° ìˆ˜ì§‘ (Playwright ì‚¬ìš©)"""
        print("ğŸ­ Playwrightë¡œ ìƒˆë¡œìš´ í† í° ìˆ˜ì§‘ ì¤‘...")
        
        try:
            from playwright_token_collector import PlaywrightTokenCollector
            
            token_collector = PlaywrightTokenCollector()
            token_data = token_collector.get_token_with_playwright()
            
            if token_data:
                # í† í° ìºì‹œ ì €ì¥ (6ì‹œê°„ ìœ íš¨)
                if self.save_token_cache(token_data, expires_hours=6):
                    print("âœ… ìƒˆë¡œìš´ í† í° ìˆ˜ì§‘ ë° ìºì‹± ì™„ë£Œ")
                    return True
            else:
                print("âŒ ìƒˆë¡œìš´ í† í° ìˆ˜ì§‘ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ í† í° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        return False
    
    def ensure_valid_token(self):
        """ìœ íš¨í•œ í† í° í™•ë³´"""
        # í† í°ì´ ì—†ê±°ë‚˜ ë§Œë£Œ ì„ë°• ì‹œ ìƒˆë¡œ ìˆ˜ì§‘
        if not self.token or (self.token_expires_at and datetime.now() > self.token_expires_at - timedelta(minutes=30)):
            print("ğŸ”„ í† í° ê°±ì‹  í•„ìš”")
            return self.get_fresh_token()
        
        print("âœ… ìœ íš¨í•œ í† í° ì¡´ì¬")
        return True
    
    def setup_headers(self):
        """API ìš”ì²­ í—¤ë” ì„¤ì •"""
        headers = {
            'authorization': f'Bearer {self.token}',
            'User-Agent': self.get_random_user_agent(),
            'Accept': 'application/json',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://new.land.naver.com/',
            'Origin': 'https://new.land.naver.com',
            'Sec-Ch-Ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'Cache-Control': 'no-cache'
        }
        return headers
    
    def get_random_user_agent(self):
        """ëœë¤ User-Agent"""
        agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0"
        ]
        return random.choice(agents)
    
    def parse_url(self, url):
        """URL íŒŒì‹±"""
        parsed = urlparse(url)
        query = parse_qs(parsed.query)
        
        ms = query.get('ms', [''])[0]
        if ms:
            parts = ms.split(',')
            lat, lon, zoom = float(parts[0]), float(parts[1]), int(parts[2])
        else:
            lat, lon, zoom = 37.5665, 126.9780, 15
        
        return {
            'lat': lat,
            'lon': lon,
            'zoom': zoom,
            'article_types': query.get('a', [''])[0],
            'purpose': query.get('e', [''])[0]
        }
    
    def get_cortar_code(self, lat, lon, zoom):
        """ì§€ì—­ì½”ë“œ ì¡°íšŒ"""
        url = "https://new.land.naver.com/api/cortars"
        params = {
            'zoom': zoom,
            'centerLat': lat,
            'centerLon': lon
        }
        
        print(f"ğŸ” ì§€ì—­ì½”ë“œ ì¡°íšŒ: ìœ„ë„ {lat}, ê²½ë„ {lon}")
        
        try:
            headers = self.setup_headers()
            time.sleep(random.uniform(1, 2))
            
            response = requests.get(url, headers=headers, params=params, cookies=self.cookies, timeout=10)
            print(f"ğŸ“Š ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                if data and 'cortarNo' in data:
                    cortar_no = data.get('cortarNo')
                    cortar_name = data.get('cortarName')
                    print(f"âœ… ì§€ì—­: {cortar_name} (ì½”ë“œ: {cortar_no})")
                    return cortar_no
                elif data and isinstance(data, list) and len(data) > 0:
                    cortar = data[0]
                    cortar_no = cortar.get('cortarNo')
                    cortar_name = cortar.get('cortarName')
                    print(f"âœ… ì§€ì—­: {cortar_name} (ì½”ë“œ: {cortar_no})")
                    return cortar_no
            elif response.status_code == 401:
                print("ğŸ”„ í† í° ë§Œë£Œ ê°ì§€, ìƒˆë¡œìš´ í† í° ìˆ˜ì§‘ ì¤‘...")
                if self.get_fresh_token():
                    return self.get_cortar_code(lat, lon, zoom)  # ì¬ì‹œë„
            else:
                print(f"âŒ ì§€ì—­ì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ ì§€ì—­ì½”ë“œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        
        return None
    
    def get_article_detail(self, article_no):
        """ê°œë³„ ë§¤ë¬¼ ìƒì„¸ì •ë³´ ìˆ˜ì§‘"""
        url = f"https://new.land.naver.com/api/articles/{article_no}"
        params = {'complexNo': ''}
        
        try:
            headers = self.setup_headers()
            time.sleep(random.uniform(0.5, 1.0))  # ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ê°„ê²©
            
            response = requests.get(url, headers=headers, params=params, cookies=self.cookies, timeout=10)
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 401:
                print(f"ğŸ”„ ë§¤ë¬¼ {article_no} ìƒì„¸ì •ë³´ - í† í° ë§Œë£Œ")
                if self.get_fresh_token():
                    headers = self.setup_headers()
                    response = requests.get(url, headers=headers, params=params, cookies=self.cookies, timeout=10)
                    if response.status_code == 200:
                        return response.json()
            else:
                print(f"âš ï¸ ë§¤ë¬¼ {article_no} ìƒì„¸ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ ë§¤ë¬¼ {article_no} ìƒì„¸ì •ë³´ ì˜¤ë¥˜: {e}")
            return None
    
    def extract_useful_details(self, detail_data):
        """ìƒì„¸ì •ë³´ì—ì„œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ"""
        if not detail_data:
            return None
            
        article_detail = detail_data.get('articleDetail', {})
        article_addition = detail_data.get('articleAddition', {})
        
        # ì¢Œí‘œì—ì„œ ì£¼ì†Œ ë³€í™˜ (ì¹´ì¹´ì˜¤ API ì‚¬ìš©)
        converted_address = None
        latitude = article_detail.get('latitude')
        longitude = article_detail.get('longitude')
        
        if self.address_converter and latitude and longitude:
            try:
                converted_address = self.address_converter.convert_coord_to_address(
                    str(latitude), str(longitude)
                )
            except Exception as e:
                print(f"âš ï¸ ì£¼ì†Œ ë³€í™˜ ì˜¤ë¥˜: {e}")
        
        # í•µì‹¬ ì •ë³´ë§Œ ì„ ë³„
        useful_info = {
            # ğŸ¢ ê±´ë¬¼ ìƒì„¸ ì •ë³´
            "ê±´ë¬¼ì •ë³´": {
                "ë²•ì ìš©ë„": article_detail.get('lawUsage'),
                "ì£¼ì°¨ëŒ€ìˆ˜": article_detail.get('parkingCount'),
                "ì£¼ì°¨ê°€ëŠ¥": "ì˜ˆ" if article_detail.get('parkingPossibleYN') == 'Y' else "ì•„ë‹ˆì˜¤",
                "ì—˜ë¦¬ë² ì´í„°": "ìˆìŒ" if "ì—˜ë¦¬ë² ì´í„°" in article_detail.get('tagList', []) else "ì—†ìŒ",
                "ì¸µêµ¬ì¡°": article_detail.get('floorLayerName')
            },
            
            # ğŸ“ ìœ„ì¹˜ ì •ë³´  
            "ìœ„ì¹˜ì •ë³´": {
                "ì •í™•í•œ_ìœ„ë„": article_detail.get('latitude'),
                "ì •í™•í•œ_ê²½ë„": article_detail.get('longitude'),
                "ìƒì„¸ì£¼ì†Œ": article_detail.get('exposureAddress'),
                "ì§€í•˜ì² ë„ë³´ì‹œê°„": f"{article_detail.get('walkingTimeToNearSubway', 0)}ë¶„"
            },
            
            # ğŸ’° ê´€ë¦¬ë¹„ ì •ë³´
            "ë¹„ìš©ì •ë³´": {
                "ì›”ê´€ë¦¬ë¹„": article_detail.get('monthlyManagementCost'),
                "ê´€ë¦¬ë¹„_í¬í•¨í•­ëª©": "ì „ê¸°, ìˆ˜ë„, ì¸í„°ë„· ë“±"  # í•„ìš”ì‹œ ìƒì„¸ íŒŒì‹±
            },
            
            # ğŸ  ì…ì£¼ ì •ë³´
            "ì…ì£¼ì •ë³´": {
                "ì…ì£¼ê°€ëŠ¥ì¼": article_detail.get('moveInTypeName'),
                "í˜‘ì˜ê°€ëŠ¥ì—¬ë¶€": article_detail.get('moveInDiscussionPossibleYN')
            },
            
            # ğŸ“· ì´ë¯¸ì§€ ì •ë³´
            "ì´ë¯¸ì§€": {
                "í˜„ì¥ì‚¬ì§„ìˆ˜": article_addition.get('siteImageCount', 0),
                "ëŒ€í‘œì´ë¯¸ì§€": article_addition.get('representativeImgUrl')
            },
            
            # ğŸ˜ï¸ ì£¼ë³€ ì‹œì„¸
            "ì£¼ë³€ì‹œì„¸": {
                "ë™ì¼ì£¼ì†Œ_ë§¤ë¬¼ìˆ˜": article_addition.get('sameAddrCnt', 0),
                "ìµœê³ ê°€": article_addition.get('sameAddrMaxPrc'),
                "ìµœì €ê°€": article_addition.get('sameAddrMinPrc')
            },
            
            # ğŸ“ ìƒì„¸ ì„¤ëª… (ì •ë¦¬ë¨)
            "ìƒì„¸ì„¤ëª…": article_detail.get('detailDescription', '').strip()[:200]  # 200ìë¡œ ì œí•œ
        }
        
        # ë³€í™˜ëœ ì£¼ì†Œ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if converted_address:
            useful_info["ì¹´ì¹´ì˜¤ì£¼ì†Œë³€í™˜"] = converted_address
        
        return useful_info
    
    def collect_articles(self, cortar_no, parsed_url, max_pages=999, include_details=True, output_file=None):
        """ë§¤ë¬¼ ìˆ˜ì§‘ (ìºì‹œëœ í† í° ì‚¬ìš©)"""
        if not self.ensure_valid_token():
            print("âŒ ìœ íš¨í•œ í† í°ì„ í™•ë³´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        url = "https://new.land.naver.com/api/articles"
        headers = self.setup_headers()
        
        base_params = {
            'cortarNo': cortar_no,
            'order': 'rank',
            'realEstateType': 'SG:SMS:GJCG:APTHGJ:GM:TJ',
            'tradeType': '',
            'tag': '::::::::',
            'rentPriceMin': '0',
            'rentPriceMax': '900000000',
            'priceMin': '0',
            'priceMax': '900000000',
            'areaMin': '0',
            'areaMax': '900000000',
            'oldBuildYears': '',
            'recentlyBuildYears': '',
            'minHouseHoldCount': '',
            'maxHouseHoldCount': '',
            'showArticle': 'false',
            'sameAddressGroup': 'false',
            'minMaintenanceCost': '',
            'maxMaintenanceCost': '',
            'priceType': 'RETAIL',
            'directions': '',
            'articleState': ''
        }
        
        if output_file:
            output_file.write('  "ë§¤ë¬¼ëª©ë¡": [\n')
            is_first_article = True
        
        total_collected = 0
        
        print("ğŸ”„ ìºì‹œëœ í† í°ìœ¼ë¡œ ë§¤ë¬¼ ìˆ˜ì§‘ ì‹œì‘")
        
        for page in range(1, min(max_pages + 1, 10)):  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10í˜ì´ì§€ ì œí•œ
            params = base_params.copy()
            params['page'] = page
            
            print(f"ğŸ“„ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
            
            try:
                # ìµœì í™”ëœ ëŒ€ê¸°ì‹œê°„
                delay = random.uniform(2, 4)
                time.sleep(delay)
                
                response = requests.get(url, headers=headers, params=params, cookies=self.cookies, timeout=15)
                
                if response.status_code == 200:
                    data = response.json()
                    articles = data.get('articleList', [])
                    is_more_data = data.get('isMoreData', False)
                    
                    if page == 1:
                        total_count = data.get('articleCount', 0)
                        print(f"ğŸ“Š ì „ì²´ ë§¤ë¬¼ ìˆ˜: {total_count}")
                    
                    if articles:
                        for article in articles:
                            # ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
                            if include_details:
                                article_no = article.get('articleNo')
                                if article_no:
                                    detail = self.get_article_detail(article_no)
                                    if detail:
                                        # í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥
                                        useful_details = self.extract_useful_details(detail)
                                        if useful_details:
                                            article['ìƒì„¸ì •ë³´'] = useful_details
                            
                            # ì‹¤ì‹œê°„ íŒŒì¼ ì“°ê¸°
                            if output_file:
                                if not is_first_article:
                                    output_file.write(',\n')
                                else:
                                    is_first_article = False
                                
                                processed_article = {
                                    "ë§¤ë¬¼ë²ˆí˜¸": article.get('articleNo'),
                                    "ë§¤ë¬¼ëª…": article.get('articleName'),
                                    "ë¶€ë™ì‚°íƒ€ì…": article.get('realEstateTypeName'),
                                    "ê±°ë˜íƒ€ì…": article.get('tradeTypeName'),
                                    "ë§¤ë§¤ê°€ê²©": article.get('dealOrWarrantPrc', ''),
                                    "ì›”ì„¸": article.get('rentPrc', ''),
                                    "ì „ìš©ë©´ì ": article.get('area1'),
                                    "ê³µê¸‰ë©´ì ": article.get('area2'),
                                    "ì¸µì •ë³´": article.get('floorInfo'),
                                    "ë°©í–¥": article.get('direction'),
                                    "ì£¼ì†Œ": article.get('address', ''),
                                    "ìƒì„¸ì£¼ì†Œ": article.get('buildingName', ''),
                                    "ë“±ë¡ì¼": article.get('articleConfirmYMD'),
                                    "íƒœê·¸": article.get('tagList', []),
                                    "ì„¤ëª…": article.get('articleFeatureDesc', '')
                                }
                                
                                # ì •ë¦¬ëœ ìƒì„¸ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                                if 'ìƒì„¸ì •ë³´' in article:
                                    processed_article['ìƒì„¸ì •ë³´'] = article['ìƒì„¸ì •ë³´']
                                
                                json.dump(processed_article, output_file, ensure_ascii=False, indent=2)
                                output_file.flush()
                            
                            total_collected += 1
                        
                        print(f"âœ… {len(articles)}ê°œ ìˆ˜ì§‘ (ëˆ„ì : {total_collected}ê°œ)")
                        
                        if not is_more_data:
                            print("ğŸ“„ ë” ì´ìƒ ë°ì´í„° ì—†ìŒ")
                            break
                    else:
                        print("ğŸ“„ ë” ì´ìƒ ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        break
                        
                elif response.status_code == 401:
                    print("ğŸ”„ í† í° ë§Œë£Œ, ìƒˆë¡œìš´ í† í° ìˆ˜ì§‘ ì¤‘...")
                    if self.get_fresh_token():
                        headers = self.setup_headers()  # í—¤ë” ê°±ì‹ 
                        continue  # ê°™ì€ í˜ì´ì§€ ì¬ì‹œë„
                    else:
                        print("âŒ í† í° ê°±ì‹  ì‹¤íŒ¨")
                        break
                else:
                    print(f"âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                    break
                    
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                break
        
        if output_file:
            output_file.write('\n  ]')
            output_file.flush()
        
        print(f"\nğŸ¯ ìºì‹œëœ í† í° ìˆ˜ì§‘ ì™„ë£Œ: {total_collected}ê°œ ë§¤ë¬¼")
        return total_collected
    
    def collect_from_url(self, url, include_details=False, max_pages=10):
        """URL ê¸°ë°˜ ìˆ˜ì§‘ (í…ŒìŠ¤íŠ¸ìš©)"""
        print("ğŸš€ ìºì‹œëœ í† í° ë„¤ì´ë²„ ë¶€ë™ì‚° ìˆ˜ì§‘ ì‹œì‘")
        print("=" * 60)
        
        # í† í° í™•ë³´
        if not self.ensure_valid_token():
            print("âŒ í† í° í™•ë³´ ì‹¤íŒ¨")
            return {'success': False, 'count': 0}
        
        # URL íŒŒì‹±
        parsed = self.parse_url(url)
        print(f"ğŸ“ ì¢Œí‘œ: {parsed['lat']}, {parsed['lon']}")
        
        # ì§€ì—­ì½”ë“œ ì¡°íšŒ
        cortar_no = self.get_cortar_code(parsed['lat'], parsed['lon'], parsed['zoom'])
        if not cortar_no:
            print("âŒ ì§€ì—­ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {'success': False, 'count': 0}
        
        # íŒŒì¼ ì¤€ë¹„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"naver_cached_token_{cortar_no}_{timestamp}.json"
        
        results_dir = os.path.join(os.path.dirname(__file__), 'results')
        os.makedirs(results_dir, exist_ok=True)
        
        filepath = os.path.join(results_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('{\n')
            f.write('  "ìˆ˜ì§‘ì •ë³´": {\n')
            f.write('    "ìˆ˜ì§‘ì‹œê°„": "' + timestamp + '",\n')
            f.write('    "ì§€ì—­ì½”ë“œ": "' + cortar_no + '",\n')
            f.write('    "ì›ë³¸URL": ' + json.dumps(parsed, ensure_ascii=False) + ',\n')
            f.write('    "ìˆ˜ì§‘ë°©ì‹": "ìºì‹œëœ_í† í°_ê¸°ë°˜"\n')
            f.write('  },\n')
            
            total_collected = self.collect_articles(
                cortar_no=cortar_no,
                parsed_url=parsed,
                max_pages=max_pages,
                include_details=include_details,
                output_file=f
            )
            
            f.write('\n}')
        
        if total_collected > 0:
            print(f"\nâœ… ìˆ˜ì§‘ ì„±ê³µ: {total_collected}ê°œ ë§¤ë¬¼")
            print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath}")
            return {'success': True, 'filepath': filepath, 'count': total_collected}
        else:
            return {'success': False, 'count': 0}

def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    collector = CachedTokenCollector()
    
    # í…ŒìŠ¤íŠ¸ URL
    url = "https://new.land.naver.com/offices?ms=37.4986291,127.0359669,13&a=SG:SMS:GJCG:APTHGJ:GM:TJ&e=RETAIL"
    
    # ìˆ˜ì§‘ ì‹¤í–‰ (ìƒì„¸ì •ë³´ í¬í•¨ í…ŒìŠ¤íŠ¸, 1í˜ì´ì§€ë§Œ)
    result = collector.collect_from_url(url, include_details=True, max_pages=1)
    
    if result['success']:
        print(f"ğŸ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {result['count']}ê°œ ë§¤ë¬¼ ìˆ˜ì§‘")
    else:
        print("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

# ==============================
# ë³‘ë ¬ ì²˜ë¦¬ìš© ë…ë¦½ í•¨ìˆ˜ 
# ==============================

def collect_by_cortar_no(cortar_no: str, include_details: bool = True, max_pages: int = 999):
    """cortar_noë¡œ ë§¤ë¬¼ ìˆ˜ì§‘ (ìµœì í™”ëœ ìºì‹œ í† í° ë²„ì „)"""
    from supabase_client import SupabaseHelper
    
    try:
        # Supabaseì—ì„œ ì§€ì—­ ì •ë³´ ì¡°íšŒ
        helper = SupabaseHelper()
        result = helper.client.table('areas').select('*').eq('cortar_no', cortar_no).execute()
        
        if not result.data:
            print(f"âŒ ì§€ì—­ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cortar_no}")
            return {'success': False, 'filepath': None, 'count': 0, 'error': 'ì§€ì—­ ì •ë³´ ì—†ìŒ'}
        
        area_info = result.data[0]
        dong_name = area_info['dong_name']
        center_lat = area_info['center_lat']
        center_lon = area_info['center_lon']
        
        print(f"ğŸ¯ ìˆ˜ì§‘ ëŒ€ìƒ: {dong_name} ({cortar_no})")
        print(f"ğŸ“ ì¤‘ì‹¬ì¢Œí‘œ: {center_lat}, {center_lon}")
        
        # ìµœì í™”ëœ ìˆ˜ì§‘ê¸° ìƒì„± (í† í° ìºì‹± ì‚¬ìš©)
        collector = CachedTokenCollector()
        
        # í† í° í™•ë³´
        if not collector.ensure_valid_token():
            print("âŒ í† í° í™•ë³´ ì‹¤íŒ¨")
            return {'success': False, 'filepath': None, 'count': 0, 'error': 'í† í° í™•ë³´ ì‹¤íŒ¨'}
        
        print(f"ğŸš€ ìµœì í™”ëœ ìˆ˜ì§‘ ì‹œì‘: {dong_name}")
        
        # íŒŒì¼ ì¤€ë¹„ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"naver_optimized_{dong_name}_{cortar_no}_{timestamp}.json"
        
        results_dir = os.path.join(os.path.dirname(__file__), 'results')
        os.makedirs(results_dir, exist_ok=True)
        
        filepath = os.path.join(results_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('{\n')
            f.write('  "ìˆ˜ì§‘ì •ë³´": {\n')
            f.write('    "ìˆ˜ì§‘ì‹œê°„": "' + timestamp + '",\n')
            f.write('    "ì§€ì—­ì½”ë“œ": "' + cortar_no + '",\n')
            f.write('    "ë™ì´ë¦„": "' + dong_name + '",\n')
            f.write('    "ìˆ˜ì§‘ë°©ì‹": "ìµœì í™”ëœ_ìºì‹œí† í°_ê¸°ë°˜",\n')
            f.write('    "ë²„ì „": "v2.0_optimized"\n')
            f.write('  },\n')
            
            # ì§ì ‘ ìˆ˜ì§‘ (ì§€ì—­ì½”ë“œ ì¡°íšŒ ìƒëµ)
            total_collected = collector.collect_articles(
                cortar_no=cortar_no,
                parsed_url={"direct_cortar": True, "dong_name": dong_name},
                max_pages=max_pages,
                include_details=include_details,
                output_file=f
            )
            
            f.write('\n}')
        
        result = {
            'success': total_collected > 0,
            'filepath': filepath,
            'count': total_collected,
            'dong_name': dong_name
        }
        
        if result['success']:
            print(f"âœ… {dong_name} ìµœì í™” ìˆ˜ì§‘ ì™„ë£Œ ({result['count']}ê°œ ë§¤ë¬¼)")
            return result
        else:
            print(f"âŒ {dong_name} ìˆ˜ì§‘ ì‹¤íŒ¨")
            return result
        
    except Exception as e:
        print(f"âŒ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return {'success': False, 'filepath': None, 'count': 0, 'error': str(e)}

if __name__ == "__main__":
    main()