#!/usr/bin/env python3
"""
ë°”ë¡œ DB ì €ì¥ ë„¤ì´ë²„ ë¶€ë™ì‚° ìˆ˜ì§‘ê¸° (í†µí•© ë¡œê·¸ ì‹œìŠ¤í…œ ì ìš©)
- JSON íŒŒì¼ ìƒëµ, ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ DBë¡œ ì €ì¥
- ì‹¤ì‹œê°„ DB ì—…ë°ì´íŠ¸ ë° í–¥ìƒëœ í†µí•© ë¡œê·¸ ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ìµœì í™” ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
- API í˜¸ì¶œ ì¶”ì  ë° ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
"""

import sys
import time
import argparse
import random
import json
import os
import requests
from datetime import datetime, date
from typing import Dict, List, Optional, Any
from multiprocessing import Pool
from integrated_logger import IntegratedProgressTracker, integrated_log_based_collection, LogLevel
from fixed_naver_collector_v2_optimized import CachedTokenCollector
from supabase_client import SupabaseHelper


class DirectDBCollector:
    """ë°”ë¡œ DB ì €ì¥ ìˆ˜ì§‘ê¸° (í–¥ìƒëœ ë¡œê·¸ ì‹œìŠ¤í…œ)"""
    
    def __init__(self, log_level: LogLevel = LogLevel.INFO):
        self.tracker = IntegratedProgressTracker(log_level=log_level)
        self.helper = SupabaseHelper()
        
        # ê°•ë‚¨êµ¬ ë™ë³„ ì •ë³´ (ìš°ì„ ìˆœìœ„ í¬í•¨)
        self.gangnam_dongs = [
            {"name": "ì—­ì‚¼ë™", "cortar_no": "1168010100", "priority": 30},
            {"name": "ì‚¼ì„±ë™", "cortar_no": "1168010500", "priority": 26},
            {"name": "ë…¼í˜„ë™", "cortar_no": "1168010800", "priority": 23},
            {"name": "ëŒ€ì¹˜ë™", "cortar_no": "1168010600", "priority": 22},
            {"name": "ì‹ ì‚¬ë™", "cortar_no": "1168010700", "priority": 22},
            {"name": "ì••êµ¬ì •ë™", "cortar_no": "1168011000", "priority": 20},
            {"name": "ì²­ë‹´ë™", "cortar_no": "1168010400", "priority": 18},
            {"name": "ë„ê³¡ë™", "cortar_no": "1168011800", "priority": 18},
            {"name": "ê°œí¬ë™", "cortar_no": "1168010300", "priority": 17},
            {"name": "ìˆ˜ì„œë™", "cortar_no": "1168011500", "priority": 12},
            {"name": "ì¼ì›ë™", "cortar_no": "1168011400", "priority": 11},
            {"name": "ìê³¡ë™", "cortar_no": "1168011200", "priority": 8},
            {"name": "ì„¸ê³¡ë™", "cortar_no": "1168011100", "priority": 6},
            {"name": "ìœ¨í˜„ë™", "cortar_no": "1168011300", "priority": 5}
        ]
        
        self.tracker.enhanced_logger.info("direct_db_collector", "ë°”ë¡œ DB ì €ì¥ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def convert_article_to_db_format(self, article: Dict, cortar_no: str, collected_date: date) -> Dict:
        """ìˆ˜ì§‘ëœ ë§¤ë¬¼ ë°ì´í„°ë¥¼ DB í˜•ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ë³€í™˜"""
        
        # ìƒì„¸ì •ë³´ ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
        details_info = article.get('ìƒì„¸ì •ë³´', {})
        kakao_addr = details_info.get('ì¹´ì¹´ì˜¤ì£¼ì†Œë³€í™˜', {})
        
        db_property = {
            'article_no': str(article.get('articleNo', article.get('ë§¤ë¬¼ë²ˆí˜¸', ''))),
            'cortar_no': cortar_no,
            'article_name': article.get('articleName', article.get('ë§¤ë¬¼ëª…', '')),
            'real_estate_type': article.get('realEstateTypeName', article.get('ë¶€ë™ì‚°íƒ€ì…', '')),
            'trade_type': article.get('tradeTypeName', article.get('ê±°ë˜íƒ€ì…', '')),
            'price': self._parse_price(article.get('dealOrWarrantPrc', article.get('ë§¤ë§¤ê°€ê²©', 0))),
            'rent_price': self._parse_price(article.get('rentPrc', article.get('ì›”ì„¸', 0))),
            'area1': self._parse_area(article.get('area1', article.get('ì „ìš©ë©´ì '))),
            'area2': self._parse_area(article.get('area2', article.get('ê³µê¸‰ë©´ì '))),
            'floor_info': article.get('floorInfo', article.get('ì¸µì •ë³´', '')),
            'direction': article.get('direction', article.get('ë°©í–¥', '')),
            'latitude': details_info.get('ìœ„ì¹˜ì •ë³´', {}).get('ì •í™•í•œ_ìœ„ë„'),
            'longitude': details_info.get('ìœ„ì¹˜ì •ë³´', {}).get('ì •í™•í•œ_ê²½ë„'),
            'address_road': kakao_addr.get('ë„ë¡œëª…ì£¼ì†Œ', ''),
            'address_jibun': kakao_addr.get('ì§€ë²ˆì£¼ì†Œ', ''),
            'address_detail': article.get('buildingName', article.get('ìƒì„¸ì£¼ì†Œ', '')),
            'building_name': kakao_addr.get('ê±´ë¬¼ëª…', article.get('buildingName', article.get('ìƒì„¸ì£¼ì†Œ', ''))),
            'postal_code': kakao_addr.get('ìš°í¸ë²ˆí˜¸', ''),
            'tag_list': article.get('tagList', article.get('íƒœê·¸', [])),
            'description': article.get('articleFeatureDesc', article.get('ì„¤ëª…', '')),
            'details': details_info if details_info else {},
            'collected_date': collected_date.isoformat(),
            'last_seen_date': collected_date.isoformat(),
            'is_active': True,
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        return db_property
    
    def _parse_price(self, price_str: Any) -> Optional[int]:
        """ê°€ê²© ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜"""
        if isinstance(price_str, (int, float)):
            return int(price_str)
        if isinstance(price_str, str):
            # "5ì–µ 3,000" ê°™ì€ í˜•ì‹ ì²˜ë¦¬
            price_str = price_str.replace(',', '').replace('ì–µ', '0000').replace('ë§Œ', '')
            try:
                return int(price_str)
            except:
                return 0
        return 0
    
    def _parse_area(self, area_str: Any) -> Optional[float]:
        """ë©´ì  ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜"""
        if isinstance(area_str, (int, float)):
            return float(area_str)
        if isinstance(area_str, str):
            try:
                return float(area_str.replace('ã¡', '').strip())
            except:
                return None
        return None
    
    def collect_dong_direct_db(self, dong_info: Dict) -> Dict:
        """
        ë™ë³„ ìˆ˜ì§‘ + ë°”ë¡œ DB ì €ì¥ (í–¥ìƒëœ ë¡œê·¸ ì‹œìŠ¤í…œ ì ìš©)
        - JSON íŒŒì¼ ìƒì„± ìƒëµ
        - ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ DBë¡œ ì‹¤ì‹œê°„ ì €ì¥
        - API í˜¸ì¶œ ì¶”ì  ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        """
        dong_name = dong_info["name"]
        cortar_no = dong_info["cortar_no"]
        
        print(f"\nğŸš€ {dong_name} ë°”ë¡œ DB ìˆ˜ì§‘ ì‹œì‘...")
        
        with integrated_log_based_collection(dong_name, cortar_no, self.tracker) as ctx:
            try:
                # 1. í† í° ìºì‹± ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
                collector = CachedTokenCollector(use_address_converter=True)
                
                if not collector.ensure_valid_token():
                    raise Exception("í† í° í™•ë³´ ì‹¤íŒ¨")
                
                ctx['enhanced_logger'].info("token_management", f"{dong_name} í† í° í™•ë³´ ì™„ë£Œ")
                
                # 2. DB ì €ì¥ìš© ë°°ì¹˜ ì„¤ì •
                batch_size = 50  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ë°°ì¹˜ í¬ê¸°
                db_batch = []
                total_collected = 0
                today = date.today()
                
                print(f"  ğŸ” {dong_name} API ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")
                ctx['enhanced_logger'].info("api_streaming", f"{dong_name} ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘", 
                                          {'batch_size': batch_size, 'cortar_no': cortar_no})
                
                # 3. í˜ì´ì§€ë³„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                url = "https://new.land.naver.com/api/articles"
                headers = collector.setup_headers()
                
                base_params = {
                    'cortarNo': cortar_no,
                    'order': 'rank',
                    'realEstateType': 'SG:SMS:GJCG:APTHGJ:GM:TJ',
                    'tradeType': '',
                    'tag': '::::::::',
                    'rentPriceMin': '0',
                    'rentPriceMax': '900000000',
                    'priceMin': '0',
                    'priceMax': '900000000',
                    'areaMin': '0',
                    'areaMax': '900000000',
                    'oldBuildYears': '',
                    'recentlyBuildYears': '',
                    'minHouseHoldCount': '',
                    'maxHouseHoldCount': '',
                    'showArticle': 'false',
                    'sameAddressGroup': 'false',
                    'minMaintenanceCost': '',
                    'maxMaintenanceCost': '',
                    'priceType': 'RETAIL',
                    'directions': '',
                    'articleState': ''
                }
                
                page = 1
                max_pages = 999  # ëª¨ë“  í˜ì´ì§€ ìˆ˜ì§‘
                consecutive_failures = 0
                max_consecutive_failures = 3
                
                while page <= max_pages:
                    params = base_params.copy()
                    params['page'] = page
                    
                    print(f"    ğŸ“„ í˜ì´ì§€ {page} ìŠ¤íŠ¸ë¦¬ë° ì¤‘...")
                    ctx['enhanced_logger'].debug("page_processing", f"{dong_name} í˜ì´ì§€ {page} ì‹œì‘")
                    
                    try:
                        # API ìš”ì²­ ì‹œì‘ ì‹œê°„ ê¸°ë¡
                        api_start_time = time.time()
                        
                        # API ìš”ì²­ ëŒ€ê¸°
                        delay = random.uniform(2, 4)
                        time.sleep(delay)
                        
                        response = requests.get(url, headers=headers, params=params, 
                                              cookies=collector.cookies, timeout=15)
                        
                        api_duration = time.time() - api_start_time
                        
                        # API í˜¸ì¶œ ë¡œê·¸ ê¸°ë¡
                        ctx['log_api_call'](
                            endpoint='/api/articles',
                            method='GET',
                            status_code=response.status_code,
                            duration=api_duration,
                            request_size=len(str(params)),
                            response_size=len(response.content) if response.content else 0,
                            extra={'page': page, 'dong_name': dong_name}
                        )
                        
                        if response.status_code == 200:
                            consecutive_failures = 0  # ì„±ê³µì‹œ ì‹¤íŒ¨ ì¹´ìš´í„° ë¦¬ì…‹
                            
                            data = response.json()
                            articles = data.get('articleList', [])
                            is_more_data = data.get('isMoreData', False)
                            
                            ctx['enhanced_logger'].debug("api_response", 
                                                       f"í˜ì´ì§€ {page} ì‘ë‹µ: {len(articles)}ê°œ ë§¤ë¬¼",
                                                       {'articles_count': len(articles), 
                                                        'is_more_data': is_more_data})
                            
                            if not articles:
                                print("    ğŸ“„ ë” ì´ìƒ ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                                ctx['enhanced_logger'].info("api_response", f"{dong_name} ë§¤ë¬¼ ìˆ˜ì§‘ ì™„ë£Œ - ë¹ˆ ì‘ë‹µ")
                                break
                            
                            # 4. ë§¤ë¬¼ë³„ ì‹¤ì‹œê°„ ì²˜ë¦¬
                            for article_idx, article in enumerate(articles):
                                try:
                                    # ìƒì„¸ì •ë³´ ìˆ˜ì§‘ (í•„ìš”ì‹œ)
                                    article_no = article.get('articleNo')
                                    if article_no:
                                        detail_start_time = time.time()
                                        detail = collector.get_article_detail(article_no)
                                        detail_duration = time.time() - detail_start_time
                                        
                                        # ìƒì„¸ì •ë³´ API í˜¸ì¶œ ë¡œê·¸
                                        ctx['log_api_call'](
                                            endpoint=f'/api/articles/{article_no}',
                                            method='GET', 
                                            status_code=200 if detail else 404,
                                            duration=detail_duration,
                                            extra={'article_no': article_no}
                                        )
                                        
                                        if detail:
                                            useful_details = collector.extract_useful_details(detail)
                                            if useful_details:
                                                article['ìƒì„¸ì •ë³´'] = useful_details
                                    
                                    # DB í˜•ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ë³€í™˜
                                    db_property = self.convert_article_to_db_format(article, cortar_no, today)
                                    db_batch.append(db_property)
                                    
                                    # ë¡œê·¸ ê¸°ë¡
                                    property_data = {
                                        'article_no': db_property['article_no'],
                                        'article_name': db_property['article_name'],
                                        'real_estate_type': db_property['real_estate_type'],
                                        'trade_type': db_property['trade_type'],
                                        'price': db_property['price'],
                                        'rent_price': db_property['rent_price'],
                                        'area1': db_property['area1'],
                                        'floor_info': db_property['floor_info'],
                                        'address_detail': db_property['address_detail'],
                                        'cortar_no': cortar_no,
                                        'collected_date': today.isoformat()
                                    }
                                    
                                    ctx['log_property'](property_data)
                                    ctx['stats']['total_collected'] += 1
                                    ctx['stats']['last_property'] = db_property['article_name']
                                    total_collected += 1
                                    
                                    ctx['enhanced_logger'].trace("property_processing", 
                                                               f"ë§¤ë¬¼ ì²˜ë¦¬: {db_property['article_no']}",
                                                               {'property': property_data})
                                
                                except Exception as article_error:
                                    ctx['enhanced_logger'].error("property_processing", 
                                                               f"ë§¤ë¬¼ ì²˜ë¦¬ ì˜¤ë¥˜: {article.get('articleNo', 'unknown')}", 
                                                               article_error)
                                    continue  # ê°œë³„ ë§¤ë¬¼ ì˜¤ë¥˜ëŠ” ê³„ì† ì§„í–‰
                                
                                # 5. ë°°ì¹˜ ë‹¨ìœ„ë¡œ DB ì €ì¥
                                if len(db_batch) >= batch_size:
                                    print(f"      ğŸ’¾ ë°°ì¹˜ DB ì €ì¥: {len(db_batch)}ê°œ")
                                    db_save_start = time.time()
                                    
                                    try:
                                        save_stats = self.helper.safe_save_converted_properties(db_batch, cortar_no)
                                        db_save_duration = time.time() - db_save_start
                                        
                                        ctx['enhanced_logger'].info("db_batch_save", 
                                                                   f"ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {len(db_batch)}ê°œ ({db_save_duration:.2f}ì´ˆ)",
                                                                   {'batch_size': len(db_batch), 
                                                                    'save_stats': save_stats,
                                                                    'duration': db_save_duration})
                                        
                                        if not save_stats.get('total_saved', 0):
                                            print(f"      âš ï¸ ë°°ì¹˜ ì €ì¥ ë¶€ë¶„ ì‹¤íŒ¨")
                                            ctx['enhanced_logger'].warn("db_batch_save", "ë°°ì¹˜ ì €ì¥ ë¶€ë¶„ ì‹¤íŒ¨", 
                                                                       {'save_stats': save_stats})
                                        
                                        db_batch = []  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
                                        
                                    except Exception as batch_error:
                                        ctx['enhanced_logger'].error("db_batch_save", "ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨", 
                                                                   batch_error, {'batch_size': len(db_batch)})
                                        db_batch = []  # ì˜¤ë¥˜ì‹œì—ë„ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
                                
                                # ì§„í–‰ ìƒí™© ì¶œë ¥
                                if total_collected % 100 == 0:
                                    print(f"      ğŸ”„ {total_collected}ê°œ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì™„ë£Œ...")
                                    ctx['enhanced_logger'].info("progress_report", 
                                                               f"{dong_name} ì§„í–‰ìƒí™©: {total_collected}ê°œ ì²˜ë¦¬")
                            
                            print(f"    âœ… í˜ì´ì§€ {page}: {len(articles)}ê°œ (ëˆ„ì : {total_collected}ê°œ)")
                            
                            if not is_more_data:
                                print("    ğŸ“„ ë” ì´ìƒ ë°ì´í„° ì—†ìŒ")
                                ctx['enhanced_logger'].info("api_completion", f"{dong_name} ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ")
                                break
                                
                        elif response.status_code == 401:
                            print("    ğŸ”„ í† í° ë§Œë£Œ, ê°±ì‹  ì¤‘...")
                            ctx['enhanced_logger'].warn("token_management", f"{dong_name} í† í° ë§Œë£Œ - ê°±ì‹  ì‹œë„")
                            
                            if collector.get_fresh_token():
                                headers = collector.setup_headers()
                                ctx['enhanced_logger'].info("token_management", f"{dong_name} í† í° ê°±ì‹  ì„±ê³µ")
                                continue  # ê°™ì€ í˜ì´ì§€ ì¬ì‹œë„
                            else:
                                raise Exception("í† í° ê°±ì‹  ì‹¤íŒ¨")
                        else:
                            consecutive_failures += 1
                            error_msg = f"í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {response.status_code}"
                            print(f"    âŒ {error_msg}")
                            ctx['enhanced_logger'].error("api_request", error_msg, 
                                                        extra={'page': page, 'status_code': response.status_code})
                            
                            if consecutive_failures >= max_consecutive_failures:
                                raise Exception(f"ì—°ì† {consecutive_failures}íšŒ API ìš”ì²­ ì‹¤íŒ¨")
                            
                            # ì‹¤íŒ¨ì‹œ ë” ê¸´ ëŒ€ê¸°
                            time.sleep(random.uniform(5, 10))
                            
                    except Exception as e:
                        consecutive_failures += 1
                        error_msg = f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì˜¤ë¥˜: {e}"
                        print(f"    âŒ {error_msg}")
                        ctx['enhanced_logger'].error("page_processing", error_msg, e, 
                                                    {'page': page, 'consecutive_failures': consecutive_failures})
                        
                        if consecutive_failures >= max_consecutive_failures:
                            raise Exception(f"ì—°ì† {consecutive_failures}íšŒ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨")
                        
                        # ì˜¤ë¥˜ì‹œ ë” ê¸´ ëŒ€ê¸° í›„ ë‹¤ìŒ í˜ì´ì§€ë¡œ
                        time.sleep(random.uniform(5, 10))
                    
                    page += 1
                
                # 6. ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
                if db_batch:
                    print(f"  ğŸ’¾ ìµœì¢… ë°°ì¹˜ DB ì €ì¥: {len(db_batch)}ê°œ")
                    final_save_start = time.time()
                    
                    try:
                        save_stats = self.helper.safe_save_converted_properties(db_batch, cortar_no)
                        final_save_duration = time.time() - final_save_start
                        
                        ctx['enhanced_logger'].info("db_final_save", 
                                                   f"ìµœì¢… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {len(db_batch)}ê°œ ({final_save_duration:.2f}ì´ˆ)",
                                                   {'batch_size': len(db_batch), 
                                                    'save_stats': save_stats,
                                                    'duration': final_save_duration})
                        
                        if not save_stats.get('total_saved', 0):
                            print(f"  âš ï¸ ìµœì¢… ë°°ì¹˜ ì €ì¥ ë¶€ë¶„ ì‹¤íŒ¨")
                            ctx['enhanced_logger'].warn("db_final_save", "ìµœì¢… ë°°ì¹˜ ì €ì¥ ë¶€ë¶„ ì‹¤íŒ¨", 
                                                       {'save_stats': save_stats})
                    
                    except Exception as final_error:
                        ctx['enhanced_logger'].error("db_final_save", "ìµœì¢… ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨", final_error)
                
                # 7. ì¼ë³„ í†µê³„ ì €ì¥
                if total_collected > 0:
                    try:
                        # ì„ì‹œë¡œ ìˆ˜ì§‘ëœ ë°ì´í„° êµ¬ì¡° ìƒì„± (í†µê³„ ê³„ì‚°ìš©)
                        temp_properties = []
                        # í†µê³„ìš©ìœ¼ë¡œ ìµœê·¼ ë°°ì¹˜ì˜ ì¼ë¶€ë§Œ ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
                        sample_size = min(100, total_collected)
                        
                        self.helper.save_daily_stats(
                            today, 
                            cortar_no, 
                            temp_properties,
                            {'new_count': total_collected, 'removed_count': 0}
                        )
                        print(f"  ğŸ“Š ì¼ë³„ í†µê³„ ì €ì¥ ì™„ë£Œ")
                        ctx['enhanced_logger'].info("daily_stats", f"{dong_name} ì¼ë³„ í†µê³„ ì €ì¥ ì™„ë£Œ", 
                                                   {'total_collected': total_collected})
                    except Exception as e:
                        print(f"  âš ï¸ ì¼ë³„ í†µê³„ ì €ì¥ ì‹¤íŒ¨: {e}")
                        ctx['enhanced_logger'].error("daily_stats", f"{dong_name} ì¼ë³„ í†µê³„ ì €ì¥ ì‹¤íŒ¨", e)
                
                # 8. ìˆ˜ì§‘ ìš”ì•½ ë¡œê·¸
                summary = {
                    'dong_name': dong_name,
                    'cortar_no': cortar_no,
                    'total_properties': total_collected,
                    'collection_method': 'direct_db_streaming_enhanced',
                    'collection_time': f'ì‹¤ì‹œê°„ DB ì €ì¥ (í–¥ìƒëœ ë¡œê·¸)',
                    'memory_efficient': True,
                    'batch_size': batch_size,
                    'api_calls_tracked': True,
                    'performance_monitored': True
                }
                ctx['log_summary'](summary)
                
                print(f"âœ… {dong_name} ë°”ë¡œ DB ìˆ˜ì§‘ ì™„ë£Œ - {total_collected}ê°œ ë§¤ë¬¼")
                ctx['enhanced_logger'].info("collection_summary", 
                                          f"{dong_name} ìˆ˜ì§‘ ì™„ë£Œ - {total_collected}ê°œ ë§¤ë¬¼",
                                          summary)
                
                return {
                    'dong_name': dong_name,
                    'status': 'completed',
                    'total_collected': total_collected,
                    'summary': summary,
                    'method': 'direct_db_enhanced'
                }
                
            except Exception as e:
                print(f"âŒ {dong_name} ë°”ë¡œ DB ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                ctx['enhanced_logger'].error("collection_failed", f"{dong_name} ì „ì²´ ìˆ˜ì§‘ ì‹¤íŒ¨", e)
                return {
                    'dong_name': dong_name,
                    'status': 'failed',
                    'error': str(e),
                    'method': 'direct_db_enhanced'
                }
    
    def run_direct_db_collection(self, max_workers: int = 1):
        """ë°”ë¡œ DB ì €ì¥ ë³‘ë ¬ ìˆ˜ì§‘ ì‹¤í–‰ (í–¥ìƒëœ ë¡œê·¸ ì‹œìŠ¤í…œ)"""
        print("ğŸš€ ê°•ë‚¨êµ¬ ë°”ë¡œ DB ì €ì¥ ë³‘ë ¬ ìˆ˜ì§‘ ì‹œì‘ (í–¥ìƒëœ ë¡œê·¸ ì‹œìŠ¤í…œ)")
        print("=" * 80)
        print(f"ğŸ”„ ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜: {max_workers}ê°œ")
        print(f"ğŸ’¾ JSON íŒŒì¼ ìƒëµ - ë©”ëª¨ë¦¬ì—ì„œ ë°”ë¡œ DB ì €ì¥")
        print(f"ğŸ“Š ì‹¤ì‹œê°„ í–¥ìƒëœ ë¡œê·¸ ê¸°ë°˜ ëª¨ë‹ˆí„°ë§ í™œì„±í™”")
        print(f"âš¡ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬")
        print(f"ğŸ” API í˜¸ì¶œ ì¶”ì  ë° ì„±ëŠ¥ ë¶„ì„")
        print(f"ğŸ›¡ï¸ ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ë° ìë™ ë³µêµ¬ ì œì•ˆ")
        
        self.tracker.enhanced_logger.info("batch_collection", "ì „ì²´ ë°°ì¹˜ ìˆ˜ì§‘ ì‹œì‘", 
                                        {'max_workers': max_workers, 
                                         'total_dongs': len(self.gangnam_dongs)})
        
        # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_dongs = sorted(self.gangnam_dongs, key=lambda x: x['priority'], reverse=True)
        
        print(f"\nğŸ“‹ ìˆ˜ì§‘ ëŒ€ìƒ: {len(sorted_dongs)}ê°œ ë™")
        print("ğŸ† ìš°ì„ ìˆœìœ„ ìˆœì„œ:")
        for i, dong in enumerate(sorted_dongs, 1):
            print(f"   {i:2d}. {dong['name']:8s} (ì ìˆ˜: {dong['priority']:2d}) - {dong['cortar_no']}")
        
        print(f"\nğŸš€ ë°”ë¡œ DB ì €ì¥ ìˆ˜ì§‘ ì‹œì‘: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“Š ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©: http://localhost:8000")
        print("=" * 80)
        
        start_time = time.time()
        
        if max_workers == 1:
            # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ìˆœì°¨ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ì•ˆì •ì„±)
            results = []
            for dong_info in sorted_dongs:
                try:
                    result = self.collect_dong_direct_db(dong_info)
                    results.append(result)
                except KeyboardInterrupt:
                    print("\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
                    self.tracker.enhanced_logger.warn("batch_collection", "ì‚¬ìš©ìì— ì˜í•œ ì¤‘ë‹¨ ìš”ì²­")
                    break
                except Exception as e:
                    error_msg = f"{dong_info['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"
                    print(f"âŒ {error_msg}")
                    self.tracker.enhanced_logger.error("batch_collection", error_msg, e)
                    results.append({
                        'dong_name': dong_info['name'],
                        'status': 'failed',
                        'error': str(e),
                        'method': 'direct_db_enhanced'
                    })
        else:
            # ë©€í‹°í”„ë¡œì„¸ì‹±ì€ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ ì œí•œì  ì‚¬ìš©
            print("âš ï¸ ë°”ë¡œ DB ì €ì¥ì—ì„œëŠ” ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤ (ë©”ëª¨ë¦¬ ì•ˆì •ì„±)")
            self.tracker.enhanced_logger.warn("batch_collection", "ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš© - ë©”ëª¨ë¦¬ ì•ˆì •ì„± ì£¼ì˜", 
                                            {'max_workers': max_workers})
            
            with Pool(processes=min(max_workers, 2)) as pool:  # ìµœëŒ€ 2ê°œ í”„ë¡œì„¸ìŠ¤
                try:
                    results = pool.map(self.collect_dong_direct_db, sorted_dongs)
                except KeyboardInterrupt:
                    print("\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
                    self.tracker.enhanced_logger.warn("batch_collection", "ë©€í‹°í”„ë¡œì„¸ì‹± ì¤‘ ì‚¬ìš©ì ì¤‘ë‹¨")
                    pool.terminate()
                    pool.join()
                    return
        
        # ê²°ê³¼ ìš”ì•½
        end_time = time.time()
        total_time = end_time - start_time
        
        completed = [r for r in results if r.get('status') == 'completed']
        failed = [r for r in results if r.get('status') == 'failed']
        total_properties = sum(r.get('total_collected', 0) for r in completed)
        
        # ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥
        performance_summary = self.tracker.get_performance_summary()
        
        print("\n" + "=" * 80)
        print("ğŸ“Š ë°”ë¡œ DB ì €ì¥ ìˆ˜ì§‘ ì™„ë£Œ ìš”ì•½ (í–¥ìƒëœ ë¡œê·¸ ì‹œìŠ¤í…œ)")
        print("=" * 80)
        print(f"ğŸ• ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"âœ… ì„±ê³µí•œ ë™: {len(completed)}ê°œ")
        print(f"âŒ ì‹¤íŒ¨í•œ ë™: {len(failed)}ê°œ")
        print(f"ğŸ¢ ì´ DB ì €ì¥ ë§¤ë¬¼: {total_properties:,}ê°œ")
        print(f"âš¡ í‰ê·  ì €ì¥ ì†ë„: {total_properties/total_time:.1f}ê°œ/ì´ˆ")
        print(f"ğŸ’¾ JSON íŒŒì¼ ìƒì„±: 0ê°œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)")
        print(f"ğŸ—ƒï¸ DB ì§ì ‘ ì €ì¥: {total_properties:,}ê°œ")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ í‘œì‹œ
        current_metrics = performance_summary.get('current_metrics', {})
        print(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {current_metrics.get('memory_mb', 0):.1f}MB ({current_metrics.get('memory_percent', 0):.1f}%)")
        print(f"ğŸ” API í˜¸ì¶œ í†µê³„: {len(performance_summary.get('api_statistics', {}))}ê°œ ì—”ë“œí¬ì¸íŠ¸ ì¶”ì ")
        
        # ì˜¤ë¥˜ ë¶„ì„ í‘œì‹œ
        error_analysis = performance_summary.get('error_analysis', {})
        if error_analysis.get('total_errors', 0) > 0:
            print(f"âš ï¸ ì´ ì˜¤ë¥˜: {error_analysis['total_errors']}ê°œ ({error_analysis.get('unique_patterns', 0)}ê°œ íŒ¨í„´)")
            recommendations = error_analysis.get('recommendations', [])
            if recommendations:
                print("ğŸ’¡ ìë™ ë³µêµ¬ ì œì•ˆ:")
                for rec in recommendations[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    print(f"   - {rec}")
        
        if failed:
            print(f"\nâŒ ì‹¤íŒ¨í•œ ë™ë“¤:")
            for fail in failed:
                print(f"   - {fail['dong_name']}: {fail.get('error', 'Unknown error')}")
        
        print(f"\nğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§: http://localhost:8000")
        print(f"ğŸ“‹ ìƒì„¸ ë¡œê·¸ ìœ„ì¹˜: {self.tracker.log_dir}")
        print("=" * 80)
        
        # ìµœì¢… ë¡œê·¸ ê¸°ë¡
        self.tracker.enhanced_logger.info("batch_collection_complete", 
                                        "ì „ì²´ ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ",
                                        {
                                            'total_time': total_time,
                                            'completed_count': len(completed),
                                            'failed_count': len(failed),
                                            'total_properties': total_properties,
                                            'performance_summary': performance_summary
                                        })
    
    def close(self):
        """ìˆ˜ì§‘ê¸° ì¢…ë£Œ"""
        self.tracker.close()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë°”ë¡œ DB ì €ì¥ ë„¤ì´ë²„ ë¶€ë™ì‚° ìˆ˜ì§‘ê¸° (í–¥ìƒëœ ë¡œê·¸ ì‹œìŠ¤í…œ)')
    parser.add_argument('--max-workers', type=int, default=1,
                        help='ìµœëŒ€ ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’: 1, ê¶Œì¥ê°’: 1-2)')
    parser.add_argument('--test-single', type=str, default=None,
                        help='ë‹¨ì¼ ë™ í…ŒìŠ¤íŠ¸ (ì˜ˆ: ì—­ì‚¼ë™)')
    parser.add_argument('--log-level', type=str, default='INFO',
                        choices=['ERROR', 'WARN', 'INFO', 'DEBUG', 'TRACE'],
                        help='ë¡œê·¸ ë ˆë²¨ ì„¤ì • (ê¸°ë³¸ê°’: INFO)')
    
    args = parser.parse_args()
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    log_level_map = {
        'ERROR': LogLevel.ERROR,
        'WARN': LogLevel.WARN,
        'INFO': LogLevel.INFO,
        'DEBUG': LogLevel.DEBUG,
        'TRACE': LogLevel.TRACE
    }
    log_level = log_level_map.get(args.log_level, LogLevel.INFO)
    
    try:
        collector = DirectDBCollector(log_level=log_level)
        
        # ë‹¨ì¼ ë™ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        if args.test_single:
            # í•´ë‹¹ ë™ ì •ë³´ ì°¾ê¸°
            target_dong = None
            for dong in collector.gangnam_dongs:
                if dong["name"] == args.test_single:
                    target_dong = dong
                    break
            
            if target_dong:
                print(f"ğŸ§ª ë‹¨ì¼ ë™ ë°”ë¡œ DB í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {args.test_single}")
                collector.tracker.enhanced_logger.info("test_mode", f"ë‹¨ì¼ ë™ í…ŒìŠ¤íŠ¸: {args.test_single}")
                
                result = collector.collect_dong_direct_db(target_dong)
                print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result}")
                
                # ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥
                performance = collector.tracker.get_performance_summary()
                print(f"\nğŸ“ˆ ì„±ëŠ¥ ìš”ì•½:")
                print(f"  ë©”ëª¨ë¦¬: {performance['current_metrics'].get('memory_mb', 0):.1f}MB")
                print(f"  API í˜¸ì¶œ: {len(performance.get('api_statistics', {}))}ê°œ ì—”ë“œí¬ì¸íŠ¸")
                if performance.get('error_analysis', {}).get('total_errors', 0) > 0:
                    print(f"  ì˜¤ë¥˜: {performance['error_analysis']['total_errors']}ê°œ")
                
            else:
                print(f"âŒ '{args.test_single}' ë™ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ì‚¬ìš© ê°€ëŠ¥í•œ ë™:", [dong["name"] for dong in collector.gangnam_dongs])
        else:
            collector.run_direct_db_collection(max_workers=args.max_workers)
        
        collector.close()
            
    except KeyboardInterrupt:
        print("\nâš ï¸ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()