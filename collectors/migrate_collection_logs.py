#!/usr/bin/env python3
"""
collection_logs í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€ ë° ë°ì´í„° ì •ë¦¬
"""

from supabase_client import SupabaseHelper
from datetime import datetime, timedelta


def migrate_collection_logs():
    """collection_logs í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¥"""
    helper = SupabaseHelper()
    
    print("ğŸ”„ collection_logs í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
    
    # ë§ˆì´ê·¸ë ˆì´ì…˜ SQL ëª…ë ¹ì–´ë“¤
    migrations = [
        # ìƒˆ ì»¬ëŸ¼ ì¶”ê°€
        """
        ALTER TABLE collection_logs 
        ADD COLUMN IF NOT EXISTS process_id INTEGER,
        ADD COLUMN IF NOT EXISTS process_name TEXT,
        ADD COLUMN IF NOT EXISTS duration_seconds DECIMAL(10,2),
        ADD COLUMN IF NOT EXISTS error_details JSONB,
        ADD COLUMN IF NOT EXISTS collection_stats JSONB
        """,
        
        # ì¸ë±ìŠ¤ ì¶”ê°€ (ì„±ëŠ¥ í–¥ìƒ)
        """
        CREATE INDEX IF NOT EXISTS idx_collection_logs_status_created 
        ON collection_logs(status, created_at DESC)
        """,
        
        """
        CREATE INDEX IF NOT EXISTS idx_collection_logs_cortar_status 
        ON collection_logs(cortar_no, status, created_at DESC)
        """,
        
        """
        CREATE INDEX IF NOT EXISTS idx_collection_logs_process 
        ON collection_logs(process_name, created_at DESC)
        """
    ]
    
    try:
        for i, migration in enumerate(migrations, 1):
            print(f"ğŸ“ ë§ˆì´ê·¸ë ˆì´ì…˜ {i}/{len(migrations)} ì‹¤í–‰ ì¤‘...")
            helper.client.rpc('exec_sql', {'sql': migration}).execute()
            print(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ {i} ì™„ë£Œ")
        
        print("ğŸ‰ ëª¨ë“  ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
        return False


def cleanup_old_logs():
    """ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬"""
    helper = SupabaseHelper()
    
    print("ğŸ§¹ ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬ ì‹œì‘...")
    
    try:
        # 30ì¼ ì´ì „ ë¡œê·¸ ì‚­ì œ
        cutoff_date = datetime.now() - timedelta(days=30)
        
        result = helper.client.table('collection_logs')\
            .delete()\
            .lt('created_at', cutoff_date.isoformat())\
            .execute()
        
        deleted_count = len(result.data) if result.data else 0
        print(f"ğŸ—‘ï¸ {deleted_count}ê°œ ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ ì™„ë£Œ")
        
        # ê³ ì•„ í”„ë¡œì„¸ìŠ¤ ë¡œê·¸ ì •ë¦¬ (2ì‹œê°„ ì´ìƒ started ìƒíƒœ)
        orphan_cutoff = datetime.now() - timedelta(hours=2)
        
        orphaned_logs = helper.client.table('collection_logs')\
            .select('id, dong_name, started_at')\
            .eq('status', 'started')\
            .lt('started_at', orphan_cutoff.isoformat())\
            .execute()
        
        if orphaned_logs.data:
            for log in orphaned_logs.data:
                helper.client.table('collection_logs')\
                    .update({
                        'status': 'timeout',
                        'completed_at': datetime.now().isoformat(),
                        'error_message': 'í”„ë¡œì„¸ìŠ¤ íƒ€ì„ì•„ì›ƒ (2ì‹œê°„ ì´ˆê³¼)',
                        'error_details': {
                            'cleanup_reason': 'orphaned_process_migration',
                            'original_started_at': log['started_at']
                        }
                    })\
                    .eq('id', log['id'])\
                    .execute()
            
            print(f"â° {len(orphaned_logs.data)}ê°œ ê³ ì•„ í”„ë¡œì„¸ìŠ¤ ë¡œê·¸ ì •ë¦¬ ì™„ë£Œ")
        
        print("âœ… ë¡œê·¸ ì •ë¦¬ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ ë¡œê·¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return False


def validate_migration():
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦"""
    helper = SupabaseHelper()
    
    print("ğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì¤‘...")
    
    try:
        # í…Œì´ë¸” êµ¬ì¡° í™•ì¸
        result = helper.client.table('collection_logs').select('*').limit(1).execute()
        
        if result.data:
            columns = list(result.data[0].keys())
            print("ğŸ“‹ í˜„ì¬ í…Œì´ë¸” ì»¬ëŸ¼:")
            for col in sorted(columns):
                print(f"  - {col}")
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = [
                'process_id', 'process_name', 'duration_seconds', 
                'error_details', 'collection_stats'
            ]
            
            missing_columns = [col for col in required_columns if col not in columns]
            
            if missing_columns:
                print(f"âŒ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_columns}")
                return False
            else:
                print("âœ… ëª¨ë“  í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬")
        
        # ë¡œê·¸ ìƒíƒœ ë¶„í¬ í™•ì¸
        print("\nğŸ“Š í˜„ì¬ ë¡œê·¸ ìƒíƒœ ë¶„í¬:")
        all_logs = helper.client.table('collection_logs').select('status').execute()
        
        status_count = {}
        for log in all_logs.data:
            status = log['status']
            status_count[status] = status_count.get(status, 0) + 1
        
        for status, count in sorted(status_count.items()):
            print(f"  {status}: {count}ê°œ")
        
        print("âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ collection_logs í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
    print("=" * 60)
    
    # 1. ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜
    if not migrate_collection_logs():
        print("âŒ ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨")
        return False
    
    # 2. ê¸°ì¡´ ë°ì´í„° ì •ë¦¬
    if not cleanup_old_logs():
        print("âŒ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨")
        return False
    
    # 3. ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦
    if not validate_migration():
        print("âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì‹¤íŒ¨")
        return False
    
    print("\nğŸ‰ collection_logs ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µ!")
    print("ì´ì œ enhanced_logger.pyë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    return True


if __name__ == "__main__":
    main()