#!/usr/bin/env python3
"""
ì¢…í•© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
- ì‹¤ì‹œê°„ ìŠ¤í‚¤ë§ˆ ìƒíƒœ ëª¨ë‹ˆí„°ë§
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì 
- ë°ì´í„° í’ˆì§ˆ ì§€í‘œ
- ìë™ ì•Œë¦¼ ë° ê¶Œì¥ì‚¬í•­
"""

import os
import sys
import json
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import asyncio
from dataclasses import dataclass, asdict
import traceback

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from supabase import create_client

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('monitoring_dashboard.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class HealthMetric:
    """ê±´ê°• ìƒíƒœ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    name: str
    value: Any
    status: str  # 'EXCELLENT', 'GOOD', 'FAIR', 'POOR', 'CRITICAL'
    unit: str = ''
    threshold: Optional[str] = None
    recommendation: Optional[str] = None
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

@dataclass
class DashboardSnapshot:
    """ëŒ€ì‹œë³´ë“œ ìŠ¤ëƒ…ìƒ· ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: datetime
    overall_status: str
    schema_health: List[HealthMetric]
    performance_metrics: List[HealthMetric]
    data_quality: List[HealthMetric]
    alerts: List[Dict[str, Any]]
    recommendations: List[str]

class ComprehensiveMonitoringDashboard:
    def __init__(self, refresh_interval: int = 30):
        """ì¢…í•© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ì´ˆê¸°í™”"""
        # Supabase ì—°ê²°
        self.supabase_url = 'https://eslhavjipwbyvbbknixv.supabase.co'
        self.supabase_key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImVzbGhhdmppcHdieXZiYmtuaXh2Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1NDI5OTUxMSwiZXhwIjoyMDY5ODc1NTExfQ.p6JB5xrdLi_yBJTuHg2mF9TZFQiwA4Tqd0hc-7FxFqE'
        
        try:
            self.client = create_client(self.supabase_url, self.supabase_key)
            logger.info("âœ… Supabase ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
            sys.exit(1)
        
        self.refresh_interval = refresh_interval
        self.dashboard_data = None
        self.alert_history = []
        self.is_monitoring = False
        
        # ìƒíƒœ ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            'cache_hit_ratio': {'good': 95, 'fair': 90, 'poor': 80},
            'index_usage_ratio': {'good': 80, 'fair': 60, 'poor': 40},
            'data_completeness': {'good': 90, 'fair': 70, 'poor': 50},
            'query_performance': {'good': 100, 'fair': 500, 'poor': 1000},  # ms
            'active_connections': {'good': 20, 'fair': 50, 'poor': 100}
        }
    
    def execute_sql_function(self, function_name: str, params: Dict = None) -> List[Dict]:
        """SQL í•¨ìˆ˜ ì‹¤í–‰"""
        try:
            if params:
                result = self.client.rpc(function_name, params).execute()
            else:
                # í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ
                query = f"SELECT * FROM {function_name}();"
                result = self.client.rpc('exec_sql', {'query': query}).execute()
            
            return result.data if result.data else []
            
        except Exception as e:
            logger.error(f"SQL í•¨ìˆ˜ {function_name} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return []
    
    def collect_schema_health_metrics(self) -> List[HealthMetric]:
        """ìŠ¤í‚¤ë§ˆ ê±´ê°• ìƒíƒœ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        logger.info("ğŸ“Š ìŠ¤í‚¤ë§ˆ ê±´ê°• ìƒíƒœ ìˆ˜ì§‘ ì¤‘...")
        
        metrics = []
        
        try:
            # 1. í…Œì´ë¸” ì¡´ì¬ ìƒíƒœ
            required_tables = [
                'properties_new', 'property_prices', 'property_locations',
                'property_physical', 'real_estate_types', 'trade_types', 
                'regions', 'realtors', 'property_tax_info'
            ]
            
            existing_tables = []
            for table in required_tables:
                try:
                    result = self.client.table(table).select('*').limit(1).execute()
                    existing_tables.append(table)
                except:
                    pass
            
            table_completeness = (len(existing_tables) / len(required_tables)) * 100
            table_status = self._determine_status(table_completeness, self.thresholds['data_completeness'])
            
            metrics.append(HealthMetric(
                name="Schema Tables",
                value=f"{len(existing_tables)}/{len(required_tables)}",
                status=table_status,
                unit="tables",
                threshold="9/9 required",
                recommendation="Complete schema deployment" if table_status != 'EXCELLENT' else None
            ))
            
            # 2. ì¸ë±ìŠ¤ ìƒíƒœ
            try:
                indexes_result = self.client.rpc('get_index_count').execute()
                index_count = indexes_result.data[0]['count'] if indexes_result.data else 0
                
                expected_indexes = 25  # ì˜ˆìƒ ì¸ë±ìŠ¤ ìˆ˜
                index_ratio = (index_count / expected_indexes) * 100
                index_status = self._determine_status(index_ratio, self.thresholds['data_completeness'])
                
                metrics.append(HealthMetric(
                    name="Performance Indexes",
                    value=index_count,
                    status=index_status,
                    unit="indexes",
                    threshold=f">={expected_indexes} expected",
                    recommendation="Create missing performance indexes" if index_status != 'EXCELLENT' else None
                ))
            except Exception as e:
                logger.warning(f"ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            
            # 3. ì™¸ë˜í‚¤ ë¬´ê²°ì„±
            try:
                # ê°„ë‹¨í•œ ì™¸ë˜í‚¤ í…ŒìŠ¤íŠ¸
                fk_test_result = self.client.table('properties_new')\
                    .select('id, real_estate_type_id, trade_type_id, region_id')\
                    .not_.is_('real_estate_type_id', 'null')\
                    .limit(10).execute()
                
                if fk_test_result.data:
                    metrics.append(HealthMetric(
                        name="Foreign Key Integrity",
                        value="Validated",
                        status="EXCELLENT",
                        unit="",
                        recommendation=None
                    ))
                else:
                    metrics.append(HealthMetric(
                        name="Foreign Key Integrity",
                        value="Issues Detected",
                        status="POOR",
                        unit="",
                        recommendation="Check reference data population"
                    ))
            except Exception as e:
                logger.warning(f"ì™¸ë˜í‚¤ ë¬´ê²°ì„± í™•ì¸ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ê±´ê°• ìƒíƒœ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            metrics.append(HealthMetric(
                name="Schema Health Check",
                value="Error",
                status="CRITICAL",
                recommendation="Check database connectivity"
            ))
        
        return metrics
    
    def collect_performance_metrics(self) -> List[HealthMetric]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        logger.info("âš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘...")
        
        metrics = []
        
        try:
            # 1. ìºì‹œ íˆíŠ¸ ë¹„ìœ¨
            try:
                cache_query = """
                SELECT ROUND(
                    (SUM(heap_blks_hit)::NUMERIC / GREATEST(SUM(heap_blks_hit + heap_blks_read), 1)) * 100, 2
                ) as hit_ratio
                FROM pg_statio_user_tables 
                WHERE schemaname = 'public'
                """
                cache_result = self.client.rpc('exec_sql', {'query': cache_query}).execute()
                
                if cache_result.data:
                    hit_ratio = float(cache_result.data[0]['hit_ratio'])
                    cache_status = self._determine_status(hit_ratio, self.thresholds['cache_hit_ratio'])
                    
                    metrics.append(HealthMetric(
                        name="Buffer Cache Hit Ratio",
                        value=hit_ratio,
                        status=cache_status,
                        unit="%",
                        threshold=">95% excellent",
                        recommendation=self._get_cache_recommendation(hit_ratio)
                    ))
            except Exception as e:
                logger.warning(f"ìºì‹œ íˆíŠ¸ ë¹„ìœ¨ í™•ì¸ ì‹¤íŒ¨: {e}")
            
            # 2. í™œì„± ì—°ê²° ìˆ˜
            try:
                connection_query = """
                SELECT COUNT(*) as active_connections
                FROM pg_stat_activity 
                WHERE state = 'active' AND pid != pg_backend_pid()
                """
                conn_result = self.client.rpc('exec_sql', {'query': connection_query}).execute()
                
                if conn_result.data:
                    active_conns = int(conn_result.data[0]['active_connections'])
                    conn_status = self._determine_status(active_conns, self.thresholds['active_connections'], reverse=True)
                    
                    metrics.append(HealthMetric(
                        name="Active Connections",
                        value=active_conns,
                        status=conn_status,
                        unit="connections",
                        threshold="<50 optimal",
                        recommendation="Monitor connection pooling" if active_conns > 50 else None
                    ))
            except Exception as e:
                logger.warning(f"í™œì„± ì—°ê²° ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")
            
            # 3. ì¿¼ë¦¬ ì„±ëŠ¥ (ìƒ˜í”Œ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„)
            try:
                start_time = time.time()
                perf_test = self.client.table('properties_new')\
                    .select('*', count='exact')\
                    .eq('is_active', True)\
                    .limit(1).execute()
                execution_time = (time.time() - start_time) * 1000  # ms
                
                perf_status = self._determine_status(execution_time, self.thresholds['query_performance'], reverse=True)
                
                metrics.append(HealthMetric(
                    name="Query Performance",
                    value=round(execution_time, 2),
                    status=perf_status,
                    unit="ms",
                    threshold="<100ms excellent",
                    recommendation="Optimize indexes" if execution_time > 500 else None
                ))
            except Exception as e:
                logger.warning(f"ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨: {e}")
            
            # 4. ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°
            try:
                size_query = """
                SELECT ROUND(pg_database_size(current_database())::NUMERIC / 1024 / 1024, 2) as db_size_mb
                """
                size_result = self.client.rpc('exec_sql', {'query': size_query}).execute()
                
                if size_result.data:
                    db_size = float(size_result.data[0]['db_size_mb'])
                    size_status = 'GOOD' if db_size < 1000 else ('FAIR' if db_size < 5000 else 'POOR')
                    
                    metrics.append(HealthMetric(
                        name="Database Size",
                        value=db_size,
                        status=size_status,
                        unit="MB",
                        threshold="<1GB optimal",
                        recommendation="Consider data archiving" if db_size > 5000 else None
                    ))
            except Exception as e:
                logger.warning(f"ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° í™•ì¸ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        return metrics
    
    def collect_data_quality_metrics(self) -> List[HealthMetric]:
        """ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        logger.info("ğŸ“Š ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘...")
        
        metrics = []
        
        try:
            # 1. ì „ì²´ ë§¤ë¬¼ ìˆ˜ ë° í™œì„± ë¹„ìœ¨
            try:
                total_result = self.client.table('properties_new').select('*', count='exact').limit(1).execute()
                active_result = self.client.table('properties_new')\
                    .select('*', count='exact')\
                    .eq('is_active', True)\
                    .limit(1).execute()
                
                total_count = total_result.count if hasattr(total_result, 'count') else 0
                active_count = active_result.count if hasattr(active_result, 'count') else 0
                
                active_ratio = (active_count / total_count * 100) if total_count > 0 else 0
                active_status = self._determine_status(active_ratio, {'good': 80, 'fair': 60, 'poor': 40})
                
                metrics.append(HealthMetric(
                    name="Active Properties Ratio",
                    value=f"{active_count:,}/{total_count:,}",
                    status=active_status,
                    unit=f"({active_ratio:.1f}%)",
                    threshold=">80% active",
                    recommendation="Review inactive property cleanup" if active_ratio < 80 else None
                ))
            except Exception as e:
                logger.warning(f"ë§¤ë¬¼ í™œì„± ë¹„ìœ¨ í™•ì¸ ì‹¤íŒ¨: {e}")
            
            # 2. í•µì‹¬ í…Œì´ë¸” ë°ì´í„° ì™„ì„±ë„
            key_tables = [
                ('property_locations', 'latitude'),
                ('property_physical', 'area_exclusive'),
                ('property_prices', 'amount'),
                ('property_tax_info', 'total_tax')
            ]
            
            for table_name, key_column in key_tables:
                try:
                    total_result = self.client.table(table_name).select('*', count='exact').limit(1).execute()
                    complete_result = self.client.table(table_name)\
                        .select('*', count='exact')\
                        .not_.is_(key_column, 'null')\
                        .limit(1).execute()
                    
                    total = total_result.count if hasattr(total_result, 'count') else 0
                    complete = complete_result.count if hasattr(complete_result, 'count') else 0
                    
                    completeness = (complete / total * 100) if total > 0 else 0
                    completeness_status = self._determine_status(completeness, self.thresholds['data_completeness'])
                    
                    metrics.append(HealthMetric(
                        name=f"{table_name.replace('_', ' ').title()} Completeness",
                        value=f"{complete:,}/{total:,}",
                        status=completeness_status,
                        unit=f"({completeness:.1f}%)",
                        threshold=">90% complete",
                        recommendation=f"Improve {table_name} data parsing" if completeness < 90 else None
                    ))
                except Exception as e:
                    logger.warning(f"{table_name} ì™„ì„±ë„ í™•ì¸ ì‹¤íŒ¨: {e}")
            
            # 3. ë°ì´í„° ì‹ ì„ ë„ (ìµœê·¼ ìˆ˜ì§‘ ë°ì´í„°)
            try:
                fresh_query = """
                SELECT 
                    EXTRACT(epoch FROM (now() - MAX(collected_date)))/3600 as hours_since_last
                FROM properties_new 
                WHERE is_active = true
                """
                fresh_result = self.client.rpc('exec_sql', {'query': fresh_query}).execute()
                
                if fresh_result.data and fresh_result.data[0]['hours_since_last'] is not None:
                    hours_since = float(fresh_result.data[0]['hours_since_last'])
                    
                    if hours_since < 6:
                        freshness_status = 'EXCELLENT'
                    elif hours_since < 24:
                        freshness_status = 'GOOD'
                    elif hours_since < 72:
                        freshness_status = 'FAIR'
                    else:
                        freshness_status = 'POOR'
                    
                    metrics.append(HealthMetric(
                        name="Data Freshness",
                        value=f"{hours_since:.1f}",
                        status=freshness_status,
                        unit="hours ago",
                        threshold="<24h optimal",
                        recommendation="Check data collection system" if hours_since > 48 else None
                    ))
            except Exception as e:
                logger.warning(f"ë°ì´í„° ì‹ ì„ ë„ í™•ì¸ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        return metrics
    
    def collect_alerts(self) -> List[Dict[str, Any]]:
        """ì‹œìŠ¤í…œ ì•Œë¦¼ ìˆ˜ì§‘"""
        alerts = []
        
        try:
            # SQL í•¨ìˆ˜ë¥¼ í†µí•œ ì„±ëŠ¥ ì•Œë¦¼ í™•ì¸
            perf_alerts = self.execute_sql_function('check_performance_alerts')
            
            for alert in perf_alerts:
                alerts.append({
                    'level': alert.get('alert_level', 'INFO'),
                    'type': alert.get('alert_type', 'Unknown'),
                    'message': alert.get('description', ''),
                    'recommendation': alert.get('immediate_action', ''),
                    'timestamp': datetime.now().isoformat()
                })
        
        except Exception as e:
            logger.warning(f"ì„±ëŠ¥ ì•Œë¦¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        # ë©”íŠ¸ë¦­ ê¸°ë°˜ ì•Œë¦¼ ìƒì„±
        try:
            # ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼
            all_metrics = (
                self.collect_schema_health_metrics() +
                self.collect_performance_metrics() +
                self.collect_data_quality_metrics()
            )
            
            for metric in all_metrics:
                if metric.status in ['POOR', 'CRITICAL']:
                    alerts.append({
                        'level': 'WARNING' if metric.status == 'POOR' else 'CRITICAL',
                        'type': 'Metric Alert',
                        'message': f"{metric.name}: {metric.value} {metric.unit}",
                        'recommendation': metric.recommendation or f"Review {metric.name} configuration",
                        'timestamp': datetime.now().isoformat()
                    })
        
        except Exception as e:
            logger.warning(f"ë©”íŠ¸ë¦­ ê¸°ë°˜ ì•Œë¦¼ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return alerts
    
    def generate_recommendations(self, metrics: List[HealthMetric], alerts: List[Dict]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ë©”íŠ¸ë¦­ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        poor_metrics = [m for m in metrics if m.status in ['POOR', 'CRITICAL']]
        fair_metrics = [m for m in metrics if m.status == 'FAIR']
        
        if poor_metrics:
            recommendations.append(f"ğŸ”´ {len(poor_metrics)} critical metrics need immediate attention")
            for metric in poor_metrics[:3]:  # ìƒìœ„ 3ê°œë§Œ
                if metric.recommendation:
                    recommendations.append(f"  - {metric.name}: {metric.recommendation}")
        
        if fair_metrics:
            recommendations.append(f"ğŸŸ¡ {len(fair_metrics)} metrics could be improved")
        
        # ì•Œë¦¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        critical_alerts = [a for a in alerts if a.get('level') == 'CRITICAL']
        if critical_alerts:
            recommendations.append(f"ğŸš¨ {len(critical_alerts)} critical alerts require immediate action")
        
        # ì „ë°˜ì ì¸ ì‹œìŠ¤í…œ ìƒíƒœ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        excellent_count = len([m for m in metrics if m.status == 'EXCELLENT'])
        total_metrics = len(metrics)
        
        if excellent_count / total_metrics > 0.8:
            recommendations.append("âœ… System is performing well - continue monitoring")
        elif excellent_count / total_metrics > 0.6:
            recommendations.append("âš¡ System needs minor optimizations")
        else:
            recommendations.append("âš ï¸ System requires significant improvements")
        
        # êµ¬ì²´ì ì¸ ì•¡ì…˜ ì•„ì´í…œ
        if any('cache' in m.name.lower() for m in poor_metrics):
            recommendations.append("ğŸ“ˆ Consider increasing shared_buffers parameter")
        
        if any('index' in m.name.lower() for m in poor_metrics):
            recommendations.append("ğŸ” Review and optimize database indexes")
        
        if any('completeness' in m.name.lower() for m in poor_metrics):
            recommendations.append("ğŸ”§ Improve data parsing and collection processes")
        
        return recommendations
    
    def create_dashboard_snapshot(self) -> DashboardSnapshot:
        """ëŒ€ì‹œë³´ë“œ ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        logger.info("ğŸ“¸ ëŒ€ì‹œë³´ë“œ ìŠ¤ëƒ…ìƒ· ìƒì„± ì¤‘...")
        
        try:
            # ëª¨ë“  ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            schema_metrics = self.collect_schema_health_metrics()
            performance_metrics = self.collect_performance_metrics()
            quality_metrics = self.collect_data_quality_metrics()
            
            all_metrics = schema_metrics + performance_metrics + quality_metrics
            alerts = self.collect_alerts()
            
            # ì „ì²´ ìƒíƒœ ê²°ì •
            critical_count = len([m for m in all_metrics if m.status == 'CRITICAL'])
            poor_count = len([m for m in all_metrics if m.status == 'POOR'])
            fair_count = len([m for m in all_metrics if m.status == 'FAIR'])
            
            if critical_count > 0:
                overall_status = 'CRITICAL'
            elif poor_count > 0:
                overall_status = 'POOR' 
            elif fair_count > len(all_metrics) * 0.3:  # 30% ì´ìƒì´ FAIR
                overall_status = 'FAIR'
            else:
                overall_status = 'GOOD'
            
            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            recommendations = self.generate_recommendations(all_metrics, alerts)
            
            snapshot = DashboardSnapshot(
                timestamp=datetime.now(),
                overall_status=overall_status,
                schema_health=schema_metrics,
                performance_metrics=performance_metrics,
                data_quality=quality_metrics,
                alerts=alerts,
                recommendations=recommendations
            )
            
            self.dashboard_data = snapshot
            return snapshot
            
        except Exception as e:
            logger.error(f"ëŒ€ì‹œë³´ë“œ ìŠ¤ëƒ…ìƒ· ìƒì„± ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ìƒíƒœ ë°˜í™˜
            return DashboardSnapshot(
                timestamp=datetime.now(),
                overall_status='ERROR',
                schema_health=[],
                performance_metrics=[],
                data_quality=[],
                alerts=[{
                    'level': 'CRITICAL',
                    'type': 'System Error',
                    'message': f'Dashboard snapshot creation failed: {str(e)}',
                    'recommendation': 'Check system logs and database connectivity',
                    'timestamp': datetime.now().isoformat()
                }],
                recommendations=['Check system configuration and database connectivity']
            )
    
    def _determine_status(self, value: float, thresholds: Dict[str, float], reverse: bool = False) -> str:
        """ì„ê³„ê°’ ê¸°ë°˜ ìƒíƒœ ê²°ì •"""
        if reverse:  # ë‚®ì€ ê°’ì´ ì¢‹ì€ ê²½ìš° (ì˜ˆ: ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„)
            if value <= thresholds['good']:
                return 'EXCELLENT'
            elif value <= thresholds['fair']:
                return 'GOOD'
            elif value <= thresholds['poor']:
                return 'FAIR'
            else:
                return 'POOR'
        else:  # ë†’ì€ ê°’ì´ ì¢‹ì€ ê²½ìš° (ì˜ˆ: ìºì‹œ íˆíŠ¸ìœ¨)
            if value >= thresholds['good']:
                return 'EXCELLENT'
            elif value >= thresholds['fair']:
                return 'GOOD'
            elif value >= thresholds['poor']:
                return 'FAIR'
            else:
                return 'POOR'
    
    def _get_cache_recommendation(self, hit_ratio: float) -> Optional[str]:
        """ìºì‹œ íˆíŠ¸ìœ¨ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­"""
        if hit_ratio < 80:
            return "Increase shared_buffers immediately - very low cache performance"
        elif hit_ratio < 90:
            return "Consider increasing shared_buffers parameter"
        elif hit_ratio < 95:
            return "Monitor cache performance and consider minor tuning"
        else:
            return None
    
    def display_dashboard(self, snapshot: DashboardSnapshot = None):
        """ëŒ€ì‹œë³´ë“œ ì½˜ì†” ì¶œë ¥"""
        if not snapshot:
            snapshot = self.dashboard_data or self.create_dashboard_snapshot()
        
        # í™”ë©´ í´ë¦¬ì–´ (ì„ íƒì )
        # os.system('clear' if os.name == 'posix' else 'cls')
        
        print("\n" + "="*100)
        print("ğŸ“Š NAVER REAL ESTATE DATABASE MONITORING DASHBOARD")
        print("="*100)
        
        # ì „ì²´ ìƒíƒœ í‘œì‹œ
        status_emoji = {
            'EXCELLENT': 'ğŸŸ¢', 'GOOD': 'ğŸŸ¢', 'FAIR': 'ğŸŸ¡', 
            'POOR': 'ğŸŸ ', 'CRITICAL': 'ğŸ”´', 'ERROR': 'ğŸ’¥'
        }
        
        print(f"\nğŸ¯ Overall Status: {status_emoji.get(snapshot.overall_status, 'â“')} {snapshot.overall_status}")
        print(f"ğŸ“… Last Updated: {snapshot.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ìŠ¤í‚¤ë§ˆ ê±´ê°• ìƒíƒœ
        if snapshot.schema_health:
            print(f"\nğŸ—ï¸ SCHEMA HEALTH ({len(snapshot.schema_health)} metrics)")
            print("-" * 80)
            for metric in snapshot.schema_health:
                emoji = status_emoji.get(metric.status, 'â“')
                print(f"  {emoji} {metric.name}: {metric.value} {metric.unit}")
                if metric.recommendation:
                    print(f"    ğŸ’¡ {metric.recommendation}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        if snapshot.performance_metrics:
            print(f"\nâš¡ PERFORMANCE METRICS ({len(snapshot.performance_metrics)} metrics)")
            print("-" * 80)
            for metric in snapshot.performance_metrics:
                emoji = status_emoji.get(metric.status, 'â“')
                print(f"  {emoji} {metric.name}: {metric.value} {metric.unit}")
                if metric.threshold:
                    print(f"    ğŸ“ Threshold: {metric.threshold}")
                if metric.recommendation:
                    print(f"    ğŸ’¡ {metric.recommendation}")
        
        # ë°ì´í„° í’ˆì§ˆ
        if snapshot.data_quality:
            print(f"\nğŸ“Š DATA QUALITY ({len(snapshot.data_quality)} metrics)")
            print("-" * 80)
            for metric in snapshot.data_quality:
                emoji = status_emoji.get(metric.status, 'â“')
                print(f"  {emoji} {metric.name}: {metric.value} {metric.unit}")
                if metric.recommendation:
                    print(f"    ğŸ’¡ {metric.recommendation}")
        
        # í™œì„± ì•Œë¦¼
        if snapshot.alerts:
            print(f"\nğŸš¨ ACTIVE ALERTS ({len(snapshot.alerts)} alerts)")
            print("-" * 80)
            for alert in snapshot.alerts:
                level_emoji = {'CRITICAL': 'ğŸ”´', 'WARNING': 'âš ï¸', 'INFO': 'â„¹ï¸'}.get(alert.get('level'), 'â“')
                print(f"  {level_emoji} [{alert.get('level')}] {alert.get('type')}")
                print(f"    {alert.get('message')}")
                if alert.get('recommendation'):
                    print(f"    ğŸ’¡ {alert.get('recommendation')}")
                print()
        
        # ê¶Œì¥ì‚¬í•­
        if snapshot.recommendations:
            print(f"\nğŸ’¡ RECOMMENDATIONS ({len(snapshot.recommendations)} items)")
            print("-" * 80)
            for i, rec in enumerate(snapshot.recommendations, 1):
                print(f"  {i}. {rec}")
        
        print("\n" + "="*100)
        print(f"Next refresh in {self.refresh_interval} seconds... (Ctrl+C to stop)")
    
    def save_snapshot_to_file(self, snapshot: DashboardSnapshot, filename: str = None) -> str:
        """ìŠ¤ëƒ…ìƒ·ì„ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = snapshot.timestamp.strftime('%Y%m%d_%H%M%S')
            filename = f"monitoring_snapshot_{timestamp}.json"
        
        try:
            filepath = current_dir / filename
            
            # dataclassë¥¼ dictë¡œ ë³€í™˜
            snapshot_dict = {
                'timestamp': snapshot.timestamp.isoformat(),
                'overall_status': snapshot.overall_status,
                'schema_health': [asdict(m) for m in snapshot.schema_health],
                'performance_metrics': [asdict(m) for m in snapshot.performance_metrics],
                'data_quality': [asdict(m) for m in snapshot.data_quality],
                'alerts': snapshot.alerts,
                'recommendations': snapshot.recommendations
            }
            
            # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            def convert_datetime(obj):
                if isinstance(obj, datetime):
                    return obj.isoformat()
                return obj
            
            # ëª¨ë“  timestamp í•„ë“œ ë³€í™˜
            for category in ['schema_health', 'performance_metrics', 'data_quality']:
                for item in snapshot_dict[category]:
                    if 'timestamp' in item and item['timestamp']:
                        item['timestamp'] = item['timestamp'].isoformat() if isinstance(item['timestamp'], datetime) else item['timestamp']
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(snapshot_dict, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“ ëª¨ë‹ˆí„°ë§ ìŠ¤ëƒ…ìƒ· ì €ì¥: {filepath}")
            return str(filepath)
            
        except Exception as e:
            logger.error(f"ìŠ¤ëƒ…ìƒ· ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def run_continuous_monitoring(self):
        """ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì‹¤í–‰"""
        logger.info(f"ğŸš€ ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ê°±ì‹  ê°„ê²©: {self.refresh_interval}ì´ˆ)")
        self.is_monitoring = True
        
        try:
            while self.is_monitoring:
                # ìƒˆ ìŠ¤ëƒ…ìƒ· ìƒì„±
                snapshot = self.create_dashboard_snapshot()
                
                # ëŒ€ì‹œë³´ë“œ í‘œì‹œ
                self.display_dashboard(snapshot)
                
                # ìŠ¤ëƒ…ìƒ· ì €ì¥ (ë§¤ 10ë¶„ë§ˆë‹¤)
                if snapshot.timestamp.minute % 10 == 0:
                    self.save_snapshot_to_file(snapshot)
                
                # ëŒ€ê¸°
                time.sleep(self.refresh_interval)
                
        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ìì— ì˜í•´ ëª¨ë‹ˆí„°ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            self.is_monitoring = False
        except Exception as e:
            logger.error(f"ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
    
    def generate_status_report(self) -> str:
        """ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±"""
        snapshot = self.create_dashboard_snapshot()
        
        report_lines = [
            "# ë„¤ì´ë²„ ë¶€ë™ì‚° DB ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸",
            f"ìƒì„± ì‹œê°„: {snapshot.timestamp.strftime('%Y-%m-%d %H:%M:%S')}",
            f"ì „ì²´ ìƒíƒœ: {snapshot.overall_status}",
            "",
            "## ì£¼ìš” ë©”íŠ¸ë¦­",
        ]
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
        categories = [
            ("ìŠ¤í‚¤ë§ˆ ê±´ê°• ìƒíƒœ", snapshot.schema_health),
            ("ì„±ëŠ¥ ë©”íŠ¸ë¦­", snapshot.performance_metrics),
            ("ë°ì´í„° í’ˆì§ˆ", snapshot.data_quality)
        ]
        
        for category_name, metrics in categories:
            report_lines.append(f"### {category_name}")
            for metric in metrics:
                status_symbol = {"EXCELLENT": "âœ…", "GOOD": "âœ…", "FAIR": "âš ï¸", "POOR": "âŒ", "CRITICAL": "ğŸš¨"}.get(metric.status, "â“")
                report_lines.append(f"- {status_symbol} {metric.name}: {metric.value} {metric.unit}")
            report_lines.append("")
        
        if snapshot.alerts:
            report_lines.append("## í™œì„± ì•Œë¦¼")
            for alert in snapshot.alerts:
                level_symbol = {"CRITICAL": "ğŸš¨", "WARNING": "âš ï¸", "INFO": "â„¹ï¸"}.get(alert.get('level'), "â“")
                report_lines.append(f"- {level_symbol} {alert.get('message')}")
            report_lines.append("")
        
        if snapshot.recommendations:
            report_lines.append("## ê¶Œì¥ì‚¬í•­")
            for i, rec in enumerate(snapshot.recommendations, 1):
                report_lines.append(f"{i}. {rec}")
        
        return "\n".join(report_lines)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ë„¤ì´ë²„ ë¶€ë™ì‚° DB ì¢…í•© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")
    parser.add_argument('--refresh', type=int, default=30, help='ê°±ì‹  ê°„ê²© (ì´ˆ)')
    parser.add_argument('--once', action='store_true', help='í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ê³  ì¢…ë£Œ')
    parser.add_argument('--report', action='store_true', help='ë¦¬í¬íŠ¸ ìƒì„±í•˜ê³  ì¢…ë£Œ')
    parser.add_argument('--save-snapshot', action='store_true', help='ìŠ¤ëƒ…ìƒ· ì €ì¥')
    
    args = parser.parse_args()
    
    # ëŒ€ì‹œë³´ë“œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    dashboard = ComprehensiveMonitoringDashboard(refresh_interval=args.refresh)
    
    try:
        if args.report:
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = dashboard.generate_status_report()
            print(report)
            
            # íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_file = current_dir / f"monitoring_report_{timestamp}.md"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"\nğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
            
        elif args.once:
            # í•œ ë²ˆë§Œ ì‹¤í–‰
            snapshot = dashboard.create_dashboard_snapshot()
            dashboard.display_dashboard(snapshot)
            
            if args.save_snapshot:
                dashboard.save_snapshot_to_file(snapshot)
                
        else:
            # ì§€ì†ì  ëª¨ë‹ˆí„°ë§
            dashboard.run_continuous_monitoring()
    
    except KeyboardInterrupt:
        print("\nëª¨ë‹ˆí„°ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)