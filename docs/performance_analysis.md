# 네이버 부동산 수집기 성능 분석 및 개선 방안

## 📊 현재 성능 분석

### 주요 병목점 식별

1. **API 호출 지연 (가장 큰 병목)**
   - 현재 설정: 5-10초 랜덤 지연 (`settings.py:49-50`)
   - 매물별 최소 2회 API 호출 (목록 + 상세정보)
   - 단일 매물당 약 7.5초 평균 대기시간

2. **순차 처리 방식**
   - 매물별 순차 처리 (`collection_service.py:139-152`)
   - 병렬 처리 없이 한 번에 하나씩만 처리
   - 데이터베이스 저장도 순차 진행

3. **과도한 데이터베이스 쿼리**
   - 매 매물마다 중복 확인 쿼리 실행 (`optimized_repository.py:136`)
   - 4개 테이블에 순차 저장 (병렬 처리 없음)
   - 변경 히스토리 저장으로 추가 쿼리 발생

4. **주소 변환 지연**
   - 카카오 API 호출로 추가 지연 발생
   - 좌표→주소 변환시 추가 네트워크 대기시간

## ⚡ 성능 개선 방안

### 1. API 호출 최적화 (가장 높은 우선순위)

#### A. 지연 시간 단축
```python
# 현재 설정 (settings.py)
'request_delay_min': 5.0,    # 5초
'request_delay_max': 10.0,   # 10초

# 개선안 1: 점진적 단축
'request_delay_min': 2.0,    # 2초
'request_delay_max': 4.0,    # 4초

# 개선안 2: 적응형 지연 (429 에러 기반 자동 조정)
```

**예상 효과**: 수집 시간 **60% 단축**

#### B. 배치 API 호출 도입
- 현재: 매물별 개별 호출
- 개선: 여러 매물 ID를 한 번에 요청하는 배치 엔드포인트 활용 검토

### 2. 병렬 처리 도입 (두 번째 우선순위)

#### A. 매물 수집 병렬화
```python
# 현재: 순차 처리
for article_no in page_articles:
    success = self.collect_single_article(article_no)

# 개선안: 동시 처리 (3-5개 스레드)
with ThreadPoolExecutor(max_workers=3) as executor:
    futures = [executor.submit(self.collect_single_article, article_no) 
              for article_no in page_articles]
```

**예상 효과**: 추가 **3-5배 속도 향상**

#### B. 데이터베이스 저장 병렬화
- 4개 테이블(중개사, 편의시설, 사진) 동시 저장
- 메인 매물 테이블 저장 후 나머지 테이블 병렬 처리

### 3. 데이터베이스 최적화 (세 번째 우선순위)

#### A. 배치 삽입 도입
```python
# 현재: 매물별 개별 INSERT
for property in properties:
    repository.save_property(property)

# 개선안: 배치 INSERT
repository.save_properties_batch(properties)
```

#### B. 중복 확인 최적화
- 매물 번호 기반 인덱스 활용
- 배치 중복 확인으로 쿼리 수 감소

### 4. 캐싱 시스템 도입

#### A. 매물 상태 캐싱
- Redis/메모리 캐시로 기존 매물 목록 저장
- 중복 확인 쿼리 대신 캐시 조회

#### B. 주소 변환 캐싱
- 좌표→주소 변환 결과 캐시 저장
- 동일 좌표 재요청 시 캐시 사용

## 📈 단계별 구현 로드맵

### Phase 1: 즉시 적용 가능한 개선 (1-2일)
1. **API 지연 시간 단축**: 5-10초 → 2-4초
2. **429 에러 기반 적응형 지연 구현**
3. **불필요한 로깅 줄이기**

**예상 효과**: 현재 대비 **50-60% 속도 향상**

### Phase 2: 병렬 처리 도입 (3-5일)
1. **매물 수집 병렬화** (3-5 스레드)
2. **데이터베이스 저장 병렬화**
3. **에러 핸들링 강화**

**예상 효과**: Phase 1 대비 추가 **200-300% 향상**

### Phase 3: 고급 최적화 (1-2주)
1. **배치 API 호출 구현**
2. **Redis 캐싱 시스템**
3. **스마트 재시도 로직**

**예상 효과**: 전체적으로 **5-10배 성능 향상**

## 🎯 구체적 수치 예측

### 현재 성능 (100개 매물 수집)
- 매물당 평균 처리시간: ~10초
- 총 소요시간: ~16분
- API 호출: 200회 (목록 + 상세)

### Phase 1 적용 후
- 매물당 평균 처리시간: ~4초
- 총 소요시간: **~7분** (57% 단축)

### Phase 2 적용 후 (병렬 처리)
- 매물당 평균 처리시간: ~1.5초 (3개 스레드)
- 총 소요시간: **~2.5분** (85% 단축)

### Phase 3 적용 후 (완전 최적화)
- 매물당 평균 처리시간: ~0.5초
- 총 소요시간: **~1분** (94% 단축)

## ⚠️ 주의사항

1. **Rate Limiting 준수**
   - 네이버 API 429 에러 모니터링 필수
   - 병렬 처리시 요청 수 제한 주의

2. **데이터 정합성**
   - 병렬 처리시 중복 저장 방지
   - 트랜잭션 관리 강화

3. **메모리 사용량**
   - 병렬 처리시 메모리 사용량 증가 예상
   - 적절한 스레드 풀 크기 설정 필요

## 🔧 권장 즉시 적용 사항

1. **`settings.py` 수정**: 지연 시간 2-4초로 단축
2. **적응형 지연 로직 추가**: 429 에러시 자동 대기시간 증가
3. **진행률 표시 개선**: 실시간 ETA 계산 추가

이러한 개선사항 적용으로 **현재 대비 5-10배 빠른 수집 속도**를 달성할 수 있을 것으로 예상됩니다.