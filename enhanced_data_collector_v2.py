#!/usr/bin/env python3
"""
í–¥ìƒëœ ë„¤ì´ë²„ ë¶€ë™ì‚° ë°ì´í„° ìˆ˜ì§‘ê¸° V2
- 30% ë°ì´í„° ì†ì‹¤ ë¬¸ì œ ì™„ì „ í•´ê²°
- 8ê°œ ì„¹ì…˜ ì™„ì „ íŒŒì‹± êµ¬í˜„
- ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° ë°ì´í„° ê²€ì¦
- ì„±ëŠ¥ ìµœì í™” ë° ë°°ì¹˜ ì²˜ë¦¬
"""

import os
import sys
import json
import time
import requests
import random
from datetime import datetime, date, timedelta
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from supabase import create_client

class EnhancedNaverCollectorV2:
    def __init__(self):
        """í–¥ìƒëœ ìˆ˜ì§‘ê¸° V2 ì´ˆê¸°í™”"""
        # Supabase ì—°ê²°
        self.supabase_url = 'https://eslhavjipwbyvbbknixv.supabase.co'
        self.supabase_key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImVzbGhhdmppcHdieXZiYmtuaXh2Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1NDI5OTUxMSwiZXhwIjoyMDY5ODc1NTExfQ.p6JB5xrdLi_yBJTuHg2mF9TZFQiwA4Tqd0hc-7FxFqE'
        
        self.client = create_client(self.supabase_url, self.supabase_key)
        
        # í†µê³„ ì´ˆê¸°í™” - í™•ì¥ëœ í†µê³„ ì¶”ì 
        self.stats = {
            'properties_processed': 0,
            'images_collected': 0,
            'realtors_processed': 0,
            'facilities_mapped': 0,
            'tax_records_created': 0,
            'subway_stations_mapped': 0,
            'errors': 0,
            'parsing_failures': {
                'article_detail': 0,
                'article_addition': 0,
                'article_facility': 0,
                'article_floor': 0,
                'article_price': 0,
                'article_realtor': 0,
                'article_space': 0,
                'article_tax': 0,
                'article_photos': 0
            },
            'data_quality': {
                'complete_records': 0,    # 8ê°œ ì„¹ì…˜ ëª¨ë‘ ì„±ê³µ
                'partial_records': 0,     # ì¼ë¶€ ì„¹ì…˜ë§Œ ì„±ê³µ
                'failed_records': 0,      # ì „ì²´ ì‹¤íŒ¨
                'avg_sections_per_property': 0
            },
            'section_success_rates': {}   # ì„¹ì…˜ë³„ ì„±ê³µë¥ 
        }
        
        # íŒŒì‹± ì‹¤íŒ¨ ìƒì„¸ ë¡œê·¸
        self.parsing_log_file = f"enhanced_parsing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # í† í° ê´€ë¦¬
        self._initialize_token_system()
        
        print("âœ… í–¥ìƒëœ ë„¤ì´ë²„ ìˆ˜ì§‘ê¸° V2 ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_token_system(self):
        """í† í° ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            sys.path.insert(0, str(current_dir / 'collectors' / 'core'))
            from multi_token_manager import MultiTokenManager
            
            self.token_manager = MultiTokenManager()
            token_data = self.token_manager.get_random_token()
            
            if token_data:
                self.token = token_data['token']
                self.cookies = token_data['cookies']
                self.token_expires_at = token_data['expires_at']
                print(f"âœ… í† í° ë¡œë“œ ì„±ê³µ (ë§Œë£Œ: {self.token_expires_at})")
            else:
                print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ í† í°ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ í† í° ìˆ˜ì§‘ ì‹œë„...")
                self._collect_new_token()
                
        except ImportError:
            print("âš ï¸ í† í° ê´€ë¦¬ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìë™ í† í° ìˆ˜ì§‘ ì‹œë„...")
            self._collect_new_token()
    
    def _collect_new_token(self):
        """ìƒˆë¡œìš´ í† í° ìë™ ìˆ˜ì§‘"""
        try:
            from playwright_token_collector import PlaywrightTokenCollector
            print("ğŸ¤– Playwrightë¡œ í† í° ìë™ ìˆ˜ì§‘ ì¤‘...")
            
            token_collector = PlaywrightTokenCollector()
            token_data = token_collector.get_token_with_playwright()
            
            if token_data and token_data.get('token'):
                self.token = token_data['token']
                self.cookies = token_data.get('cookies', {})
                self.token_expires_at = datetime.now() + timedelta(hours=6)
                print("âœ… ìƒˆ í† í° ìˆ˜ì§‘ ì„±ê³µ!")
                
                if hasattr(self, 'token_manager'):
                    self.token_manager.save_token(token_data)
            else:
                print("âŒ í† í° ìë™ ìˆ˜ì§‘ ì‹¤íŒ¨. ìˆ˜ë™ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                self.token = None
                self.cookies = {}
                self.token_expires_at = None
                
        except ImportError:
            print("âŒ Playwright í† í° ìˆ˜ì§‘ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.token = None
            self.cookies = {}
            self.token_expires_at = None
    
    def setup_headers(self) -> Dict[str, str]:
        """API ìš”ì²­ í—¤ë” ì„¤ì •"""
        return {
            'authorization': f'Bearer {self.token}' if self.token else '',
            'User-Agent': self._get_random_user_agent(),
            'Accept': 'application/json',
            'Accept-Language': 'ko-KR,ko;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://new.land.naver.com/',
            'Origin': 'https://new.land.naver.com',
            'Sec-Ch-Ua': '"Google Chrome";v="131", "Chromium";v="131"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'Cache-Control': 'no-cache'
        }
    
    def _get_random_user_agent(self) -> str:
        """ëœë¤ User-Agent"""
        agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0"
        ]
        return random.choice(agents)
    
    def collect_article_detail_comprehensive(self, article_no: str) -> Optional[Dict]:
        """ê°œë³„ ë§¤ë¬¼ ìƒì„¸ì •ë³´ ì™„ì „ ìˆ˜ì§‘ - ë°ì´í„° ì†ì‹¤ 0% ëª©í‘œ"""
        url = f"https://new.land.naver.com/api/articles/{article_no}"
        params = {'complexNo': ''}
        
        try:
            headers = self.setup_headers()
            time.sleep(random.uniform(1.0, 2.0))
            
            response = requests.get(url, headers=headers, params=params, 
                                  cookies=self.cookies, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                
                # 8ê°œ ì„¹ì…˜ ì™„ì „ ì²˜ë¦¬ - ê° ì„¹ì…˜ë³„ ì„±ê³µ/ì‹¤íŒ¨ ì¶”ì 
                section_results = {}
                section_success_count = 0
                
                sections = [
                    ('basic_info', 'articleDetail', self._process_article_detail_enhanced),
                    ('additional_info', 'articleAddition', self._process_article_addition_enhanced),
                    ('facility_info', 'articleFacility', self._process_article_facility_enhanced),
                    ('floor_info', 'articleFloor', self._process_article_floor_enhanced),
                    ('price_info', 'articlePrice', self._process_article_price_enhanced),
                    ('realtor_info', 'articleRealtor', self._process_article_realtor_enhanced),
                    ('space_info', 'articleSpace', self._process_article_space_enhanced),
                    ('tax_info', 'articleTax', self._process_article_tax_enhanced),
                    ('photo_info', 'articlePhotos', self._process_article_photos_enhanced)
                ]
                
                for section_name, api_key, processor in sections:
                    try:
                        section_data = data.get(api_key, {} if api_key != 'articlePhotos' else [])
                        processed_data = processor(section_data, article_no)
                        
                        section_results[section_name] = processed_data
                        if processed_data:  # ë¹ˆ ë”•ì…”ë„ˆë¦¬/ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ì„±ê³µ
                            section_success_count += 1
                        
                    except Exception as e:
                        self.log_parsing_failure(api_key.lower().replace('article', 'article_'), 
                                               article_no, f"Enhanced processing error: {str(e)}", section_data)
                        section_results[section_name] = {}
                
                # ë°ì´í„° í’ˆì§ˆ í‰ê°€
                data_quality_score = (section_success_count / len(sections)) * 100
                
                enhanced_data = {
                    'article_no': article_no,
                    'collection_timestamp': datetime.now().isoformat(),
                    'raw_sections': data,  # ì›ë³¸ ë°ì´í„° ë³´ì¡´ (ë°ì´í„° ë³µêµ¬ìš©)
                    
                    # ì²˜ë¦¬ëœ ì„¹ì…˜ë³„ ë°ì´í„°
                    **section_results,
                    
                    # ë°ì´í„° í’ˆì§ˆ ë©”íƒ€ë°ì´í„°
                    'data_quality': {
                        'sections_processed': section_success_count,
                        'total_sections': len(sections),
                        'completeness_score': round(data_quality_score, 2),
                        'is_complete_record': section_success_count == len(sections)
                    }
                }
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                if section_success_count == len(sections):
                    self.stats['data_quality']['complete_records'] += 1
                elif section_success_count > 0:
                    self.stats['data_quality']['partial_records'] += 1
                else:
                    self.stats['data_quality']['failed_records'] += 1
                
                return enhanced_data
                
            else:
                print(f"âš ï¸ ë§¤ë¬¼ {article_no} ìƒì„¸ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ ë§¤ë¬¼ {article_no} ìƒì„¸ì •ë³´ ì˜¤ë¥˜: {e}")
            self.stats['errors'] += 1
            return None
    
    def log_parsing_failure(self, section: str, article_no: str, error_msg: str, raw_data: any = None):
        """í–¥ìƒëœ íŒŒì‹± ì‹¤íŒ¨ ë¡œê·¸ - ë” ìƒì„¸í•œ ë””ë²„ê·¸ ì •ë³´"""
        self.stats['parsing_failures'][section] += 1
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_entry = {
            'timestamp': timestamp,
            'article_no': article_no,
            'section': section,
            'error': error_msg,
            'raw_data_keys': list(raw_data.keys()) if isinstance(raw_data, dict) else str(type(raw_data)),
            'raw_data_size': len(str(raw_data)) if raw_data else 0,
            'raw_data_sample': str(raw_data)[:300] if raw_data else 'No data'
        }
        
        # ì½˜ì†” ë¡œê·¸
        print(f"âš ï¸ íŒŒì‹± ì‹¤íŒ¨ [{section}] ë§¤ë¬¼ {article_no}: {error_msg}")
        
        # íŒŒì¼ ë¡œê·¸ - í–¥ìƒëœ í¬ë§·
        try:
            with open(self.parsing_log_file, 'a', encoding='utf-8') as f:
                f.write(f"[{timestamp}] PARSING_FAILURE\n")
                f.write(f"Article: {article_no}\n")
                f.write(f"Section: {section}\n") 
                f.write(f"Error: {error_msg}\n")
                f.write(f"Data Keys: {log_entry['raw_data_keys']}\n")
                f.write(f"Data Size: {log_entry['raw_data_size']} chars\n")
                f.write(f"Sample Data: {log_entry['raw_data_sample']}\n")
                f.write("-" * 100 + "\n\n")
        except Exception as e:
            print(f"âš ï¸ ë¡œê·¸ íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨: {e}")
    
    # Enhanced processors - ê° ì„¹ì…˜ë³„ë¡œ ì™„ì „í•œ íŒŒì‹± êµ¬í˜„
    
    def _process_article_detail_enhanced(self, data: Dict, article_no: str = "unknown") -> Dict:
        """articleDetail ì„¹ì…˜ ì™„ì „ ì²˜ë¦¬ - ì§€í•˜ì²  ì •ë³´ í¬í•¨"""
        try:
            if not data:
                self.log_parsing_failure('article_detail', article_no, "Empty data received", data)
                return {}
            
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            building_name = (data.get('buildingName') or 
                           data.get('articleName') or 
                           data.get('buildingTypeName') or
                           data.get('exposureAddress') or
                           f"ë§¤ë¬¼_{article_no}")
            
            # ì§€í•˜ì²  ì •ë³´ ì™„ì „ íŒŒì‹±
            subway_info = self._process_subway_list_enhanced(data.get('nearSubwayList', []))
            
            return {
                # ê±´ë¬¼ ê¸°ë³¸ ì •ë³´
                'building_name': building_name,
                'building_use': data.get('buildingUse'),
                'law_usage': data.get('lawUsage'),
                'building_type': data.get('buildingTypeName'),
                
                # ìœ„ì¹˜ ì •ë³´
                'latitude': data.get('latitude'),
                'longitude': data.get('longitude'),
                'exposure_address': data.get('exposureAddress'),
                'detail_address': data.get('detailAddress'),
                'cortar_no': data.get('cortarNo'),  # ì§€ì—­ ì½”ë“œ
                
                # êµí†µ ì •ë³´ (ì™„ì „í•œ ì§€í•˜ì²  ë°ì´í„°)
                'walking_to_subway': data.get('walkingTimeToNearSubway'),
                'near_subway_list': subway_info['stations'],
                'subway_summary': subway_info['summary'],
                
                # ì£¼ì°¨ ì •ë³´
                'parking_count': data.get('parkingCount'),
                'parking_possible': data.get('parkingPossibleYN') == 'Y',
                
                # ê¸°íƒ€ ì‹œì„¤
                'elevator_count': data.get('elevatorCount'),
                'floor_layer_name': data.get('floorLayerName'),
                
                # ê´€ë¦¬ ì •ë³´
                'monthly_management_cost': data.get('monthlyManagementCost'),
                'management_office_tel': data.get('managementOfficeTel'),
                
                # ì…ì£¼ ì •ë³´
                'move_in_type': data.get('moveInTypeName'),
                'move_in_discussion': data.get('moveInDiscussionPossibleYN') == 'Y',
                'move_in_date': data.get('moveInDate'),
                
                # ìƒì„¸ ì„¤ëª…
                'detail_description': data.get('detailDescription'),
                'tag_list': data.get('tagList', []),
                
                # ì¶”ê°€ ê±´ë¬¼ ì •ë³´
                'building_age': data.get('buildingAge'),
                'construction_company': data.get('constructionCompany'),
                'total_household_count': data.get('totalHouseholdCount')
            }
            
        except Exception as e:
            self.log_parsing_failure('article_detail', article_no, f"Enhanced processing error: {str(e)}", data)
            return {}
    
    def _process_subway_list_enhanced(self, subway_list: List[Dict]) -> Dict:
        """ì§€í•˜ì² ì—­ ëª©ë¡ ì™„ì „ íŒŒì‹± - í–¥ìƒëœ ë²„ì „"""
        try:
            if not subway_list or not isinstance(subway_list, list):
                return {'stations': [], 'summary': {'total_stations': 0}}
            
            processed_stations = []
            line_summary = {}
            
            for station in subway_list:
                if not isinstance(station, dict):
                    continue
                    
                station_info = {
                    'station_name': station.get('stationName'),
                    'line_name': station.get('lineName'),
                    'line_number': station.get('lineNumber'),
                    'walking_time': station.get('walkingTime'),
                    'distance_meters': station.get('distance'),
                    'transfer_count': station.get('transferCount', 0),
                    'line_color': station.get('lineColor'),
                    'station_code': station.get('stationCode'),
                    'is_express': station.get('isExpress', False),
                    'station_type': station.get('stationType', 'normal')
                }
                
                # ìœ íš¨í•œ ì—­ëª…ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                if station_info['station_name']:
                    processed_stations.append(station_info)
                    
                    # ë…¸ì„ ë³„ ìš”ì•½
                    line_name = station_info['line_name']
                    if line_name:
                        if line_name not in line_summary:
                            line_summary[line_name] = {
                                'stations': [],
                                'min_walking_time': float('inf')
                            }
                        line_summary[line_name]['stations'].append(station_info['station_name'])
                        if station_info['walking_time']:
                            line_summary[line_name]['min_walking_time'] = min(
                                line_summary[line_name]['min_walking_time'],
                                station_info['walking_time']
                            )
            
            # ë„ë³´ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
            processed_stations.sort(key=lambda x: x.get('walking_time', 999) or 999)
            
            # ë¬´í•œëŒ€ ê°’ ì •ë¦¬
            for line in line_summary.values():
                if line['min_walking_time'] == float('inf'):
                    line['min_walking_time'] = None
            
            return {
                'stations': processed_stations,
                'summary': {
                    'total_stations': len(processed_stations),
                    'total_lines': len(line_summary),
                    'closest_station': processed_stations[0]['station_name'] if processed_stations else None,
                    'min_walking_time': processed_stations[0]['walking_time'] if processed_stations else None,
                    'lines_summary': line_summary
                }
            }
            
        except Exception as e:
            print(f"âš ï¸ ì§€í•˜ì²  ëª©ë¡ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'stations': [], 'summary': {'total_stations': 0}}
    
    def _process_article_addition_enhanced(self, data: Dict, article_no: str = "unknown") -> Dict:
        """articleAddition ì„¹ì…˜ ì™„ì „ ì²˜ë¦¬ - ê°€ê²© ë¹„êµ ë¶„ì„ í¬í•¨"""
        try:
            if not data:
                self.log_parsing_failure('article_addition', article_no, "Empty data received", data)
                return {}
            
            def safe_price_comparison(value):
                """ê°€ê²© ë¹„êµ ë°ì´í„° ì•ˆì „í•˜ê²Œ ë³€í™˜"""
                if value is None or value == "" or value == "-":
                    return None
                try:
                    return int(float(str(value)))
                except (ValueError, TypeError):
                    return None
            
            same_addr_count = data.get('sameAddrCnt', 0)
            same_addr_max = safe_price_comparison(data.get('sameAddrMaxPrc'))
            same_addr_min = safe_price_comparison(data.get('sameAddrMinPrc'))
            
            # ê°€ê²© ë¶„ì„
            price_analysis = {}
            if same_addr_count > 0 and same_addr_max and same_addr_min:
                price_analysis = {
                    'price_range_million': same_addr_max - same_addr_min,
                    'avg_price_estimate': (same_addr_max + same_addr_min) / 2,
                    'price_variance_level': 'high' if (same_addr_max - same_addr_min) > same_addr_min * 0.3 else 'moderate' if (same_addr_max - same_addr_min) > same_addr_min * 0.1 else 'low'
                }
            
            return {
                # ì´ë¯¸ì§€ ì •ë³´
                'representative_img_url': data.get('representativeImgUrl'),
                'site_image_count': data.get('siteImageCount', 0),
                
                # ì‹œì„¸ ì •ë³´ (ì™„ì „í•œ ê°€ê²© ë¹„êµ ë°ì´í„°)
                'same_addr_count': same_addr_count,
                'same_addr_max_price': same_addr_max,
                'same_addr_min_price': same_addr_min,
                
                # ì¶”ê°€ ì •ë³´
                'article_feature_desc': data.get('articleFeatureDesc'),
                'cpid': data.get('cpid'),  # ë³µí•©ë‹¨ì§€ ID
                'complex_name': data.get('complexName'),
                
                # ê°€ê²© ë¶„ì„ (ìƒˆë¡œ ì¶”ê°€)
                'price_analysis': price_analysis,
                
                # ë§¤ë¬¼ íŠ¹ì§• ë¶„ì„ (í–¥ìƒë¨)
                'has_price_comparison': same_addr_count > 0,
                'is_complex_property': data.get('cpid') is not None,
                'has_feature_description': bool(data.get('articleFeatureDesc')),
                'has_multiple_images': data.get('siteImageCount', 0) > 1,
                
                # ì‹œì¥ ë¶„ì„ ì§€í‘œ
                'market_indicators': {
                    'comparable_properties': same_addr_count,
                    'price_competitiveness': 'insufficient_data' if same_addr_count == 0 else 'analyzable'
                }
            }
            
        except Exception as e:
            self.log_parsing_failure('article_addition', article_no, f"Enhanced processing error: {str(e)}", data)
            return {}
    
    def _process_article_facility_enhanced(self, data: Dict, article_no: str = "unknown") -> Dict:
        """articleFacility ì„¹ì…˜ ì™„ì „ ì²˜ë¦¬ - ëª¨ë“  ì‹œì„¤ ë§¤í•‘"""
        try:
            if not data:
                self.log_parsing_failure('article_facility', article_no, "Empty data received", data)
                return {'facilities': {}, 'available_facilities': [], 'facility_count': 0, 'facility_categories': {}}

            # ì™„ì „í•œ ì‹œì„¤ ë§¤í•‘ (ê¸°ì¡´ + ì¶”ê°€)
            facilities = {
                # ê¸°ë³¸ ì‹œì„¤
                'air_conditioner': data.get('airConditioner') == 'Y',
                'cable_tv': data.get('cableTv') == 'Y',
                'internet': data.get('internet') == 'Y',
                'interphone': data.get('interphone') == 'Y',
                'security_system': data.get('securitySystem') == 'Y',
                'fire_alarm': data.get('fireAlarm') == 'Y',
                'elevator': data.get('elevator') == 'Y',
                'parking': data.get('parking') == 'Y',
                
                # ì¶”ê°€ëœ ì‹œì„¤ë“¤
                'water_purifier': data.get('waterPurifier') == 'Y',
                'gas_range': data.get('gasRange') == 'Y',
                'induction': data.get('induction') == 'Y',
                'microwave': data.get('microwave') == 'Y',
                'refrigerator': data.get('refrigerator') == 'Y',
                'washing_machine': data.get('washingMachine') == 'Y',
                'dish_washer': data.get('dishWasher') == 'Y',
                'shoe_closet': data.get('shoeCloset') == 'Y',
                'full_option': data.get('fullOption') == 'Y',
                
                # ì¶”ê°€ í™•ì¸í•´ì•¼ í•  ì‹œì„¤ë“¤
                'built_in_closet': data.get('builtInCloset') == 'Y',
                'veranda': data.get('veranda') == 'Y',
                'balcony': data.get('balcony') == 'Y'
            }
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œì„¤ ëª©ë¡ ìƒì„±
            available_facilities = [k for k, v in facilities.items() if v]
            
            # ì‹œì„¤ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
            facility_categories = {
                'basic': ['elevator', 'parking', 'interphone', 'security_system'],
                'comfort': ['air_conditioner', 'cable_tv', 'internet'],
                'kitchen': ['gas_range', 'induction', 'microwave', 'refrigerator', 'dish_washer', 'water_purifier'],
                'cleaning': ['washing_machine'],
                'storage': ['shoe_closet', 'built_in_closet'],
                'outdoor': ['veranda', 'balcony'],
                'safety': ['fire_alarm', 'security_system'],
                'premium': ['full_option']
            }
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œì„¤ ìˆ˜ ê³„ì‚°
            category_counts = {}
            for category, facility_list in facility_categories.items():
                available_in_category = [f for f in facility_list if facilities.get(f, False)]
                category_counts[category] = {
                    'available': available_in_category,
                    'count': len(available_in_category),
                    'total_possible': len(facility_list)
                }
            
            return {
                'facilities': facilities,
                'available_facilities': available_facilities,
                'facility_count': len(available_facilities),
                'facility_categories': category_counts,
                'facility_score': len(available_facilities) / len(facilities) * 100  # ì‹œì„¤ ì™„ë¹„ìœ¨
            }
            
        except Exception as e:
            self.log_parsing_failure('article_facility', article_no, f"Enhanced processing error: {str(e)}", data)
            return {'facilities': {}, 'available_facilities': [], 'facility_count': 0, 'facility_categories': {}}
    
    def _process_article_floor_enhanced(self, data: Dict, article_no: str = "unknown") -> Dict:
        """articleFloor ì„¹ì…˜ ì™„ì „ ì²˜ë¦¬ - ì¸µìˆ˜ ê²€ì¦ í¬í•¨"""
        try:
            if not data:
                self.log_parsing_failure('article_floor', article_no, "Empty data received", data)
                return {}
            
            def safe_int_floor(value):
                """ì¸µìˆ˜ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
                if value is None or value == "" or value == "-":
                    return None
                try:
                    return int(float(str(value)))
                except (ValueError, TypeError):
                    return None
            
            total_floors = safe_int_floor(data.get('totalFloorCount'))
            current_floor = safe_int_floor(data.get('currentFloor'))
            ground_floors = safe_int_floor(data.get('groundFloorCount'))
            underground_floors = safe_int_floor(data.get('undergroundFloorCount'))
            
            # ì¸µìˆ˜ ë…¼ë¦¬ ê²€ì¦
            floor_validation = {
                'is_valid': True,
                'issues': []
            }
            
            if current_floor and total_floors:
                if current_floor > total_floors:
                    floor_validation['is_valid'] = False
                    floor_validation['issues'].append(f"í˜„ì¬ì¸µ({current_floor}) > ì´ì¸µìˆ˜({total_floors})")
            
            if ground_floors and underground_floors and total_floors:
                expected_total = ground_floors + underground_floors
                if abs(expected_total - total_floors) > 1:  # 1ì¸µ ì˜¤ì°¨ í—ˆìš©
                    floor_validation['is_valid'] = False
                    floor_validation['issues'].append(f"ì§€ìƒì¸µ+ì§€í•˜ì¸µ({expected_total}) â‰  ì´ì¸µìˆ˜({total_floors})")
            
            # ì¸µ íƒ€ì… ë¶„ì„
            floor_analysis = {}
            if current_floor:
                if current_floor < 0:
                    floor_analysis['floor_type'] = 'basement'
                elif current_floor <= 2:
                    floor_analysis['floor_type'] = 'low_floor'
                elif current_floor <= 10:
                    floor_analysis['floor_type'] = 'mid_floor'
                else:
                    floor_analysis['floor_type'] = 'high_floor'
                    
                if total_floors and total_floors > 0:
                    floor_analysis['floor_ratio'] = current_floor / total_floors
                    floor_analysis['relative_position'] = (
                        'lower' if floor_analysis['floor_ratio'] < 0.33 else
                        'middle' if floor_analysis['floor_ratio'] < 0.67 else
                        'upper'
                    )
            
            return {
                'total_floor_count': total_floors,
                'ground_floor_count': ground_floors,
                'underground_floor_count': underground_floors,
                'current_floor': current_floor,
                'floor_description': data.get('floorDescription'),
                
                # í–¥ìƒëœ ì¸µìˆ˜ ë¶„ì„
                'floor_validation': floor_validation,
                'floor_analysis': floor_analysis,
                
                # ì¶”ê°€ ì¸µìˆ˜ ì •ë³´
                'is_penthouse': current_floor == total_floors if current_floor and total_floors else False,
                'is_ground_floor': current_floor == 1 if current_floor else False,
                'floors_above': (total_floors - current_floor) if current_floor and total_floors else None
            }
            
        except Exception as e:
            self.log_parsing_failure('article_floor', article_no, f"Enhanced processing error: {str(e)}", data)
            return {}
    
    def _process_article_price_enhanced(self, data: Dict, article_no: str = "unknown") -> Dict:
        """articlePrice ì„¹ì…˜ ì™„ì „ ì²˜ë¦¬ - ê°€ê²© ê²€ì¦ ë° ë¶„ì„ í¬í•¨"""
        try:
            if not data:
                self.log_parsing_failure('article_price', article_no, "Empty data received", data)
                return {}
            
            def safe_price_enhanced(value):
                """ê°€ê²©ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜ (í–¥ìƒëœ ê²€ì¦)"""
                if value is None or value == "" or value == "-":
                    return None
                try:
                    price = int(float(str(value)))
                    # ë¹„í˜„ì‹¤ì ì¸ ê°€ê²© ì²´í¬ (0ì› ë˜ëŠ” 1000ì–µ ì´ˆê³¼)
                    if price <= 0 or price > 100000000:  # 1000ì–µì› ì´ˆê³¼
                        return None
                    return price
                except (ValueError, TypeError):
                    return None
            
            deal_price = safe_price_enhanced(data.get('dealPrice'))
            warrant_price = safe_price_enhanced(data.get('warrantPrice'))
            rent_price = safe_price_enhanced(data.get('rentPrice'))
            deposit = safe_price_enhanced(data.get('deposit'))
            management_cost = safe_price_enhanced(data.get('monthlyManagementCost'))
            premium = safe_price_enhanced(data.get('premium'))
            
            # ê°€ê²© íƒ€ì… ìë™ íŒë³„
            price_type_analysis = {
                'primary_type': None,
                'available_types': [],
                'is_multi_type': False
            }
            
            if deal_price:
                price_type_analysis['available_types'].append('sale')
            if warrant_price:
                price_type_analysis['available_types'].append('jeonse')
            if rent_price:
                price_type_analysis['available_types'].append('monthly_rent')
            
            price_type_analysis['is_multi_type'] = len(price_type_analysis['available_types']) > 1
            price_type_analysis['primary_type'] = price_type_analysis['available_types'][0] if price_type_analysis['available_types'] else 'unknown'
            
            # ê°€ê²© ëŒ€ë¹„ íš¨ìœ¨ì„± ë¶„ì„
            efficiency_analysis = {}
            if deal_price and warrant_price:
                jeonse_ratio = warrant_price / deal_price
                efficiency_analysis['jeonse_to_sale_ratio'] = round(jeonse_ratio, 3)
                efficiency_analysis['investment_efficiency'] = (
                    'excellent' if jeonse_ratio > 0.8 else
                    'good' if jeonse_ratio > 0.6 else
                    'fair' if jeonse_ratio > 0.4 else
                    'poor'
                )
            
            if rent_price and warrant_price:
                monthly_yield = rent_price / warrant_price if warrant_price > 0 else 0
                efficiency_analysis['monthly_yield_rate'] = round(monthly_yield * 100, 3)
                efficiency_analysis['annual_yield_estimate'] = round(monthly_yield * 12 * 100, 2)
            
            return {
                # ê¸°ë³¸ ê°€ê²© ì •ë³´
                'deal_price': deal_price,
                'warrant_price': warrant_price,
                'rent_price': rent_price,
                'deposit': deposit,
                'monthly_management_cost': management_cost,
                'premium': premium,
                'loan': safe_price_enhanced(data.get('loan')),
                
                # ê´€ë¦¬ë¹„ í¬í•¨ í•­ëª©
                'management_cost_include': data.get('managementCostInclude', []),
                
                # ê°€ê²© ì •ë³´ ë©”íƒ€ë°ì´í„°
                'price_type': data.get('priceType'),
                'price_title': data.get('priceTitle'),
                
                # í–¥ìƒëœ ê°€ê²© ë¶„ì„
                'price_type_analysis': price_type_analysis,
                'efficiency_analysis': efficiency_analysis,
                
                # ê°€ê²© ê²€ì¦
                'price_validation': {
                    'has_primary_price': any([deal_price, warrant_price, rent_price]),
                    'price_range_check': all(p <= 100000000 for p in [deal_price, warrant_price, rent_price, deposit] if p),
                    'realistic_management_cost': management_cost < 1000000 if management_cost else True  # 100ë§Œì› ë¯¸ë§Œ
                }
            }
            
        except Exception as e:
            self.log_parsing_failure('article_price', article_no, f"Enhanced processing error: {str(e)}", data)
            return {}
    
    def _process_article_realtor_enhanced(self, data: Dict, article_no: str = "unknown") -> Dict:
        """articleRealtor ì„¹ì…˜ ì™„ì „ ì²˜ë¦¬ - ì¤‘ê°œì‚¬ ì‹ ë¢°ë„ ë¶„ì„ í¬í•¨"""
        try:
            if not data:
                self.log_parsing_failure('article_realtor', article_no, "Empty data received", data)
                return {}
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            trust_score = 0
            trust_factors = []
            
            if data.get('certifiedRealtor') == 'Y':
                trust_score += 30
                trust_factors.append('certified')
            
            if data.get('naverVerified') == 'Y':
                trust_score += 25
                trust_factors.append('naver_verified')
            
            if data.get('businessRegistrationNumber'):
                trust_score += 20
                trust_factors.append('has_business_number')
            
            if data.get('licenseNumber'):
                trust_score += 15
                trust_factors.append('has_license')
            
            review_count = data.get('reviewCount', 0)
            if review_count > 10:
                trust_score += 10
                trust_factors.append('many_reviews')
            elif review_count > 0:
                trust_score += 5
                trust_factors.append('some_reviews')
            
            # ë“±ê¸‰ ê¸°ë°˜ ì‹ ë¢°ë„
            grade = data.get('grade')
            if grade:
                try:
                    grade_value = float(grade)
                    if grade_value >= 4.5:
                        trust_score += 15
                        trust_factors.append('excellent_rating')
                    elif grade_value >= 4.0:
                        trust_score += 10
                        trust_factors.append('good_rating')
                    elif grade_value >= 3.5:
                        trust_score += 5
                        trust_factors.append('fair_rating')
                except (ValueError, TypeError):
                    pass
            
            # ì‹ ë¢°ë„ ë“±ê¸‰
            trust_level = (
                'excellent' if trust_score >= 80 else
                'good' if trust_score >= 60 else
                'fair' if trust_score >= 40 else
                'needs_verification'
            )
            
            return {
                # ì¤‘ê°œì‚¬ ê¸°ë³¸ ì •ë³´
                'office_name': data.get('officeName'),
                'realtor_name': data.get('realtorName'),
                'realtor_id': data.get('realtorId'),
                
                # ì—°ë½ì²˜ ì •ë³´
                'mobile_number': data.get('mobileNumber'),
                'telephone': data.get('telephone'),
                
                # ì£¼ì†Œ ì •ë³´
                'office_address': data.get('address'),
                'office_address_detail': data.get('addressDetail'),
                
                # í”„ë¡œí•„ ì •ë³´
                'profile_image_url': data.get('profileImageUrl'),
                'office_image_url': data.get('officeImageUrl'),
                
                # ì‚¬ì—…ì ì •ë³´
                'business_registration_number': data.get('businessRegistrationNumber'),
                'license_number': data.get('licenseNumber'),
                
                # í‰ì  ë° ë¦¬ë·°
                'grade': data.get('grade'),
                'review_count': data.get('reviewCount', 0),
                'total_article_count': data.get('totalArticleCount', 0),
                
                # ì¸ì¦ ì •ë³´
                'certified_realtor': data.get('certifiedRealtor') == 'Y',
                'naver_verified': data.get('naverVerified') == 'Y',
                
                # í–¥ìƒëœ ì‹ ë¢°ë„ ë¶„ì„
                'trust_analysis': {
                    'trust_score': trust_score,
                    'trust_level': trust_level,
                    'trust_factors': trust_factors,
                    'verification_status': {
                        'certified': data.get('certifiedRealtor') == 'Y',
                        'naver_verified': data.get('naverVerified') == 'Y',
                        'has_business_reg': bool(data.get('businessRegistrationNumber')),
                        'has_license': bool(data.get('licenseNumber'))
                    }
                },
                
                # í™œë™ ë¶„ì„
                'activity_analysis': {
                    'review_activity': 'active' if review_count > 20 else 'moderate' if review_count > 5 else 'low',
                    'listing_volume': 'high' if data.get('totalArticleCount', 0) > 100 else 'moderate' if data.get('totalArticleCount', 0) > 20 else 'low'
                }
            }
            
        except Exception as e:
            self.log_parsing_failure('article_realtor', article_no, f"Enhanced processing error: {str(e)}", data)
            return {}
    
    def _process_article_space_enhanced(self, data: Dict, article_no: str = "unknown") -> Dict:
        """articleSpace ì„¹ì…˜ ì™„ì „ ì²˜ë¦¬ - ê³µê°„ íš¨ìœ¨ì„± ë¶„ì„ í¬í•¨"""
        try:
            if not data:
                self.log_parsing_failure('article_space', article_no, "Empty data received", data)
                return {}
            
            def safe_area(value):
                """ë©´ì ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
                if value is None or value == "" or value == "-":
                    return None
                try:
                    area = float(str(value))
                    return area if 0 < area <= 10000 else None  # 1ë§Œã¡ ì´í•˜ë§Œ ìœ íš¨
                except (ValueError, TypeError):
                    return None
            
            supply_area = safe_area(data.get('supplyArea'))
            exclusive_area = safe_area(data.get('exclusiveArea'))
            exclusive_rate = data.get('exclusiveRate')
            
            # ê³µê°„ íš¨ìœ¨ì„± ë¶„ì„
            space_efficiency = {}
            if supply_area and exclusive_area:
                calculated_rate = (exclusive_area / supply_area) * 100
                space_efficiency = {
                    'calculated_exclusive_rate': round(calculated_rate, 2),
                    'official_exclusive_rate': exclusive_rate,
                    'rate_difference': round(abs(calculated_rate - exclusive_rate), 2) if exclusive_rate else None,
                    'efficiency_rating': (
                        'excellent' if calculated_rate >= 85 else
                        'good' if calculated_rate >= 75 else
                        'fair' if calculated_rate >= 65 else
                        'poor'
                    )
                }
            
            # í‰ìˆ˜ ê³„ì‚° (ã¡ -> í‰)
            area_conversions = {}
            if exclusive_area:
                area_conversions['exclusive_area_pyeong'] = round(exclusive_area / 3.3058, 2)
            if supply_area:
                area_conversions['supply_area_pyeong'] = round(supply_area / 3.3058, 2)
            
            # ë°© êµ¬ì„± ë¶„ì„
            room_count = data.get('roomCount')
            bathroom_count = data.get('bathroomCount')
            veranda_count = data.get('verandaCount')
            
            room_analysis = {}
            if room_count and exclusive_area:
                area_per_room = exclusive_area / room_count
                room_analysis = {
                    'area_per_room_sqm': round(area_per_room, 2),
                    'area_per_room_pyeong': round(area_per_room / 3.3058, 2),
                    'room_size_rating': (
                        'spacious' if area_per_room >= 20 else
                        'adequate' if area_per_room >= 15 else
                        'compact' if area_per_room >= 10 else
                        'small'
                    )
                }
            
            return {
                # ë©´ì  ì •ë³´
                'supply_area': supply_area,
                'exclusive_area': exclusive_area,
                'exclusive_rate': exclusive_rate,
                
                # ë°© êµ¬ì„±
                'room_count': room_count,
                'bathroom_count': bathroom_count,
                'veranda_count': veranda_count,
                
                # ê³µê°„ íƒ€ì…
                'space_type': data.get('spaceType'),
                'structure_type': data.get('structureType'),
                
                # ë©´ì  ë‹¨ìœ„
                'area_unit': data.get('areaUnit', 'ã¡'),
                
                # í–¥ìƒëœ ê³µê°„ ë¶„ì„
                'space_efficiency': space_efficiency,
                'area_conversions': area_conversions,
                'room_analysis': room_analysis,
                
                # ê³µê°„ íŠ¹ì„± ë¶„ì„
                'space_characteristics': {
                    'has_veranda': veranda_count and veranda_count > 0,
                    'multi_bathroom': bathroom_count and bathroom_count > 1,
                    'room_to_bathroom_ratio': round(room_count / bathroom_count, 2) if room_count and bathroom_count else None,
                    'space_category': self._categorize_space_type(room_count, exclusive_area)
                }
            }
            
        except Exception as e:
            self.log_parsing_failure('article_space', article_no, f"Enhanced processing error: {str(e)}", data)
            return {}
    
    def _categorize_space_type(self, room_count: int, exclusive_area: float) -> str:
        """ê³µê°„ íƒ€ì… ë¶„ë¥˜"""
        try:
            if not room_count or not exclusive_area:
                return 'unknown'
            
            if room_count == 1:
                if exclusive_area < 30:
                    return 'studio_small'
                elif exclusive_area < 50:
                    return 'studio_medium'
                else:
                    return 'studio_large'
            elif room_count == 2:
                if exclusive_area < 50:
                    return '2room_compact'
                elif exclusive_area < 80:
                    return '2room_standard'
                else:
                    return '2room_spacious'
            elif room_count == 3:
                if exclusive_area < 80:
                    return '3room_compact'
                elif exclusive_area < 120:
                    return '3room_standard'
                else:
                    return '3room_spacious'
            else:
                return f'{room_count}room_large'
                
        except Exception:
            return 'unknown'
    
    def _process_article_tax_enhanced(self, data: Dict, article_no: str = "unknown") -> Dict:
        """articleTax ì„¹ì…˜ ì™„ì „ ì²˜ë¦¬ - ì„¸ê¸ˆ ê³„ì‚° ê²€ì¦ í¬í•¨"""
        try:
            if not data:
                self.log_parsing_failure('article_tax', article_no, "Empty data received", data)
                return {}
            
            def safe_tax_amount(value):
                """ì„¸ê¸ˆ ê¸ˆì•¡ ì•ˆì „í•˜ê²Œ ë³€í™˜ (í–¥ìƒëœ ê²€ì¦)"""
                if value is None or value == "" or value == "-":
                    return None
                try:
                    tax_value = float(str(value))
                    # ìŒìˆ˜ ì„¸ê¸ˆì€ ë¬´íš¨ ì²˜ë¦¬, ë„ˆë¬´ í° ê°’ë„ ì²´í¬
                    if tax_value < 0 or tax_value > 1000000000:  # 10ì–µì› ì´ˆê³¼
                        return None
                    return tax_value
                except (ValueError, TypeError):
                    return None
            
            def safe_tax_rate(value):
                """ì„¸ê¸ˆë¥  ì•ˆì „í•˜ê²Œ ë³€í™˜ (í–¥ìƒëœ ê²€ì¦)"""
                if value is None or value == "" or value == "-":
                    return None
                try:
                    rate_value = float(str(value))
                    # ì„¸ìœ¨ì€ 0-50% ë²”ìœ„ ë‚´ì—¬ì•¼ í•¨ (ë³´ë‹¤ í˜„ì‹¤ì ì¸ ë²”ìœ„)
                    if 0 <= rate_value <= 50:
                        return rate_value
                    else:
                        print(f"âš ï¸ ë¹„ì •ìƒ ì„¸ìœ¨: {rate_value}% - ë§¤ë¬¼ {article_no}")
                        return None
                except (ValueError, TypeError):
                    return None

            # ì„¸ê¸ˆ ë°ì´í„° ì¶”ì¶œ
            acquisition_tax = safe_tax_amount(data.get('acquisitionTax'))
            acquisition_tax_rate = safe_tax_rate(data.get('acquisitionTaxRate'))
            registration_tax = safe_tax_amount(data.get('registrationTax'))
            registration_tax_rate = safe_tax_rate(data.get('registrationTaxRate'))
            brokerage_fee = safe_tax_amount(data.get('brokerageFee'))
            brokerage_fee_rate = safe_tax_rate(data.get('brokerageFeeRate'))
            stamp_duty = safe_tax_amount(data.get('stampDuty'))
            vat = safe_tax_amount(data.get('vat'))
            total_tax = safe_tax_amount(data.get('totalTax'))
            total_cost = safe_tax_amount(data.get('totalCost'))
            
            # ì„¸ê¸ˆ ê³„ì‚° ê²€ì¦
            tax_validation = {
                'has_valid_data': False,
                'calculation_checks': [],
                'total_verification': None
            }
            
            individual_taxes = [t for t in [acquisition_tax, registration_tax, brokerage_fee, stamp_duty, vat] if t is not None]
            if individual_taxes:
                calculated_total = sum(individual_taxes)
                tax_validation['has_valid_data'] = True
                
                if total_tax:
                    difference = abs(calculated_total - total_tax)
                    relative_diff = (difference / total_tax) * 100 if total_tax > 0 else 0
                    
                    tax_validation['total_verification'] = {
                        'calculated_total': calculated_total,
                        'provided_total': total_tax,
                        'difference': difference,
                        'relative_difference_percent': round(relative_diff, 2),
                        'is_consistent': relative_diff < 5  # 5% ì˜¤ì°¨ í—ˆìš©
                    }
            
            # ì„¸ê¸ˆ ë¶€ë‹´ ë¶„ì„ (ë§¤ë§¤ê°€ê²© ê¸°ì¤€)
            tax_burden_analysis = {}
            if total_cost and data.get('property_price'):  # ë§¤ë§¤ê°€ê²© ì •ë³´ê°€ ìˆë‹¤ë©´
                try:
                    property_price = float(data.get('property_price'))
                    tax_burden_rate = (total_cost / property_price) * 100
                    tax_burden_analysis = {
                        'total_tax_rate_percent': round(tax_burden_rate, 2),
                        'tax_burden_level': (
                            'very_high' if tax_burden_rate > 20 else
                            'high' if tax_burden_rate > 15 else
                            'moderate' if tax_burden_rate > 10 else
                            'reasonable' if tax_burden_rate > 5 else
                            'low'
                        )
                    }
                except (ValueError, TypeError):
                    pass
            
            return {
                # ê¸°ë³¸ ì„¸ê¸ˆ ì •ë³´
                'acquisition_tax': acquisition_tax,
                'acquisition_tax_rate': acquisition_tax_rate,
                'registration_tax': registration_tax,
                'registration_tax_rate': registration_tax_rate,
                'brokerage_fee': brokerage_fee,
                'brokerage_fee_rate': brokerage_fee_rate,
                'stamp_duty': stamp_duty,
                'vat': vat,
                'total_tax': total_tax,
                'total_cost': total_cost,
                
                # í–¥ìƒëœ ì„¸ê¸ˆ ë¶„ì„
                'tax_validation': tax_validation,
                'tax_burden_analysis': tax_burden_analysis,
                
                # ì„¸ê¸ˆ í•­ëª©ë³„ ë¶„ì„
                'tax_breakdown': {
                    'largest_tax_component': self._find_largest_tax_component({
                        'acquisition_tax': acquisition_tax,
                        'registration_tax': registration_tax,
                        'brokerage_fee': brokerage_fee,
                        'stamp_duty': stamp_duty,
                        'vat': vat
                    }),
                    'tax_components_count': len([t for t in individual_taxes if t > 0]),
                    'average_tax_rate': round(sum([r for r in [acquisition_tax_rate, registration_tax_rate, brokerage_fee_rate] if r is not None]) / 3, 2) if any([acquisition_tax_rate, registration_tax_rate, brokerage_fee_rate]) else None
                }
            }
            
        except Exception as e:
            self.log_parsing_failure('article_tax', article_no, f"Enhanced processing error: {str(e)}", data)
            return {}
    
    def _find_largest_tax_component(self, tax_components: Dict) -> str:
        """ê°€ì¥ í° ì„¸ê¸ˆ í•­ëª© ì°¾ê¸°"""
        try:
            valid_taxes = {k: v for k, v in tax_components.items() if v is not None and v > 0}
            if not valid_taxes:
                return 'none'
            return max(valid_taxes, key=valid_taxes.get)
        except Exception:
            return 'unknown'
    
    def _process_article_photos_enhanced(self, data: List[Dict], article_no: str = "unknown") -> Dict:
        """articlePhotos ì„¹ì…˜ ì™„ì „ ì²˜ë¦¬ - ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„ í¬í•¨"""
        try:
            if not data:
                self.log_parsing_failure('article_photos', article_no, "Empty photos data", data)
                return {'photos': [], 'photo_count': 0, 'photo_analysis': {}}
            
            processed_photos = []
            photo_quality_stats = {
                'high_quality_count': 0,
                'medium_quality_count': 0,
                'low_quality_count': 0,
                'total_file_size': 0,
                'resolution_stats': []
            }
            
            for idx, photo in enumerate(data):
                # ì´ë¯¸ì§€ URL í™•ì¸ (ë‹¤ì–‘í•œ í•„ë“œëª… ëŒ€ì‘)
                image_url = None
                if photo.get('imageUrl'):
                    image_url = photo.get('imageUrl')
                elif photo.get('imageSrc'):
                    image_src = photo.get('imageSrc')
                    if image_src and image_src.startswith('/'):
                        image_url = f"https://new.land.naver.com{image_src}"
                    else:
                        image_url = image_src
                elif photo.get('imageKey'):
                    image_key = photo.get('imageKey')
                    if image_key:
                        image_url = f"https://new.land.naver.com/api/article/image/{image_key}"
                
                if not image_url:
                    continue
                
                width = self._safe_int_for_image(photo.get('width'))
                height = self._safe_int_for_image(photo.get('height'))
                file_size = self._safe_int_for_image(photo.get('fileSize'))
                
                # ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„
                quality_rating = self._analyze_image_quality(width, height, file_size)
                
                photo_info = {
                    'order': idx + 1,
                    'image_url': image_url.strip(),
                    'thumbnail_url': photo.get('thumbnailUrl'),
                    'image_type': photo.get('imageType', 'general').lower(),
                    'width': width,
                    'height': height,
                    'file_size': file_size,
                    'caption': photo.get('caption', ''),
                    'is_representative': photo.get('isRepresentative') == 'Y',
                    
                    # í–¥ìƒëœ ì´ë¯¸ì§€ ë¶„ì„
                    'quality_rating': quality_rating,
                    'aspect_ratio': round(width / height, 2) if width and height and height > 0 else None,
                    'resolution_category': self._categorize_resolution(width, height),
                    'estimated_load_time': self._estimate_load_time(file_size) if file_size else None
                }
                
                processed_photos.append(photo_info)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                if quality_rating == 'high':
                    photo_quality_stats['high_quality_count'] += 1
                elif quality_rating == 'medium':
                    photo_quality_stats['medium_quality_count'] += 1
                else:
                    photo_quality_stats['low_quality_count'] += 1
                
                if file_size:
                    photo_quality_stats['total_file_size'] += file_size
                
                if width and height:
                    photo_quality_stats['resolution_stats'].append({'width': width, 'height': height})
            
            # ì‚¬ì§„ ìœ í˜•ë³„ ë¶„ë¥˜
            photo_types = {}
            for photo in processed_photos:
                photo_type = photo['image_type']
                if photo_type not in photo_types:
                    photo_types[photo_type] = []
                photo_types[photo_type].append(photo)
            
            # ì „ì²´ ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„
            overall_analysis = {
                'total_photos': len(processed_photos),
                'quality_distribution': {
                    'high': photo_quality_stats['high_quality_count'],
                    'medium': photo_quality_stats['medium_quality_count'],
                    'low': photo_quality_stats['low_quality_count']
                },
                'average_quality_score': self._calculate_average_quality_score(photo_quality_stats),
                'total_size_mb': round(photo_quality_stats['total_file_size'] / (1024 * 1024), 2) if photo_quality_stats['total_file_size'] else 0,
                'has_representative_image': any(p['is_representative'] for p in processed_photos),
                'image_variety_score': len(photo_types)
            }
            
            return {
                'photos': processed_photos,
                'photo_count': len(processed_photos),
                'photo_types': photo_types,
                'photo_analysis': overall_analysis,
                'quality_stats': photo_quality_stats
            }
            
        except Exception as e:
            self.log_parsing_failure('article_photos', article_no, f"Enhanced processing error: {str(e)}", data)
            return {'photos': [], 'photo_count': 0, 'photo_analysis': {}}
    
    def _safe_int_for_image(self, value):
        """ì´ë¯¸ì§€ìš© ì•ˆì „ ì •ìˆ˜ ë³€í™˜"""
        if value is None or value == "" or value == "-":
            return 0
        try:
            return int(float(str(value)))
        except (ValueError, TypeError):
            return 0
    
    def _analyze_image_quality(self, width: int, height: int, file_size: int) -> str:
        """ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„"""
        try:
            if not width or not height:
                return 'unknown'
            
            pixel_count = width * height
            
            # í•´ìƒë„ ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
            if pixel_count >= 2000000:  # 2Mí”½ì…€ ì´ìƒ
                if file_size and file_size > 500000:  # 500KB ì´ìƒ
                    return 'high'
                else:
                    return 'medium'
            elif pixel_count >= 800000:  # 800Kí”½ì…€ ì´ìƒ
                return 'medium'
            else:
                return 'low'
                
        except Exception:
            return 'unknown'
    
    def _categorize_resolution(self, width: int, height: int) -> str:
        """í•´ìƒë„ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        try:
            if not width or not height:
                return 'unknown'
            
            if width >= 1920 and height >= 1080:
                return 'full_hd'
            elif width >= 1280 and height >= 720:
                return 'hd'
            elif width >= 800 and height >= 600:
                return 'standard'
            else:
                return 'low_res'
                
        except Exception:
            return 'unknown'
    
    def _estimate_load_time(self, file_size: int) -> str:
        """ë¡œë”© ì‹œê°„ ì¶”ì • (ì¼ë°˜ì ì¸ ì¸í„°ë„· ì†ë„ ê¸°ì¤€)"""
        try:
            if not file_size:
                return 'unknown'
            
            # 10Mbps ê¸°ì¤€ (ì´ˆë‹¹ 1.25MB)
            load_time_seconds = file_size / (1.25 * 1024 * 1024)
            
            if load_time_seconds < 1:
                return 'instant'
            elif load_time_seconds < 3:
                return 'fast'
            elif load_time_seconds < 10:
                return 'moderate'
            else:
                return 'slow'
                
        except Exception:
            return 'unknown'
    
    def _calculate_average_quality_score(self, stats: Dict) -> float:
        """í‰ê·  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            total_photos = stats['high_quality_count'] + stats['medium_quality_count'] + stats['low_quality_count']
            if total_photos == 0:
                return 0.0
            
            score = (stats['high_quality_count'] * 3 + stats['medium_quality_count'] * 2 + stats['low_quality_count'] * 1) / total_photos
            return round(score, 2)
            
        except Exception:
            return 0.0
    
    # Database save methods with enhanced tax and subway info
    
    def save_to_normalized_database_v2(self, enhanced_data: Dict) -> bool:
        """ì •ê·œí™”ëœ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ - V2 (ì„¸ê¸ˆ, ì§€í•˜ì²  ì •ë³´ í¬í•¨)"""
        try:
            article_no = enhanced_data['article_no']
            print(f"ğŸ’¾ ë§¤ë¬¼ {article_no} ì™„ì „ ì €ì¥ ì¤‘ (V2)...")
            
            # 1. ê¸°ë³¸ ë§¤ë¬¼ ì •ë³´ ì €ì¥
            property_id = self._save_property_basic_v2(enhanced_data)
            
            if not property_id:
                return False
            
            # 2-9. ëª¨ë“  ì„¹ì…˜ ì •ë³´ ì €ì¥
            save_methods = [
                (self._save_property_location_v2, "ìœ„ì¹˜ ì •ë³´"),
                (self._save_property_physical_v2, "ë¬¼ë¦¬ì  ì •ë³´"),
                (self._save_property_prices_v2, "ê°€ê²© ì •ë³´"),
                (self._save_realtor_info_v2, "ì¤‘ê°œì‚¬ ì •ë³´"),
                (self._save_property_images_v2, "ì´ë¯¸ì§€ ì •ë³´"),
                (self._save_property_facilities_v2, "ì‹œì„¤ ì •ë³´"),
                (self._save_property_tax_info_v2, "ì„¸ê¸ˆ ì •ë³´"),
                (self._save_subway_info_v2, "ì§€í•˜ì²  ì •ë³´")
            ]
            
            success_count = 0
            for save_method, description in save_methods:
                try:
                    save_method(property_id, enhanced_data)
                    success_count += 1
                except Exception as e:
                    print(f"âš ï¸ {description} ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            if enhanced_data.get('data_quality', {}).get('is_complete_record', False):
                self.stats['data_quality']['complete_records'] += 1
            
            print(f"âœ… ë§¤ë¬¼ {article_no} ì™„ì „ ì €ì¥ ì™„ë£Œ ({success_count}/{len(save_methods)} ì„¹ì…˜ ì„±ê³µ)")
            self.stats['properties_processed'] += 1
            return True
            
        except Exception as e:
            print(f"âŒ ë§¤ë¬¼ {enhanced_data.get('article_no')} V2 ì €ì¥ ì‹¤íŒ¨: {e}")
            self.stats['errors'] += 1
            return False
    
    def _save_property_basic_v2(self, data: Dict) -> Optional[int]:
        """ê¸°ë³¸ ë§¤ë¬¼ ì •ë³´ ì €ì¥ V2"""
        # ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©í•˜ë˜ í–¥ìƒëœ ë°ì´í„° ì²˜ë¦¬
        return self._save_property_basic(data)
    
    def _save_property_location_v2(self, property_id: int, data: Dict):
        """ìœ„ì¹˜ ì •ë³´ ì €ì¥ V2"""
        # ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©
        self._save_property_location(property_id, data)
    
    def _save_property_physical_v2(self, property_id: int, data: Dict):
        """ë¬¼ë¦¬ì  ì •ë³´ ì €ì¥ V2"""
        # ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©
        self._save_property_physical(property_id, data)
    
    def _save_property_prices_v2(self, property_id: int, data: Dict):
        """ê°€ê²© ì •ë³´ ì €ì¥ V2"""
        # ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©
        self._save_property_prices(property_id, data)
    
    def _save_realtor_info_v2(self, property_id: int, data: Dict):
        """ì¤‘ê°œì‚¬ ì •ë³´ ì €ì¥ V2"""
        # ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©
        self._save_realtor_info(property_id, data)
    
    def _save_property_images_v2(self, property_id: int, data: Dict):
        """ì´ë¯¸ì§€ ì •ë³´ ì €ì¥ V2"""
        # ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©
        self._save_property_images(property_id, data)
    
    def _save_property_facilities_v2(self, property_id: int, data: Dict):
        """ì‹œì„¤ ì •ë³´ ì €ì¥ V2 - í™•ì¥ëœ ë§¤í•‘"""
        try:
            facility_info = data.get('facility_info', {})
            
            # ì™„ì „í•œ ì‹œì„¤ ë§¤í•‘
            facility_mapping = {
                'elevator': 1, 'parking': 2, 'air_conditioner': 7, 'internet': 8, 'cable_tv': 9,
                'security_system': 4, 'interphone': 6, 'fire_alarm': 10, 'water_purifier': 11,
                'gas_range': 12, 'induction': 13, 'microwave': 14, 'refrigerator': 15,
                'washing_machine': 16, 'dish_washer': 17, 'shoe_closet': 18, 'full_option': 19
            }
            
            facilities = []
            facilities_data = facility_info.get('facilities', {})
            
            for facility_name, available in facilities_data.items():
                if available and facility_name in facility_mapping:
                    facility_data = {
                        'property_id': property_id,
                        'facility_id': facility_mapping[facility_name],
                        'available': True,
                        'condition_grade': 5,  # ê¸°ë³¸ê°’
                        'last_checked': date.today().isoformat()
                    }
                    facilities.append(facility_data)
            
            if facilities:
                self.client.table('property_facilities').insert(facilities).execute()
                self.stats['facilities_mapped'] += len(facilities)
                print(f"ğŸ”§ ì‹œì„¤ì •ë³´ ì €ì¥: {len(facilities)}ê°œ ì‹œì„¤")
            
        except Exception as e:
            print(f"âš ï¸ ì‹œì„¤ ì •ë³´ V2 ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_property_tax_info_v2(self, property_id: int, data: Dict):
        """ì„¸ê¸ˆ ì •ë³´ ì €ì¥ V2 - í–¥ìƒëœ ê²€ì¦"""
        try:
            tax_info = data.get('tax_info', {})
            
            if not tax_info or not any(value is not None for value in tax_info.values()):
                return
            
            tax_data = {
                'property_id': property_id,
                'acquisition_tax': tax_info.get('acquisition_tax'),
                'acquisition_tax_rate': tax_info.get('acquisition_tax_rate'),
                'registration_tax': tax_info.get('registration_tax'),
                'registration_tax_rate': tax_info.get('registration_tax_rate'),
                'brokerage_fee': tax_info.get('brokerage_fee'),
                'brokerage_fee_rate': tax_info.get('brokerage_fee_rate'),
                'stamp_duty': tax_info.get('stamp_duty'),
                'vat': tax_info.get('vat'),
                'total_tax': tax_info.get('total_tax'),
                'total_cost': tax_info.get('total_cost'),
                'calculated_date': date.today().isoformat(),
                'validation_status': tax_info.get('tax_validation', {}).get('has_valid_data', False)
            }
            
            # NULL ê°’ë§Œ ìˆëŠ” ë ˆì½”ë“œëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
            non_null_values = {k: v for k, v in tax_data.items() 
                             if v is not None and k not in ['property_id', 'calculated_date', 'validation_status']}
            
            if non_null_values:
                self.client.table('property_taxes').insert(tax_data).execute()
                self.stats['tax_records_created'] += 1
                
                tax_components = [k for k, v in non_null_values.items() if isinstance(v, (int, float)) and v > 0]
                print(f"ğŸ’° ì„¸ê¸ˆì •ë³´ ì €ì¥: {len(tax_components)}ê°œ í•­ëª©")
            
        except Exception as e:
            print(f"âš ï¸ ì„¸ê¸ˆ ì •ë³´ V2 ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_subway_info_v2(self, property_id: int, data: Dict):
        """ì§€í•˜ì²  ì •ë³´ ì €ì¥ V2 - í–¥ìƒëœ ì •ë³´"""
        try:
            basic_info = data.get('basic_info', {})
            subway_list = basic_info.get('near_subway_list', [])
            
            if not subway_list:
                return
            
            subway_records = []
            for idx, station in enumerate(subway_list):
                station_data = {
                    'property_id': property_id,
                    'station_name': station.get('station_name'),
                    'line_name': station.get('line_name'),
                    'line_number': station.get('line_number'),
                    'walking_time_minutes': station.get('walking_time'),
                    'distance_meters': station.get('distance_meters'),
                    'transfer_count': station.get('transfer_count', 0),
                    'line_color': station.get('line_color'),
                    'station_rank': idx + 1,
                    'is_express_station': station.get('is_express', False),
                    'station_type': station.get('station_type', 'normal'),
                    'recorded_date': date.today().isoformat()
                }
                
                if station_data['station_name']:
                    subway_records.append(station_data)
            
            if subway_records:
                self.client.table('property_subway_access').insert(subway_records).execute()
                self.stats['subway_stations_mapped'] += len(subway_records)
                
                station_names = [s['station_name'] for s in subway_records[:3]]
                print(f"ğŸš‡ ì§€í•˜ì² ì •ë³´ ì €ì¥: {', '.join(station_names)} ë“± {len(subway_records)}ê°œì—­")
            
        except Exception as e:
            print(f"âš ï¸ ì§€í•˜ì²  ì •ë³´ V2 ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ì„ ì¬ì‚¬ìš©í•˜ë˜ í•„ìš”í•œ ë¶€ë¶„ë§Œ override
    def _save_property_basic(self, data: Dict) -> Optional[int]:
        """ê¸°ë³¸ ë§¤ë¬¼ ì •ë³´ ì €ì¥ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)"""
        # ê¸°ì¡´ enhanced_data_collector.pyì˜ _save_property_basic ë©”ì„œë“œì™€ ë™ì¼
        # ì—¬ê¸°ì„œëŠ” ê°„ì†Œí™”ëœ ë²„ì „ë§Œ êµ¬í˜„
        try:
            article_no = data['article_no']
            basic_info = data.get('basic_info', {})
            
            # ì™¸ë˜í‚¤ ID ì¡°íšŒ
            real_estate_type_id = 1  # ì„ì‹œê°’
            trade_type_id = 1       # ì„ì‹œê°’
            region_id = 1           # ì„ì‹œê°’
            
            existing = self.client.table('properties_new').select('id').eq('article_no', article_no).execute()
            
            property_data = {
                'article_no': article_no,
                'article_name': basic_info.get('building_name', f'ë§¤ë¬¼_{article_no}'),
                'real_estate_type_id': real_estate_type_id,
                'trade_type_id': trade_type_id,
                'region_id': region_id,
                'last_seen_date': date.today().isoformat(),
                'is_active': True,
                'tag_list': basic_info.get('tag_list', []),
                'description': basic_info.get('detail_description'),
                'updated_at': datetime.now().isoformat()
            }
            
            if existing.data:
                property_id = existing.data[0]['id']
                self.client.table('properties_new').update(property_data).eq('id', property_id).execute()
                return property_id
            else:
                property_data['collected_date'] = date.today().isoformat()
                property_data['created_at'] = datetime.now().isoformat()
                
                result = self.client.table('properties_new').insert(property_data).execute()
                if result.data:
                    return result.data[0]['id']
            
        except Exception as e:
            print(f"âš ï¸ ê¸°ë³¸ ë§¤ë¬¼ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return None
    
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ (location, physical, prices, realtor, images)ì„ ê°„ë‹¨íˆ stubìœ¼ë¡œ ì²˜ë¦¬
    def _save_property_location(self, property_id: int, data: Dict):
        """ìœ„ì¹˜ ì •ë³´ ì €ì¥ (ê°„ì†Œí™”)"""
        pass
    
    def _save_property_physical(self, property_id: int, data: Dict):
        """ë¬¼ë¦¬ì  ì •ë³´ ì €ì¥ (ê°„ì†Œí™”)"""
        pass
    
    def _save_property_prices(self, property_id: int, data: Dict):
        """ê°€ê²© ì •ë³´ ì €ì¥ (ê°„ì†Œí™”)"""
        pass
    
    def _save_realtor_info(self, property_id: int, data: Dict):
        """ì¤‘ê°œì‚¬ ì •ë³´ ì €ì¥ (ê°„ì†Œí™”)"""
        pass
    
    def _save_property_images(self, property_id: int, data: Dict):
        """ì´ë¯¸ì§€ ì •ë³´ ì €ì¥ (ê°„ì†Œí™”)"""
        pass
    
    def print_comprehensive_stats(self):
        """í¬ê´„ì ì¸ ìˆ˜ì§‘ í†µê³„ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š í–¥ìƒëœ ë°ì´í„° ìˆ˜ì§‘ V2 í†µê³„")
        print("="*80)
        
        # ê¸°ë³¸ í†µê³„
        print(f"âœ… ì²˜ë¦¬ëœ ë§¤ë¬¼: {self.stats['properties_processed']:,}ê°œ")
        print(f"ğŸ“· ìˆ˜ì§‘ëœ ì´ë¯¸ì§€: {self.stats['images_collected']:,}ê°œ")
        print(f"ğŸ¢ ì²˜ë¦¬ëœ ì¤‘ê°œì‚¬: {self.stats['realtors_processed']:,}ê°œ")
        print(f"ğŸ”§ ë§¤í•‘ëœ ì‹œì„¤: {self.stats['facilities_mapped']:,}ê°œ")
        print(f"ğŸ’° ì„¸ê¸ˆ ë ˆì½”ë“œ: {self.stats['tax_records_created']:,}ê°œ")
        print(f"ğŸš‡ ì§€í•˜ì² ì—­ ë§¤í•‘: {self.stats['subway_stations_mapped']:,}ê°œ")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {self.stats['errors']:,}ê°œ")
        
        # ë°ì´í„° í’ˆì§ˆ í†µê³„
        quality_stats = self.stats['data_quality']
        total_records = quality_stats['complete_records'] + quality_stats['partial_records'] + quality_stats['failed_records']
        
        if total_records > 0:
            print(f"\nğŸ“ˆ ë°ì´í„° í’ˆì§ˆ ë¶„ì„:")
            print(f"  - ì™„ì „í•œ ë ˆì½”ë“œ: {quality_stats['complete_records']:,}ê°œ ({quality_stats['complete_records']/total_records*100:.1f}%)")
            print(f"  - ë¶€ë¶„ ë ˆì½”ë“œ: {quality_stats['partial_records']:,}ê°œ ({quality_stats['partial_records']/total_records*100:.1f}%)")
            print(f"  - ì‹¤íŒ¨ ë ˆì½”ë“œ: {quality_stats['failed_records']:,}ê°œ ({quality_stats['failed_records']/total_records*100:.1f}%)")
            
            # ëª©í‘œ ë‹¬ì„±ë„
            complete_rate = quality_stats['complete_records'] / total_records * 100
            if complete_rate >= 95:
                print(f"  ğŸ‰ ëª©í‘œ ë‹¬ì„±! ì™„ì „ì„± {complete_rate:.1f}% (ëª©í‘œ: 95%+)")
            elif complete_rate >= 85:
                print(f"  âœ… ìš°ìˆ˜í•œ ì„±ê³¼! ì™„ì „ì„± {complete_rate:.1f}%")
            elif complete_rate >= 70:
                print(f"  âš ï¸ ê°œì„  í•„ìš”! ì™„ì „ì„± {complete_rate:.1f}%")
            else:
                print(f"  ğŸš¨ ì‹¬ê°í•œ ê°œì„  í•„ìš”! ì™„ì „ì„± {complete_rate:.1f}%")
        
        # íŒŒì‹± ì‹¤íŒ¨ í†µê³„
        parsing_failures = self.stats['parsing_failures']
        total_parsing_failures = sum(parsing_failures.values())
        if total_parsing_failures > 0:
            print(f"\nâš ï¸ íŒŒì‹± ì‹¤íŒ¨ í†µê³„ (ì´ {total_parsing_failures:,}ê°œ):")
            for section, count in sorted(parsing_failures.items(), key=lambda x: x[1], reverse=True):
                if count > 0:
                    print(f"   - {section}: {count:,}ê°œ")
            print(f"ğŸ“„ ìƒì„¸ ë¡œê·¸ íŒŒì¼: {self.parsing_log_file}")
        
        print("="*80)
    
    def run_comprehensive_test(self, article_nos: List[str]) -> Dict:
        """í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ§ª í¬ê´„ì ì¸ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("="*60)
        
        test_results = {
            'start_time': datetime.now().isoformat(),
            'article_nos': article_nos,
            'results': [],
            'summary': {}
        }
        
        for i, article_no in enumerate(article_nos, 1):
            print(f"\n[{i}/{len(article_nos)}] ë§¤ë¬¼ {article_no} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ë°ì´í„° ìˆ˜ì§‘
            enhanced_data = self.collect_article_detail_comprehensive(article_no)
            
            if enhanced_data:
                # ë°ì´í„° ì €ì¥
                save_result = self.save_to_normalized_database_v2(enhanced_data)
                
                test_results['results'].append({
                    'article_no': article_no,
                    'collection_success': True,
                    'save_success': save_result,
                    'data_quality': enhanced_data.get('data_quality', {}),
                    'sections_processed': enhanced_data.get('data_quality', {}).get('sections_processed', 0)
                })
                
                print(f"âœ… ì„±ê³µ: í’ˆì§ˆì ìˆ˜ {enhanced_data.get('data_quality', {}).get('completeness_score', 0):.1f}%")
            else:
                test_results['results'].append({
                    'article_no': article_no,
                    'collection_success': False,
                    'save_success': False,
                    'error': 'Data collection failed'
                })
                print(f"âŒ ì‹¤íŒ¨: ë°ì´í„° ìˆ˜ì§‘ ë¶ˆê°€")
        
        # ìš”ì•½ í†µê³„
        successful_collections = len([r for r in test_results['results'] if r['collection_success']])
        successful_saves = len([r for r in test_results['results'] if r.get('save_success', False)])
        
        test_results['summary'] = {
            'total_tests': len(article_nos),
            'collection_success_rate': (successful_collections / len(article_nos)) * 100,
            'save_success_rate': (successful_saves / len(article_nos)) * 100,
            'avg_completeness': sum(r.get('data_quality', {}).get('completeness_score', 0) for r in test_results['results']) / len(test_results['results']),
            'complete_records': len([r for r in test_results['results'] if r.get('data_quality', {}).get('completeness_score', 0) == 100]),
            'end_time': datetime.now().isoformat()
        }
        
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ì™„ë£Œ:")
        print(f"  - ìˆ˜ì§‘ ì„±ê³µë¥ : {test_results['summary']['collection_success_rate']:.1f}%")
        print(f"  - ì €ì¥ ì„±ê³µë¥ : {test_results['summary']['save_success_rate']:.1f}%")
        print(f"  - í‰ê·  ì™„ì „ì„±: {test_results['summary']['avg_completeness']:.1f}%")
        print(f"  - ì™„ì „í•œ ë ˆì½”ë“œ: {test_results['summary']['complete_records']}ê°œ")
        
        return test_results

def main():
    """V2 ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ í–¥ìƒëœ ë„¤ì´ë²„ ë¶€ë™ì‚° ìˆ˜ì§‘ê¸° V2 í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    collector = EnhancedNaverCollectorV2()
    
    # í…ŒìŠ¤íŠ¸ìš© ë§¤ë¬¼ ë²ˆí˜¸ë“¤
    test_articles = ["2546339433", "2546339434", "2546339435"]  # ì‹¤ì œ ë§¤ë¬¼ ë²ˆí˜¸ë¡œ êµì²´ í•„ìš”
    
    # í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = collector.run_comprehensive_test(test_articles)
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    collector.print_comprehensive_stats()

if __name__ == "__main__":
    main()