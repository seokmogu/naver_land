#!/usr/bin/env python3
"""
íŒŒì„œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ
- 8ê°œ ì„¹ì…˜ë³„ íŒŒì‹± ì„±ëŠ¥ ì¸¡ì •
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- ì²˜ë¦¬ ì†ë„ ë²¤ì¹˜ë§ˆí‚¹
"""

import os
import sys
import json
import time
import psutil
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import statistics

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from enhanced_data_collector import EnhancedNaverCollector

class ParserPerformanceTest:
    def __init__(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.collector = EnhancedNaverCollector()
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'test_start': datetime.now().isoformat(),
            'total_properties_tested': 0,
            'section_performance': {},
            'memory_usage': {},
            'error_rates': {},
            'processing_times': {
                'total': [],
                'per_section': {}
            }
        }
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì‹¤ì œ API ì‘ë‹µ ìƒ˜í”Œ)
        self.test_samples = []
    
    def generate_test_data(self, sample_count: int = 10) -> List[str]:
        """í…ŒìŠ¤íŠ¸ìš© ë§¤ë¬¼ ë²ˆí˜¸ ìƒì„±"""
        print(f"ğŸ” í…ŒìŠ¤íŠ¸ìš© ë§¤ë¬¼ {sample_count}ê°œ ìˆ˜ì§‘ ì¤‘...")
        
        try:
            # ì—­ì‚¼ë™ì—ì„œ í…ŒìŠ¤íŠ¸ ë§¤ë¬¼ ìˆ˜ì§‘
            test_articles = self.collector.collect_single_page_articles("1168010100", 1)
            
            if len(test_articles) >= sample_count:
                return test_articles[:sample_count]
            else:
                print(f"âš ï¸ ìš”ì²­ëœ {sample_count}ê°œë³´ë‹¤ ì ì€ {len(test_articles)}ê°œ ë§¤ë¬¼ ë°œê²¬")
                return test_articles
                
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def measure_memory_usage(self) -> Dict:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        process = psutil.Process()
        memory_info = process.memory_info()
        
        return {
            'rss_mb': round(memory_info.rss / 1024 / 1024, 2),  # Resident Set Size
            'vms_mb': round(memory_info.vms / 1024 / 1024, 2),  # Virtual Memory Size
            'percent': round(process.memory_percent(), 2)
        }
    
    def test_section_performance(self, article_data: Dict, article_no: str) -> Dict:
        """ê°œë³„ ì„¹ì…˜ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        section_times = {}
        section_errors = {}
        
        # 8ê°œ ì„¹ì…˜ ê°œë³„ í…ŒìŠ¤íŠ¸
        sections_to_test = [
            ('article_detail', '_process_article_detail', 'articleDetail'),
            ('article_addition', '_process_article_addition', 'articleAddition'),
            ('article_facility', '_process_article_facility', 'articleFacility'),
            ('article_floor', '_process_article_floor', 'articleFloor'),
            ('article_price', '_process_article_price', 'articlePrice'),
            ('article_realtor', '_process_article_realtor', 'articleRealtor'),
            ('article_space', '_process_article_space', 'articleSpace'),
            ('article_tax', '_process_article_tax', 'articleTax'),
            ('article_photos', '_process_article_photos', 'articlePhotos')
        ]
        
        for section_name, method_name, data_key in sections_to_test:
            try:
                # ì„¹ì…˜ ë°ì´í„° ì¶”ì¶œ
                if data_key == 'articlePhotos':
                    section_data = article_data.get(data_key, [])
                else:
                    section_data = article_data.get(data_key, {})
                
                # ì„±ëŠ¥ ì¸¡ì •
                start_time = time.perf_counter()
                
                # íŒŒì„œ ë©”ì„œë“œ í˜¸ì¶œ
                parser_method = getattr(self.collector, method_name)
                
                if data_key == 'articlePhotos':
                    result = parser_method(section_data, article_no)
                else:
                    result = parser_method(section_data, article_no)
                
                end_time = time.perf_counter()
                processing_time = (end_time - start_time) * 1000  # ë°€ë¦¬ì´ˆ
                
                section_times[section_name] = {
                    'processing_time_ms': round(processing_time, 3),
                    'data_size': len(str(section_data)),
                    'output_fields': len(result) if isinstance(result, dict) else 0,
                    'success': True
                }
                
            except Exception as e:
                section_times[section_name] = {
                    'processing_time_ms': 0,
                    'error': str(e),
                    'success': False
                }
                section_errors[section_name] = str(e)
        
        return section_times, section_errors
    
    def test_full_property_performance(self, article_no: str) -> Dict:
        """ì „ì²´ ë§¤ë¬¼ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"  ğŸ  ë§¤ë¬¼ {article_no} ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        memory_before = self.measure_memory_usage()
        start_time = time.perf_counter()
        
        try:
            # ì „ì²´ ë§¤ë¬¼ ë°ì´í„° ìˆ˜ì§‘
            enhanced_data = self.collector.collect_article_detail_enhanced(article_no)
            
            if not enhanced_data:
                return {
                    'success': False,
                    'error': 'Data collection failed',
                    'processing_time_ms': 0
                }
            
            end_time = time.perf_counter()
            total_time = (end_time - start_time) * 1000  # ë°€ë¦¬ì´ˆ
            
            memory_after = self.measure_memory_usage()
            memory_delta = memory_after['rss_mb'] - memory_before['rss_mb']
            
            # ì„¹ì…˜ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            raw_data = enhanced_data.get('raw_sections', {})
            section_times, section_errors = self.test_section_performance(raw_data, article_no)
            
            return {
                'success': True,
                'article_no': article_no,
                'total_processing_time_ms': round(total_time, 3),
                'memory_usage': {
                    'before_mb': memory_before['rss_mb'],
                    'after_mb': memory_after['rss_mb'],
                    'delta_mb': round(memory_delta, 2)
                },
                'section_performance': section_times,
                'section_errors': section_errors,
                'data_quality': {
                    'sections_processed': len([s for s in section_times.values() if s.get('success', False)]),
                    'total_sections': len(section_times),
                    'success_rate': len([s for s in section_times.values() if s.get('success', False)]) / len(section_times) * 100
                }
            }
            
        except Exception as e:
            end_time = time.perf_counter()
            total_time = (end_time - start_time) * 1000
            
            return {
                'success': False,
                'article_no': article_no,
                'error': str(e),
                'traceback': traceback.format_exc(),
                'processing_time_ms': round(total_time, 3)
            }
    
    def run_performance_test(self, test_count: int = 10) -> Dict:
        """ì „ì²´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ íŒŒì„œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘
        test_articles = self.generate_test_data(test_count)
        if not test_articles:
            print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"ğŸ“Š {len(test_articles)}ê°œ ë§¤ë¬¼ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì§„í–‰")
        
        test_results = []
        successful_tests = 0
        total_processing_times = []
        section_aggregated_times = {}
        
        for i, article_no in enumerate(test_articles, 1):
            print(f"\n[{i}/{len(test_articles)}] ë§¤ë¬¼ {article_no} í…ŒìŠ¤íŠ¸...")
            
            result = self.test_full_property_performance(article_no)
            test_results.append(result)
            
            if result['success']:
                successful_tests += 1
                total_processing_times.append(result['total_processing_time_ms'])
                
                # ì„¹ì…˜ë³„ ì‹œê°„ ì§‘ê³„
                for section, timing in result['section_performance'].items():
                    if timing.get('success', False):
                        if section not in section_aggregated_times:
                            section_aggregated_times[section] = []
                        section_aggregated_times[section].append(timing['processing_time_ms'])
                
                print(f"    âœ… ì„±ê³µ ({result['total_processing_time_ms']:.1f}ms)")
                print(f"    ğŸ“Š ì„¹ì…˜ ì„±ê³µë¥ : {result['data_quality']['success_rate']:.1f}%")
            else:
                print(f"    âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
        
        # í†µê³„ ê³„ì‚°
        success_rate = (successful_tests / len(test_articles)) * 100 if test_articles else 0
        
        performance_summary = {
            'test_summary': {
                'total_tests': len(test_articles),
                'successful_tests': successful_tests,
                'success_rate': round(success_rate, 2),
                'test_duration': datetime.now().isoformat()
            },
            'processing_time_stats': self._calculate_time_stats(total_processing_times),
            'section_performance': {},
            'memory_efficiency': self._analyze_memory_usage(test_results),
            'error_analysis': self._analyze_errors(test_results),
            'detailed_results': test_results
        }
        
        # ì„¹ì…˜ë³„ ì„±ëŠ¥ í†µê³„
        for section, times in section_aggregated_times.items():
            performance_summary['section_performance'][section] = self._calculate_time_stats(times)
        
        self.performance_stats.update(performance_summary)
        
        return performance_summary
    
    def _calculate_time_stats(self, times: List[float]) -> Dict:
        """ì‹œê°„ í†µê³„ ê³„ì‚°"""
        if not times:
            return {'count': 0, 'avg_ms': 0, 'min_ms': 0, 'max_ms': 0, 'std_ms': 0}
        
        return {
            'count': len(times),
            'avg_ms': round(statistics.mean(times), 3),
            'min_ms': round(min(times), 3),
            'max_ms': round(max(times), 3),
            'std_ms': round(statistics.stdev(times) if len(times) > 1 else 0, 3),
            'median_ms': round(statistics.median(times), 3)
        }
    
    def _analyze_memory_usage(self, test_results: List[Dict]) -> Dict:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        memory_deltas = []
        peak_usage = 0
        
        for result in test_results:
            if result.get('success') and 'memory_usage' in result:
                memory_info = result['memory_usage']
                memory_deltas.append(memory_info['delta_mb'])
                peak_usage = max(peak_usage, memory_info['after_mb'])
        
        return {
            'avg_memory_per_property_mb': round(statistics.mean(memory_deltas), 3) if memory_deltas else 0,
            'peak_memory_usage_mb': peak_usage,
            'memory_efficiency': 'Good' if statistics.mean(memory_deltas) < 10 else 'Needs Optimization' if memory_deltas else 'Unknown'
        }
    
    def _analyze_errors(self, test_results: List[Dict]) -> Dict:
        """ì—ëŸ¬ ë¶„ì„"""
        section_errors = {}
        total_errors = 0
        
        for result in test_results:
            if 'section_errors' in result:
                for section, error in result['section_errors'].items():
                    if section not in section_errors:
                        section_errors[section] = []
                    section_errors[section].append(error)
                    total_errors += 1
        
        error_summary = {}
        for section, errors in section_errors.items():
            error_summary[section] = {
                'error_count': len(errors),
                'common_errors': list(set(errors))[:3]  # ìƒìœ„ 3ê°œ ì—ëŸ¬ íƒ€ì…
            }
        
        return {
            'total_section_errors': total_errors,
            'sections_with_errors': len(section_errors),
            'section_error_details': error_summary
        }
    
    def generate_performance_report(self) -> str:
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"parser_performance_report_{timestamp}.json"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(self.performance_stats, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ“Š ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥: {report_file}")
            return report_file
            
        except Exception as e:
            print(f"âŒ ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def print_performance_summary(self, results: Dict):
        """ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“ˆ íŒŒì„œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        if 'test_summary' in results:
            summary = results['test_summary']
            print(f"ğŸ” í…ŒìŠ¤íŠ¸ ê°œìš”:")
            print(f"  - í…ŒìŠ¤íŠ¸ ë§¤ë¬¼: {summary['total_tests']}ê°œ")
            print(f"  - ì„±ê³µë¥ : {summary['success_rate']}%")
            print(f"  - ì„±ê³µ ê±´ìˆ˜: {summary['successful_tests']}ê°œ")
        
        if 'processing_time_stats' in results:
            time_stats = results['processing_time_stats']
            print(f"\nâ±ï¸ ì²˜ë¦¬ ì‹œê°„ í†µê³„:")
            print(f"  - í‰ê· : {time_stats.get('avg_ms', 0):.1f}ms")
            print(f"  - ìµœì†Œ: {time_stats.get('min_ms', 0):.1f}ms")
            print(f"  - ìµœëŒ€: {time_stats.get('max_ms', 0):.1f}ms")
            print(f"  - ì¤‘ê°„ê°’: {time_stats.get('median_ms', 0):.1f}ms")
        
        if 'section_performance' in results:
            print(f"\nğŸ“‹ ì„¹ì…˜ë³„ ì„±ëŠ¥ (í‰ê·  ì²˜ë¦¬ ì‹œê°„):")
            section_perf = results['section_performance']
            for section, stats in sorted(section_perf.items(), key=lambda x: x[1].get('avg_ms', 0), reverse=True):
                avg_time = stats.get('avg_ms', 0)
                count = stats.get('count', 0)
                print(f"  - {section}: {avg_time:.2f}ms (í…ŒìŠ¤íŠ¸ {count}íšŒ)")
        
        if 'memory_efficiency' in results:
            memory = results['memory_efficiency']
            print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±:")
            print(f"  - ë§¤ë¬¼ë‹¹ í‰ê·  ë©”ëª¨ë¦¬: {memory.get('avg_memory_per_property_mb', 0):.2f}MB")
            print(f"  - ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory.get('peak_memory_usage_mb', 0):.2f}MB")
            print(f"  - íš¨ìœ¨ì„± í‰ê°€: {memory.get('memory_efficiency', 'Unknown')}")
        
        if 'error_analysis' in results:
            errors = results['error_analysis']
            total_errors = errors.get('total_section_errors', 0)
            error_sections = errors.get('sections_with_errors', 0)
            
            print(f"\nâš ï¸ ì—ëŸ¬ ë¶„ì„:")
            print(f"  - ì´ ì„¹ì…˜ ì—ëŸ¬: {total_errors}ê°œ")
            print(f"  - ì—ëŸ¬ê°€ ìˆëŠ” ì„¹ì…˜: {error_sections}ê°œ")
            
            if 'section_error_details' in errors:
                for section, details in errors['section_error_details'].items():
                    error_count = details.get('error_count', 0)
                    print(f"    - {section}: {error_count}ê°œ ì—ëŸ¬")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¬ íŒŒì„œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    test_count = int(input("í…ŒìŠ¤íŠ¸í•  ë§¤ë¬¼ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 10): ") or "10")
    
    tester = ParserPerformanceTest()
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = tester.run_performance_test(test_count)
    
    if results:
        # ê²°ê³¼ ì¶œë ¥
        tester.print_performance_summary(results)
        
        # ë³´ê³ ì„œ ì €ì¥
        report_file = tester.generate_performance_report()
        
        print(f"\nâœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ğŸ“„ ìƒì„¸ ê²°ê³¼: {report_file}")
    else:
        print("âŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")

if __name__ == "__main__":
    main()