#!/usr/bin/env python3
"""
Scalability Framework for Multi-Region Data Collection System
=============================================================

This component provides horizontal scaling capabilities, distributed processing,
and multi-region support for the data collection system.

Key Features:
- Auto-scaling based on load and performance metrics
- Multi-region distributed processing
- Load balancing across worker pools
- Database sharding and partitioning strategies
- Caching layers for performance optimization
- Resource allocation and optimization
- Backup and disaster recovery mechanisms
"""

import asyncio
import time
import json
import redis
import logging
import threading
import psutil
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import hashlib
import boto3  # For AWS integration (optional)
from concurrent.futures import ThreadPoolExecutor
import aiohttp

class ScalingTrigger(Enum):
    """Triggers for scaling operations"""
    CPU_THRESHOLD = "cpu_threshold"
    MEMORY_THRESHOLD = "memory_threshold"
    QUEUE_LENGTH = "queue_length"
    RESPONSE_TIME = "response_time"
    ERROR_RATE = "error_rate"
    MANUAL = "manual"
    SCHEDULED = "scheduled"

class RegionStatus(Enum):
    """Status of different regions"""
    ACTIVE = "active"
    STANDBY = "standby"
    MAINTENANCE = "maintenance"
    OFFLINE = "offline"
    DEGRADED = "degraded"

@dataclass
class RegionConfig:
    """Configuration for a specific region"""
    region_id: str
    region_name: str
    endpoint_url: str
    database_config: Dict[str, Any]
    redis_config: Dict[str, Any]
    max_workers: int
    priority: int  # Higher priority regions get more load
    status: RegionStatus = RegionStatus.STANDBY
    health_score: float = 1.0
    last_health_check: Optional[datetime] = None
    
    def to_dict(self) -> Dict:
        return {
            'region_id': self.region_id,
            'region_name': self.region_name,
            'endpoint_url': self.endpoint_url,
            'database_config': self.database_config,
            'redis_config': self.redis_config,
            'max_workers': self.max_workers,
            'priority': self.priority,
            'status': self.status.value,
            'health_score': self.health_score,
            'last_health_check': self.last_health_check.isoformat() if self.last_health_check else None
        }

@dataclass
class ScalingMetrics:
    """Metrics used for scaling decisions"""
    timestamp: datetime
    cpu_usage_percent: float
    memory_usage_percent: float
    queue_length: int
    active_workers: int
    avg_response_time_ms: float
    error_rate: float
    throughput_per_minute: float
    pending_tasks: int
    
    def needs_scale_up(self, thresholds: Dict[str, float]) -> bool:
        """Check if metrics indicate need for scaling up"""
        return (
            self.cpu_usage_percent > thresholds.get('cpu_scale_up', 80) or
            self.memory_usage_percent > thresholds.get('memory_scale_up', 85) or
            self.queue_length > thresholds.get('queue_scale_up', 100) or
            self.avg_response_time_ms > thresholds.get('response_time_scale_up', 5000)
        )
    
    def needs_scale_down(self, thresholds: Dict[str, float]) -> bool:
        """Check if metrics indicate possibility for scaling down"""
        return (
            self.cpu_usage_percent < thresholds.get('cpu_scale_down', 30) and
            self.memory_usage_percent < thresholds.get('memory_scale_down', 50) and
            self.queue_length < thresholds.get('queue_scale_down', 10) and
            self.avg_response_time_ms < thresholds.get('response_time_scale_down', 1000)
        )

@dataclass
class WorkerPool:
    """Worker pool configuration and management"""
    pool_id: str
    region_id: str
    min_workers: int
    max_workers: int
    current_workers: int
    target_workers: int
    worker_type: str  # e.g., 'data_collector', 'processor', 'validator'
    last_scaled: Optional[datetime] = None
    scaling_cooldown: int = 300  # 5 minutes
    
    def can_scale(self) -> bool:
        """Check if scaling is allowed (respects cooldown)"""
        if not self.last_scaled:
            return True
        return (datetime.now() - self.last_scaled).total_seconds() > self.scaling_cooldown

@dataclass
class CacheLayer:
    """Cache layer configuration"""
    cache_type: str  # 'redis', 'memcached', 'local'
    endpoint: str
    ttl_seconds: int
    max_memory_mb: int
    hit_rate: float = 0.0
    last_updated: Optional[datetime] = None

class LoadBalancer:
    """Intelligent load balancer for distributing tasks across regions and workers"""
    
    def __init__(self):
        self.regions: Dict[str, RegionConfig] = {}
        self.region_weights: Dict[str, float] = {}
        self.task_distribution_history: List[Dict] = []
        self.lock = threading.Lock()
        self.logger = logging.getLogger('LoadBalancer')
    
    def add_region(self, region: RegionConfig):
        """Add region to load balancer"""
        with self.lock:
            self.regions[region.region_id] = region
            self._update_weights()
        
        self.logger.info(f"Added region to load balancer: {region.region_name}")
    
    def remove_region(self, region_id: str):
        """Remove region from load balancer"""
        with self.lock:
            if region_id in self.regions:
                del self.regions[region_id]
                self._update_weights()
        
        self.logger.info(f"Removed region from load balancer: {region_id}")
    
    def _update_weights(self):
        """Update region weights based on health and priority"""
        total_weight = 0
        
        for region in self.regions.values():
            if region.status == RegionStatus.ACTIVE:
                # Weight based on priority, health score, and inverse of current load
                weight = region.priority * region.health_score
                self.region_weights[region.region_id] = weight
                total_weight += weight
            else:
                self.region_weights[region.region_id] = 0
        
        # Normalize weights
        if total_weight > 0:
            for region_id in self.region_weights:
                self.region_weights[region_id] /= total_weight
    
    def select_region(self, task_requirements: Dict[str, Any] = None) -> Optional[str]:
        """Select optimal region for task execution"""
        with self.lock:
            active_regions = [
                region_id for region_id, region in self.regions.items()
                if region.status == RegionStatus.ACTIVE
            ]
            
            if not active_regions:
                return None
            
            # Simple weighted random selection
            import random
            rand_val = random.random()
            cumulative_weight = 0
            
            for region_id in active_regions:
                cumulative_weight += self.region_weights.get(region_id, 0)
                if rand_val <= cumulative_weight:
                    return region_id
            
            # Fallback to first active region
            return active_regions[0]
    
    def get_region_stats(self) -> Dict[str, Any]:
        """Get load balancer statistics"""
        with self.lock:
            return {
                'total_regions': len(self.regions),
                'active_regions': len([
                    r for r in self.regions.values() 
                    if r.status == RegionStatus.ACTIVE
                ]),
                'region_weights': self.region_weights.copy(),
                'task_distribution': self._calculate_distribution_stats()
            }
    
    def _calculate_distribution_stats(self) -> Dict[str, int]:
        """Calculate task distribution statistics"""
        # Count recent task assignments (last hour)
        cutoff_time = datetime.now() - timedelta(hours=1)
        recent_tasks = [
            task for task in self.task_distribution_history 
            if task['timestamp'] >= cutoff_time
        ]
        
        distribution = {}
        for task in recent_tasks:
            region_id = task['region_id']
            distribution[region_id] = distribution.get(region_id, 0) + 1
        
        return distribution

class AutoScaler:
    """Automatic scaling manager based on metrics and policies"""
    
    def __init__(self, config):
        self.config = config
        self.worker_pools: Dict[str, WorkerPool] = {}
        self.scaling_policies: Dict[str, Dict] = {}
        self.metrics_history: List[ScalingMetrics] = []
        self.lock = threading.Lock()
        self.logger = logging.getLogger('AutoScaler')
        
        # Default scaling thresholds
        self.scaling_thresholds = {
            'cpu_scale_up': 80,
            'cpu_scale_down': 30,
            'memory_scale_up': 85,
            'memory_scale_down': 50,
            'queue_scale_up': 100,
            'queue_scale_down': 10,
            'response_time_scale_up': 5000,
            'response_time_scale_down': 1000
        }
    
    def add_worker_pool(self, pool: WorkerPool):
        """Add worker pool for auto-scaling management"""
        with self.lock:
            self.worker_pools[pool.pool_id] = pool
        
        self.logger.info(f"Added worker pool for auto-scaling: {pool.pool_id}")
    
    def add_scaling_policy(self, pool_id: str, policy: Dict):
        """Add scaling policy for specific worker pool"""
        required_fields = ['scale_up_threshold', 'scale_down_threshold', 'scale_step']
        if not all(field in policy for field in required_fields):
            raise ValueError(f"Scaling policy must contain: {required_fields}")
        
        with self.lock:
            self.scaling_policies[pool_id] = policy
        
        self.logger.info(f"Added scaling policy for pool {pool_id}")
    
    async def evaluate_scaling(self, current_metrics: ScalingMetrics) -> List[Dict[str, Any]]:
        """Evaluate scaling needs and return scaling actions"""
        with self.lock:
            self.metrics_history.append(current_metrics)
            
            # Keep only recent metrics (last 2 hours)
            cutoff_time = datetime.now() - timedelta(hours=2)
            self.metrics_history = [
                m for m in self.metrics_history 
                if m.timestamp >= cutoff_time
            ]
        
        scaling_actions = []
        
        for pool_id, pool in self.worker_pools.items():
            if not pool.can_scale():
                continue
            
            policy = self.scaling_policies.get(pool_id, {})
            
            # Check if scaling up is needed
            if self._should_scale_up(current_metrics, pool, policy):
                action = await self._create_scale_up_action(pool, current_metrics)
                if action:
                    scaling_actions.append(action)
            
            # Check if scaling down is possible
            elif self._should_scale_down(current_metrics, pool, policy):
                action = await self._create_scale_down_action(pool, current_metrics)
                if action:
                    scaling_actions.append(action)
        
        return scaling_actions
    
    def _should_scale_up(self, metrics: ScalingMetrics, pool: WorkerPool, policy: Dict) -> bool:
        """Determine if worker pool should be scaled up"""
        if pool.current_workers >= pool.max_workers:
            return False
        
        # Use policy thresholds or defaults
        thresholds = {**self.scaling_thresholds, **policy}
        
        # Check if current metrics exceed thresholds
        if metrics.needs_scale_up(thresholds):
            # Verify trend over time to avoid thrashing
            recent_metrics = self.metrics_history[-5:]  # Last 5 data points
            if len(recent_metrics) >= 3:
                trend_exceeds = sum(
                    1 for m in recent_metrics 
                    if m.needs_scale_up(thresholds)
                ) >= 2
                return trend_exceeds
            else:
                return True  # Scale up if not enough history
        
        return False
    
    def _should_scale_down(self, metrics: ScalingMetrics, pool: WorkerPool, policy: Dict) -> bool:
        """Determine if worker pool should be scaled down"""
        if pool.current_workers <= pool.min_workers:
            return False
        
        # Use policy thresholds or defaults
        thresholds = {**self.scaling_thresholds, **policy}
        
        # Check if current metrics are below thresholds
        if metrics.needs_scale_down(thresholds):
            # Verify sustained low usage before scaling down
            recent_metrics = self.metrics_history[-10:]  # Last 10 data points
            if len(recent_metrics) >= 5:
                sustained_low = sum(
                    1 for m in recent_metrics 
                    if m.needs_scale_down(thresholds)
                ) >= 4
                return sustained_low
        
        return False
    
    async def _create_scale_up_action(self, pool: WorkerPool, metrics: ScalingMetrics) -> Optional[Dict]:
        """Create scale up action"""
        policy = self.scaling_policies.get(pool.pool_id, {})
        scale_step = policy.get('scale_step', 2)
        
        new_target = min(pool.current_workers + scale_step, pool.max_workers)
        
        if new_target > pool.current_workers:
            return {
                'action': 'scale_up',
                'pool_id': pool.pool_id,
                'region_id': pool.region_id,
                'current_workers': pool.current_workers,
                'target_workers': new_target,
                'reason': self._get_scaling_reason(metrics),
                'timestamp': datetime.now()
            }
        
        return None
    
    async def _create_scale_down_action(self, pool: WorkerPool, metrics: ScalingMetrics) -> Optional[Dict]:
        """Create scale down action"""
        policy = self.scaling_policies.get(pool.pool_id, {})
        scale_step = policy.get('scale_step', 1)  # More conservative for scale down
        
        new_target = max(pool.current_workers - scale_step, pool.min_workers)
        
        if new_target < pool.current_workers:
            return {
                'action': 'scale_down',
                'pool_id': pool.pool_id,
                'region_id': pool.region_id,
                'current_workers': pool.current_workers,
                'target_workers': new_target,
                'reason': 'Low resource utilization',
                'timestamp': datetime.now()
            }
        
        return None
    
    def _get_scaling_reason(self, metrics: ScalingMetrics) -> str:
        """Get human-readable reason for scaling"""
        reasons = []
        
        if metrics.cpu_usage_percent > 80:
            reasons.append(f"High CPU: {metrics.cpu_usage_percent:.1f}%")
        if metrics.memory_usage_percent > 85:
            reasons.append(f"High Memory: {metrics.memory_usage_percent:.1f}%")
        if metrics.queue_length > 100:
            reasons.append(f"Long Queue: {metrics.queue_length}")
        if metrics.avg_response_time_ms > 5000:
            reasons.append(f"Slow Response: {metrics.avg_response_time_ms:.0f}ms")
        
        return "; ".join(reasons) if reasons else "Performance threshold exceeded"

class CacheManager:
    """Multi-layer caching system for performance optimization"""
    
    def __init__(self):
        self.cache_layers: Dict[str, CacheLayer] = {}
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'evictions': 0
        }
        self.lock = threading.Lock()
        self.logger = logging.getLogger('CacheManager')
    
    def add_cache_layer(self, name: str, cache_layer: CacheLayer):
        """Add cache layer"""
        with self.lock:
            self.cache_layers[name] = cache_layer
        
        self.logger.info(f"Added cache layer: {name} ({cache_layer.cache_type})")
    
    async def get(self, key: str, cache_layers: List[str] = None) -> Optional[Any]:
        """Get value from cache with multi-layer fallback"""
        if cache_layers is None:
            cache_layers = list(self.cache_layers.keys())
        
        for layer_name in cache_layers:
            if layer_name not in self.cache_layers:
                continue
            
            try:
                value = await self._get_from_layer(layer_name, key)
                if value is not None:
                    self.cache_stats['hits'] += 1
                    
                    # Promote to higher priority layers
                    await self._promote_to_higher_layers(key, value, layer_name, cache_layers)
                    return value
                    
            except Exception as e:
                self.logger.warning(f"Cache get error in layer {layer_name}: {e}")
        
        self.cache_stats['misses'] += 1
        return None
    
    async def set(self, key: str, value: Any, ttl: int = None, cache_layers: List[str] = None):
        """Set value in cache layers"""
        if cache_layers is None:
            cache_layers = list(self.cache_layers.keys())
        
        for layer_name in cache_layers:
            if layer_name not in self.cache_layers:
                continue
            
            try:
                await self._set_in_layer(layer_name, key, value, ttl)
                self.cache_stats['sets'] += 1
                
            except Exception as e:
                self.logger.warning(f"Cache set error in layer {layer_name}: {e}")
    
    async def _get_from_layer(self, layer_name: str, key: str) -> Optional[Any]:
        """Get value from specific cache layer"""
        layer = self.cache_layers[layer_name]
        
        if layer.cache_type == 'redis':
            # Redis implementation
            import redis
            r = redis.from_url(layer.endpoint)
            value = r.get(key)
            return json.loads(value) if value else None
        
        elif layer.cache_type == 'local':
            # Local memory cache implementation
            # This would use a local dictionary or LRU cache
            pass
        
        return None
    
    async def _set_in_layer(self, layer_name: str, key: str, value: Any, ttl: int = None):
        """Set value in specific cache layer"""
        layer = self.cache_layers[layer_name]
        ttl = ttl or layer.ttl_seconds
        
        if layer.cache_type == 'redis':
            import redis
            r = redis.from_url(layer.endpoint)
            r.setex(key, ttl, json.dumps(value))
        
        elif layer.cache_type == 'local':
            # Local memory cache implementation
            pass
    
    async def _promote_to_higher_layers(self, key: str, value: Any, found_layer: str, all_layers: List[str]):
        """Promote cache entry to higher priority layers"""
        found_index = all_layers.index(found_layer)
        
        # Set in higher priority layers (lower index)
        for i in range(found_index):
            layer_name = all_layers[i]
            try:
                await self._set_in_layer(layer_name, key, value, None)
            except Exception as e:
                self.logger.warning(f"Cache promotion error in layer {layer_name}: {e}")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = self.cache_stats['hits'] / total_requests if total_requests > 0 else 0
        
        return {
            'total_requests': total_requests,
            'hit_rate': hit_rate,
            'hits': self.cache_stats['hits'],
            'misses': self.cache_stats['misses'],
            'sets': self.cache_stats['sets'],
            'evictions': self.cache_stats['evictions'],
            'cache_layers': {
                name: layer.cache_type for name, layer in self.cache_layers.items()
            }
        }

class DataShardManager:
    """Database sharding and partitioning manager"""
    
    def __init__(self):
        self.shards: Dict[str, Dict] = {}
        self.sharding_strategy = 'hash'  # 'hash', 'range', 'directory'
        self.shard_key = 'region_code'
        self.logger = logging.getLogger('DataShardManager')
    
    def add_shard(self, shard_id: str, connection_config: Dict):
        """Add database shard"""
        self.shards[shard_id] = {
            'shard_id': shard_id,
            'connection_config': connection_config,
            'status': 'active',
            'last_health_check': None,
            'health_score': 1.0
        }
        
        self.logger.info(f"Added database shard: {shard_id}")
    
    def get_shard_for_key(self, key: str) -> Optional[str]:
        """Get appropriate shard for given key"""
        if not self.shards:
            return None
        
        if self.sharding_strategy == 'hash':
            shard_hash = hashlib.md5(key.encode()).hexdigest()
            shard_index = int(shard_hash, 16) % len(self.shards)
            return list(self.shards.keys())[shard_index]
        
        elif self.sharding_strategy == 'range':
            # Range-based sharding (would need configuration)
            pass
        
        elif self.sharding_strategy == 'directory':
            # Directory-based sharding (would need lookup table)
            pass
        
        return list(self.shards.keys())[0]  # Fallback
    
    def get_shard_stats(self) -> Dict[str, Any]:
        """Get sharding statistics"""
        return {
            'total_shards': len(self.shards),
            'active_shards': len([s for s in self.shards.values() if s['status'] == 'active']),
            'sharding_strategy': self.sharding_strategy,
            'shard_key': self.shard_key,
            'shards': {
                shard_id: {
                    'status': shard['status'],
                    'health_score': shard['health_score']
                }
                for shard_id, shard in self.shards.items()
            }
        }

class DisasterRecoveryManager:
    """Disaster recovery and backup management"""
    
    def __init__(self):
        self.backup_locations: List[Dict] = []
        self.recovery_procedures: Dict[str, Callable] = {}
        self.backup_schedule = {}
        self.last_backup = {}
        self.logger = logging.getLogger('DisasterRecoveryManager')
    
    def add_backup_location(self, location_config: Dict):
        """Add backup location"""
        required_fields = ['location_id', 'type', 'endpoint']
        if not all(field in location_config for field in required_fields):
            raise ValueError(f"Backup location must contain: {required_fields}")
        
        self.backup_locations.append(location_config)
        self.logger.info(f"Added backup location: {location_config['location_id']}")
    
    def register_recovery_procedure(self, scenario: str, procedure: Callable):
        """Register recovery procedure for specific scenario"""
        self.recovery_procedures[scenario] = procedure
        self.logger.info(f"Registered recovery procedure: {scenario}")
    
    async def create_backup(self, backup_type: str, data_source: str) -> Dict[str, Any]:
        """Create backup of specified data source"""
        backup_id = f"backup_{backup_type}_{int(time.time())}"
        
        backup_result = {
            'backup_id': backup_id,
            'backup_type': backup_type,
            'data_source': data_source,
            'timestamp': datetime.now(),
            'status': 'in_progress',
            'locations': [],
            'size_bytes': 0
        }
        
        try:
            # Create backup in each configured location
            for location in self.backup_locations:
                location_result = await self._create_backup_in_location(
                    backup_id, backup_type, data_source, location
                )
                backup_result['locations'].append(location_result)
            
            backup_result['status'] = 'completed'
            self.last_backup[backup_type] = backup_result
            
            self.logger.info(f"Backup created successfully: {backup_id}")
            
        except Exception as e:
            backup_result['status'] = 'failed'
            backup_result['error'] = str(e)
            self.logger.error(f"Backup creation failed: {e}")
        
        return backup_result
    
    async def _create_backup_in_location(self, backup_id: str, backup_type: str, data_source: str, location: Dict) -> Dict:
        """Create backup in specific location"""
        location_result = {
            'location_id': location['location_id'],
            'status': 'in_progress',
            'backup_path': None,
            'size_bytes': 0
        }
        
        try:
            if location['type'] == 's3':
                # AWS S3 backup
                location_result = await self._backup_to_s3(backup_id, data_source, location)
            
            elif location['type'] == 'local':
                # Local filesystem backup
                location_result = await self._backup_to_local(backup_id, data_source, location)
            
            elif location['type'] == 'ftp':
                # FTP backup
                location_result = await self._backup_to_ftp(backup_id, data_source, location)
            
            location_result['status'] = 'completed'
            
        except Exception as e:
            location_result['status'] = 'failed'
            location_result['error'] = str(e)
        
        return location_result
    
    async def _backup_to_s3(self, backup_id: str, data_source: str, location: Dict) -> Dict:
        """Backup to AWS S3"""
        # S3 backup implementation
        return {
            'location_id': location['location_id'],
            'backup_path': f"s3://{location['bucket']}/{backup_id}",
            'size_bytes': 0  # Would be calculated
        }
    
    async def _backup_to_local(self, backup_id: str, data_source: str, location: Dict) -> Dict:
        """Backup to local filesystem"""
        backup_path = Path(location['path']) / f"{backup_id}.backup"
        
        # Local backup implementation would go here
        
        return {
            'location_id': location['location_id'],
            'backup_path': str(backup_path),
            'size_bytes': 0  # Would be calculated
        }
    
    async def _backup_to_ftp(self, backup_id: str, data_source: str, location: Dict) -> Dict:
        """Backup to FTP server"""
        # FTP backup implementation
        return {
            'location_id': location['location_id'],
            'backup_path': f"ftp://{location['host']}/{backup_id}.backup",
            'size_bytes': 0  # Would be calculated
        }
    
    async def execute_recovery(self, scenario: str, recovery_options: Dict = None) -> Dict[str, Any]:
        """Execute disaster recovery procedure"""
        if scenario not in self.recovery_procedures:
            raise ValueError(f"No recovery procedure registered for scenario: {scenario}")
        
        recovery_result = {
            'scenario': scenario,
            'started_at': datetime.now(),
            'status': 'in_progress',
            'steps_completed': [],
            'error_details': None
        }
        
        try:
            procedure = self.recovery_procedures[scenario]
            result = await procedure(recovery_options or {})
            
            recovery_result.update(result)
            recovery_result['status'] = 'completed'
            recovery_result['completed_at'] = datetime.now()
            
            self.logger.info(f"Disaster recovery completed for scenario: {scenario}")
            
        except Exception as e:
            recovery_result['status'] = 'failed'
            recovery_result['error_details'] = str(e)
            recovery_result['failed_at'] = datetime.now()
            
            self.logger.error(f"Disaster recovery failed for scenario {scenario}: {e}")
        
        return recovery_result

class ScalabilityFramework:
    """
    Main scalability framework coordinating all scaling and distribution components
    """
    
    def __init__(self, config):
        self.config = config
        
        # Components
        self.load_balancer = LoadBalancer()
        self.auto_scaler = AutoScaler(config)
        self.cache_manager = CacheManager()
        self.shard_manager = DataShardManager()
        self.disaster_recovery = DisasterRecoveryManager()
        
        # Multi-region support
        self.regions: Dict[str, RegionConfig] = {}
        self.active_region = None
        
        # Background tasks
        self.background_tasks = []
        self.shutdown_event = asyncio.Event()
        
        # Logging
        self.logger = logging.getLogger('ScalabilityFramework')
        
        print("‚ö° Scalability Framework initialized")
    
    async def initialize(self):
        """Initialize scalability framework"""
        try:
            # Setup default cache layers
            await self._setup_default_caching()
            
            # Setup default sharding
            await self._setup_default_sharding()
            
            # Setup default backup locations
            await self._setup_default_backups()
            
            # Start background services
            self.background_tasks.extend([
                asyncio.create_task(self._scaling_monitor_service()),
                asyncio.create_task(self._health_check_service()),
                asyncio.create_task(self._backup_service())
            ])
            
            self.logger.info("Scalability Framework initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"Scalability Framework initialization failed: {e}")
            return False
    
    async def _setup_default_caching(self):
        """Setup default cache layers"""
        # Local memory cache (highest priority)
        local_cache = CacheLayer(
            cache_type='local',
            endpoint='memory',
            ttl_seconds=300,  # 5 minutes
            max_memory_mb=512
        )
        self.cache_manager.add_cache_layer('local', local_cache)
        
        # Redis cache (lower priority but persistent)
        redis_cache = CacheLayer(
            cache_type='redis',
            endpoint='redis://localhost:6379',
            ttl_seconds=3600,  # 1 hour
            max_memory_mb=2048
        )
        self.cache_manager.add_cache_layer('redis', redis_cache)
    
    async def _setup_default_sharding(self):
        """Setup default database sharding"""
        # Add primary shard (would be configured based on actual database)
        self.shard_manager.add_shard('primary', {
            'host': 'localhost',
            'database': 'naver_collection_primary',
            'connection_pool_size': 20
        })
    
    async def _setup_default_backups(self):
        """Setup default backup locations"""
        # Local backup location
        self.disaster_recovery.add_backup_location({
            'location_id': 'local_backup',
            'type': 'local',
            'path': './backups',
            'retention_days': 7
        })
        
        # Register default recovery procedures
        self.disaster_recovery.register_recovery_procedure(
            'database_failure',
            self._database_recovery_procedure
        )
    
    async def add_region(self, region_config: Dict) -> str:
        """Add new region for multi-region support"""
        region = RegionConfig(
            region_id=region_config['region_id'],
            region_name=region_config['region_name'],
            endpoint_url=region_config['endpoint_url'],
            database_config=region_config['database_config'],
            redis_config=region_config['redis_config'],
            max_workers=region_config.get('max_workers', 10),
            priority=region_config.get('priority', 1)
        )
        
        self.regions[region.region_id] = region
        self.load_balancer.add_region(region)
        
        # Add worker pool for this region
        worker_pool = WorkerPool(
            pool_id=f"{region.region_id}_collectors",
            region_id=region.region_id,
            min_workers=2,
            max_workers=region.max_workers,
            current_workers=2,
            target_workers=2,
            worker_type='data_collector'
        )
        self.auto_scaler.add_worker_pool(worker_pool)
        
        self.logger.info(f"Added region: {region.region_name}")
        return region.region_id
    
    async def activate_region(self, region_id: str):
        """Activate region for processing"""
        if region_id in self.regions:
            self.regions[region_id].status = RegionStatus.ACTIVE
            self.active_region = region_id
            self.logger.info(f"Activated region: {region_id}")
    
    async def get_optimal_task_distribution(self, tasks: List[Dict]) -> Dict[str, List[Dict]]:
        """Get optimal distribution of tasks across regions"""
        distribution = {}
        
        for task in tasks:
            region_id = self.load_balancer.select_region(task)
            if region_id:
                if region_id not in distribution:
                    distribution[region_id] = []
                distribution[region_id].append(task)
        
        return distribution
    
    async def scale_resources(self, scaling_trigger: ScalingTrigger, metrics: ScalingMetrics) -> List[Dict]:
        """Scale resources based on trigger and metrics"""
        scaling_actions = await self.auto_scaler.evaluate_scaling(metrics)
        
        executed_actions = []
        for action in scaling_actions:
            try:
                result = await self._execute_scaling_action(action)
                executed_actions.append(result)
                
            except Exception as e:
                self.logger.error(f"Failed to execute scaling action: {e}")
        
        return executed_actions
    
    async def _execute_scaling_action(self, action: Dict) -> Dict:
        """Execute individual scaling action"""
        action_type = action['action']
        pool_id = action['pool_id']
        target_workers = action['target_workers']
        
        if pool_id in self.auto_scaler.worker_pools:
            pool = self.auto_scaler.worker_pools[pool_id]
            pool.target_workers = target_workers
            pool.last_scaled = datetime.now()
            
            self.logger.info(
                f"Scaling {action_type} for pool {pool_id}: "
                f"{pool.current_workers} -> {target_workers} workers"
            )
            
            return {
                'action': action_type,
                'pool_id': pool_id,
                'old_workers': pool.current_workers,
                'new_workers': target_workers,
                'status': 'completed',
                'timestamp': datetime.now()
            }
        
        return {
            'action': action_type,
            'pool_id': pool_id,
            'status': 'failed',
            'error': 'Pool not found'
        }
    
    async def _scaling_monitor_service(self):
        """Background service for monitoring scaling needs"""
        while not self.shutdown_event.is_set():
            try:
                # Collect current metrics
                current_metrics = ScalingMetrics(
                    timestamp=datetime.now(),
                    cpu_usage_percent=psutil.cpu_percent(),
                    memory_usage_percent=psutil.virtual_memory().percent,
                    queue_length=0,  # Would get from actual queue
                    active_workers=sum(pool.current_workers for pool in self.auto_scaler.worker_pools.values()),
                    avg_response_time_ms=1000,  # Would calculate from actual metrics
                    error_rate=0.01,  # Would calculate from actual metrics
                    throughput_per_minute=50,  # Would calculate from actual metrics
                    pending_tasks=0  # Would get from actual queue
                )
                
                # Evaluate scaling needs
                await self.scale_resources(ScalingTrigger.CPU_THRESHOLD, current_metrics)
                
            except Exception as e:
                self.logger.error(f"Scaling monitor service error: {e}")
            
            await asyncio.sleep(60)  # Check every minute
    
    async def _health_check_service(self):
        """Background service for health checking regions"""
        while not self.shutdown_event.is_set():
            try:
                for region in self.regions.values():
                    health_score = await self._check_region_health(region)
                    region.health_score = health_score
                    region.last_health_check = datetime.now()
                    
                    # Update load balancer weights
                    self.load_balancer._update_weights()
                
            except Exception as e:
                self.logger.error(f"Health check service error: {e}")
            
            await asyncio.sleep(30)  # Check every 30 seconds
    
    async def _check_region_health(self, region: RegionConfig) -> float:
        """Check health of specific region"""
        try:
            # Simple health check by pinging the endpoint
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5)) as session:
                async with session.get(f"{region.endpoint_url}/health") as response:
                    if response.status == 200:
                        return 1.0
                    else:
                        return 0.5
        except:
            return 0.1  # Very low health score if unreachable
    
    async def _backup_service(self):
        """Background service for automated backups"""
        while not self.shutdown_event.is_set():
            try:
                # Check if backup is needed (daily)
                last_backup = self.disaster_recovery.last_backup.get('daily')
                if (
                    not last_backup or 
                    datetime.now() - last_backup['timestamp'] > timedelta(days=1)
                ):
                    await self.disaster_recovery.create_backup('daily', 'database')
                
            except Exception as e:
                self.logger.error(f"Backup service error: {e}")
            
            await asyncio.sleep(3600)  # Check every hour
    
    async def _database_recovery_procedure(self, options: Dict) -> Dict:
        """Default database recovery procedure"""
        recovery_steps = []
        
        # Step 1: Switch to backup database
        recovery_steps.append("Switched to backup database")
        
        # Step 2: Restore from latest backup
        recovery_steps.append("Restored from latest backup")
        
        # Step 3: Verify data integrity
        recovery_steps.append("Verified data integrity")
        
        return {
            'steps_completed': recovery_steps,
            'recovery_time_minutes': 30,
            'data_loss_hours': 0
        }
    
    def get_framework_status(self) -> Dict[str, Any]:
        """Get comprehensive framework status"""
        return {
            'regions': {
                region_id: region.to_dict() 
                for region_id, region in self.regions.items()
            },
            'load_balancer': self.load_balancer.get_region_stats(),
            'cache_stats': self.cache_manager.get_cache_stats(),
            'shard_stats': self.shard_manager.get_shard_stats(),
            'worker_pools': {
                pool_id: {
                    'current_workers': pool.current_workers,
                    'target_workers': pool.target_workers,
                    'max_workers': pool.max_workers,
                    'can_scale': pool.can_scale()
                }
                for pool_id, pool in self.auto_scaler.worker_pools.items()
            },
            'active_region': self.active_region,
            'timestamp': datetime.now().isoformat()
        }
    
    async def health_check(self) -> str:
        """Component health check"""
        try:
            # Check if background tasks are running
            if any(task.done() for task in self.background_tasks):
                return 'degraded'
            
            # Check if any regions are active
            active_regions = [
                r for r in self.regions.values() 
                if r.status == RegionStatus.ACTIVE
            ]
            
            if not active_regions:
                return 'degraded'
            
            return 'healthy'
            
        except Exception as e:
            self.logger.error(f"Health check failed: {e}")
            return 'unhealthy'

# Testing
async def test_scalability_framework():
    """Test scalability framework"""
    print("üß™ Testing Scalability Framework")
    print("=" * 50)
    
    # Create framework
    from robust_architecture_design import SystemConfiguration
    config = SystemConfiguration()
    
    framework = ScalabilityFramework(config)
    
    # Initialize
    success = await framework.initialize()
    print(f"‚úÖ Initialization: {'Success' if success else 'Failed'}")
    
    # Add test region
    region_config = {
        'region_id': 'test_region',
        'region_name': 'Test Region',
        'endpoint_url': 'http://localhost:8080',
        'database_config': {'host': 'localhost'},
        'redis_config': {'host': 'localhost'},
        'max_workers': 10,
        'priority': 1
    }
    
    region_id = await framework.add_region(region_config)
    await framework.activate_region(region_id)
    print(f"‚úÖ Added and activated region: {region_id}")
    
    # Test task distribution
    test_tasks = [
        {'task_id': f'task_{i}', 'type': 'collection'}
        for i in range(5)
    ]
    
    distribution = await framework.get_optimal_task_distribution(test_tasks)
    print(f"üìä Task Distribution: {len(distribution)} regions")
    
    # Test scaling
    test_metrics = ScalingMetrics(
        timestamp=datetime.now(),
        cpu_usage_percent=85,  # High CPU to trigger scaling
        memory_usage_percent=70,
        queue_length=150,
        active_workers=2,
        avg_response_time_ms=3000,
        error_rate=0.02,
        throughput_per_minute=30,
        pending_tasks=50
    )
    
    scaling_actions = await framework.scale_resources(ScalingTrigger.CPU_THRESHOLD, test_metrics)
    print(f"‚ö° Scaling Actions: {len(scaling_actions)}")
    
    # Get framework status
    status = framework.get_framework_status()
    print(f"üìà Framework Status: {len(status['regions'])} regions, {len(status['worker_pools'])} pools")
    
    # Health check
    health = await framework.health_check()
    print(f"üè• Health: {health}")
    
    print("‚úÖ Scalability Framework test complete")

if __name__ == "__main__":
    asyncio.run(test_scalability_framework())